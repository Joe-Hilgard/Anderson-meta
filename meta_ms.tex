% Note: Ferguson:2007a is "Evidence for publication bias"; Ferguson:2007b is "Good Bad and Ugly: Meta-analytic review"

% Bibtex is screwing up the AAP public policy statement citation. Also Konijn et al.

% It has also been widely cautioned that because trim and fill and some other techniques for assessing publication bias are based on an association between effect size and sample size, other explanations of this association should be considered. For example, effect sizes in experimental studies may be larger than those in crosssectional or longitudinal studies due to the reduced error variance that results from tight experimental controls; researchers may know this and therefore may intentionally plan to use larger sample sizes when conducting nonexperimental studies. Similarly, in some research contexts with very large sample sizes (e.g., national surveys) a researcher may have to use less precise measures (e.g., fewer items) that result in smaller effect sizes. In sum, it is possible that the effects in the studies with small samples really are larger than those in the studies with large samples (cf. Sterne and Egger, 2005). \citep[p. 152-153]{Anderson:etal:2010}
% Anderson et al also complain that \citet{Ferguson:2007a} used ``a very small set of available studies'' and ``For example, counter to widely accepted procedures for reducing the impact of publication bias, only published articles were included in the analyses and then procedures for addressing publication bias were misinterpreted. Also, studies published prior to 1995 were ignored and a large number of studies published since that time apparently were missed.'' \citep[p. 152]{Anderson:etal:2010}

\documentclass[man]{apa6}
%\documentclass{article}
\usepackage[natbibapa]{apacite}
\usepackage[longnamesfirst]{natbib}

\rightheader{Bias in Violent Games Research}
\shorttitle{Bias in Violent Games Research}

\leftheader{Hilgard et al.}

\author{Joseph Hilgard, Christopher R. Engelhardt, Bruce D. Bartholow, and Jeffrey N. Rouder}
% Broad Consensus? Open the coffin? Much ado about something out of nothing?
%\title{There Should Not Be Broad Consensus: Bias in Violent Games Research}
\title{Bias in Violent Games Research: A PET-PEESE Meta-Regression of Anderson et al. (2010)}

\affiliation{University of Missouri}

\authornote{
Joseph Hilgard, University of Missouri-Columbia.
Please direct correspondence regarding this article to Joseph Hilgard. E-mail: jhilgard@gmail.com
}

\abstract{
Violent video games are theorized to be a significant cause of aggressive thoughts, feelings, and behaviors. A meta-analysis by Anderson and colleagues (2010) is thought to condense the available evidence into robust and incontrovertible evidence that violent video games affect these outcomes in experimental, cross-sectional, and longitudinal research.
In the present manuscript, we examine previous meta-analytic evidence and apply PET-PEESE meta-regression, a statistical meta-analytic technique that tests and adjusts for small-study effects including (but not limited to) publication bias and flexible analysis. 
We find evidence that effects have been overestimated for some outcomes in experimental research, particularly those studies selected by Anderson and colleagues as best-practices research. In fact, among experimental studies selected as best-practices, no significant effects were present after adjusting for small-study effects. On the other hand, effects as measured in the full selection of experimental studies (that is, both those studies that were selected as best- and not-best-practices) reveal less bias and some evidence for an effect, albeit a smaller one than reported by Anderson et al. Finally, correlations observed cross-sectional studies were relatively robust and less influenced by research bias. 
We find that the evidence for effects of violent games on aggressive outcomes in best-practices experiments have been overestimated through publication, selection, or analytic bias.
We outline possible sources of research, selection, and analytic bias and suggest directions for stronger future experimental research.
%We conclude that the extant evidence is contaminated by bias and is not sufficient for decisive conclusions. 
The results indicate the need for an open, transparent, and pre-registered research process to test the existence of the basic phenomenon.
}

\begin{document}
\maketitle

%1. It is believed by many that violent games cause violent outcomes and that the experimental research is convincing, but others are less persuaded
Do violent games make their players more aggressive? Despite decades of research and hundreds of studies, there remains scientific debate. Results have been aggregated in several meta-analyses, the most-cited and most comprehensive of which \citep{Anderson:etal:2010} claimed decisive evidence for effects on aggressive thoughts, feelings, and behaviors in experimental, cross-sectional, and longitudinal research designs. 
This impressive volume of converging research findings has been hailed by some as decisive evidence \citep{Bushman:etal:2010,Huesmann:2010,Huesmann:2014}. 
% Cited in recent public policy statements or updates of same?
Policy statements from a number of professional organizations \citep[e.g.,][]{AAP:2009} reflect this perspective, urging the public of the considerable evidence demonstrating harmful outcomes of violent media use.

%2. Results from naive meta-analysis and consequent certainty among researchers. Nailing the coffin shut on doubts (Huesmann, 2010), Why is it so hard to convince / new media old problems (Donnerstein, Strasburger, Bushman, 2014), There is broad consensus (Bushman et al), how do we silence denialists (Anderson et al)

% and yet, still skeptics
Despite this meta-analysis, there are still skeptics of causal effects of violent video games on aggressive outcomes.
One point of skepticism is that the actual effect size is zero. Supporting this point, meta-analyses conducted by skeptics estimate smaller effects \citep{Ferguson:2007a,Ferguson:2007b,Ferguson:InPress,Sherry:2001}, %Although these are sometimes criticized for containing fewer studies than the  % Bushman much ado about something citation?
and some experiments by skeptics fail to detect significant effects of violent game content \citep{Adachi:Willoughby:2011,Elson:etal:2013,Ferguson:etal:2008,Valadez:Ferguson:2012}. However, these meta-analyses are sometimes criticized for containing fewer studies than the \citet{Anderson:etal:2010} meta-analysis \citep[see, e.g., a response by ]{Bushman:XXXX}, and the weight of evidence in nonsignificant experimental results tends to vary \citep{Hilgard:etal:2014}.

% Is this paragraph necessary? On the one hand, it illustrates that there's a debate, but on the other hand, it doesn't refer to my main point: pub bias.
Another point of skepticism is that the obtained effects may be statistically robust, but the interpretation is flawed. One argument is that the effects observed in experiments are due to confounds, not violent content \citep{Adachi:Willoughby:2011b;Elson:etal:2013}. In experimental research, these confounds would include differences between video games in dimensions other than violent content. In correlational and longitudinal research, these confounds are proposed also to include dispositional features that may attract players to both violent games and aggressive behavior over time.
Others argue that the public impact of the research has been overstated. This argument questions the external validity of laboratory measures and posits that causal changes observed in laboratory measures may not reflect real-world causal changes in real-world aggressive behavior \citep{Elson:Ferguson:2014}. %potential ecological fallacy, . arguing that society is becoming less aggressive despite increased use of video games 
% Could cite Markey et al. somewhere in here, his time-lag paper is really neat.

%2. Sources of controversy include failures to replicate and speculation about biased analysis, reporting, and meta-analysis of research.
However, the debate is not limited to theoretical and methodological concerns.  
Skeptics have suggested that the literature is contaminated by biases in analysis and report that make the evidence appear stronger than it is. For example,  \citet{Ferguson:2007a} suggests that studies that do not find significant effects are less likely to be published. If this is the case, then meta-analysis of the published data would systematically overestimate the effect, observing studies that estimate larger effects but not observing studies that estimate smaller or even negative effects. 
Similarly, others have suggested that obtained study data have been flexibly analyzed until the desired research conclusion was reached \citep{Elson:etal:2014;Ferguson:CITATION:NEEDED}. Such flexible analysis would bias the results of individual studies, nudging their effect sizes until they were large enough to reach statistical significance. In aggregate, then, the sum of these biased studies would itself be biased, again overestimating the true effect size.
If either of these are the case, then the extant data may not permit an appropriate hypothesis test, however overwhelming the evidence may otherwise seem to be. In the presence of bias in publication or analysis, the effect of violent games will be overestimated.

Another point of contention has been the application of ``best-practices criteria'' to studies gathered for meta-analysis. In their meta-analysis, \citet{Anderson:etal:2010} collected all available studies, then applied a set of ``best-practices criteria'' to separate these into what they argued were and were not appropriate tests of the research hypothesis. The authors reported that studies which had been performed according to these criteria found larger effects of violent games than did studies which had not been performed in accordance. It has been argued, however, that these criteria were vague in definition and inconsistent in application \citep{Elson:Ferguson:2014,Ferguson:Kilburn:2010}. 
% Was this implied, or stated explicitly?
It is implied that the inconsistency in application was motivated, with the meta-analysts selecting hypothesis-confirming studies as being best-practices studies while discarding studies with non-significant as being not-best-practices studies.

% Paragraph five is the ``nut 'graph''
In the present manuscript, we inspect the strength of the available literature by revisiting the meta-analysis presented by \citet{Anderson:etal:2010}. In that manuscript, authors applied a trim-and-fill procedure \citep{Duval:Tweedie:2000} to inspect and adjust for the presence of bias. 
% Read his citation before making this argument. Consider citing Data Colada post? 
% http://onlinelibrary.wiley.com/doi/10.1002/sim.2889/abstract
However, the trim-and-fill procedure is understood to be flawed, having assumptions that are likely to be unmet in actual practice. It is expected to under-correct in the presence of bias and over-correct in the absence of bias \citep{Peters:etal:2007,Simonsohn:DATACOLADAPOST}.
New meta-analytic techniques have since been developed that promise greater accuracy than trim-and-fill. We apply PET-PEESE meta-regression \citep{Stanley:Doucouliagos:2014}, which inspects the degree of research bias by testing the correlation between sample size and effect size, and report adjusted effect sizes and new inferences.

% Time is running out for academic freedom
\subsection{Is the Debate Concluded?}
\begin{quote}
Despite thousands of research studies on media effects, many people simply refuse to believe them. Some academics may contribute to this because they like to ``buck the establishment,'' which is an easy way to promote themselves and their research. Of course, many people still believe that President Obama wasn't born in the United States, President Kennedy wasn't assassinated, men didn't walk on the moon, and the Holocaust didn't occur. \citep[p. 572]{Strasburger:etal:2014}
\end{quote}
%end blockquote

In the past few years, there has been a change in the tone of the debate surrounding violent video game effects. Some proponents are sufficiently convinced of the effects that any remaining skepticism seems to be due to unfathomable stubbornness. Because these proponents consider the research question settled, they have moved to novel research topics. Some study conceptual extensions of the basic phenomenon to include prosocial effects of prosocial video games. Others study science denial and biased assimilation among those motivated to distrust violent games research. 

% Could cite some of the biased-assimilation research re: gamers reading the evidence. E.g. Przybyslki, but some others, too.
% Greitemeyer 2014 I am right, you are wrong
% Przybylski 2014 Who believes
% Nauroth:etal:2013 Gamers against science
This latter topic has begun to expand into a line of science-communication research exploring ways to understand skepticism about violent-game effects.   For example, \citet{Nauroth:etal:2014} present evidence that gamers are themselves particularly resistant to evidence of negative effects of video games. \citet{Greitemeyer:2014} similarly finds that readers favor studies of violent-game effects that confirm their beliefs. Strategies are now being developed to make violent media research more convincing and actionable. One such strategy is the proposal that researchers ``advance the debate'' by speaking directly to the public to avoid skeptics within academia \citep{Strasburger:Donnerstein:2014}. Claims of consensus have been advanced \citep{Bushman:etal:2014}, and attempts have been made to separate ``true media violence scientists'' who believe in the effect from less-expert sources motivated to deny evidence \citep{Anderson:etal:2014}. It has been suggested that the next challenge for violent-games research is not to better understand the phenomenon, but rather, to foster belief in the phenomenon among both the wider scientific community and the laity, perhaps in part by excluding skeptics from public debate \citep{Anderson:etal:2014}.

Whether this is an appropriate course of action depends on the strength of evidence. If the evidence is incontrovertible, then skeptics may be misleading their audiences by refusing to update their beliefs in light of research findings. One could debate the nature of the effect and the validity of manipulations and measures, but one could not argue that the effect does not exist. On the other hand, if the evidence is flawed, then proponents risk stifling research and debate where it is most needed. A thorough and conservative inspection of the evidence, then, is of vital importance.

% Social Psychologists are Knuckleheads Theory
% citations: Bakker et al, 2011, Rules of the Game Known as Psych Science; Simmons et al., 2011, flexible research; Ioannidis, 2011, Most Published Research Findings Are False; Bones:2012, We Knew the Future All Along
% This subsection really needs some work.
\subsection{Publication Bias and Small-Study Effects}
In recent years, psychology has experienced a crisis of confidence as researchers realize that many published research findings may be false. Using the typical inferential framework, researchers have been able to provide experimental evidence for impossible phenomena such as extra-sensory precogition \citep[psi][]{Bem:2011} and a song that makes its listeners younger \citep{Simmons:etal:2011}. Critics have pointed out that hypothesis-confirming results appear in the literature much more frequently than would be expected given reasonable estimates of statistical power. % Greg Francis? Arina K Bones? Uli Schimmack? John Ioannidis?
It has even been suggested that the current ``publish or perish'' reward structure of academia encourages capitalization on Type I error, encouraging researchers to publish many studies with poor predictive value rather than publish few studies with substantial predictive power \citep{Bakker:etal:2011}. In this light, one might expect the possibility of research bias in violent video games research, as in many other fields of research. 

% What is publication bias and small-study effects?
Two processess may contribute to such research bias. The first, publication bias, is the phenomenon that studies with statistically significant (i.e., $p<.05$) findings are more likely to be submitted and accepted for publication. Publication bias is a problem that contributes to the overestimation of effect sizes and the propagation of Type I error. It is an especially pernicious problem for meta-analysis, as the selective reporting of studies that ``work'' (i.e., attain significance) leads to an overestimated effect size and may lead to conclusions of statistically significant effects when there are none. The error introduced by publication bias is larger when research studies are underpowered, as only the studies that overestimate the effect obtain statistically significant effect size estimates.

The other process is called by many names: flexible analysis, questionable research practices, $p$-hacking. These names refer to biased research practices that increase the likelihood of finding significant effects by increasing Type I error rates. One such practice is the inspection of many statistical tests and the presentation of only the significant ones. For example, one might collect several study outcomes but report only the one that showed significant differences, censoring the non-significant outcomes from report. Similarly, one might collect several treatment conditions but censor from report those conditions whose outcomes do not support the hypothesis. One could go ``moderator munging'', exploring several moderators until one obtains a significant interaction. Covariates could be added or removed from the model until the desired relationship becomes significant. Observations might be labeled as outliers and excluded not for their leverage, but for whether they support or oppose the hypothesized relationship. One particularly subtle form of flexible analysis is ``sampling to a foregone conclusion,'' in which the $p$-value is repeatedly inspected and additional data is collected until the $p$-value reaches the necessary threshold. While such sequential analyses can be appropriate and efficient in preregistered research plans, they have historically been used in an {\em ad hoc} fashion that inflates Type I error rates and effect size estimates.
% citation worthwhile? http://onlinelibrary.wiley.com/doi/10.1002/0471667196.ess5042.pub2/full

Because these two problems are typical in research, many meta-analytic techniques have been developed to detect and adjust for research bias. The application of such techniques are a vital part of meta-analytic practice. Additionally, because new techniques are continuously being developed, each promising potential improvements in accuracy, it may be helpful to revisit previous meta-analyses and apply new techniques for detecting and adjusting for publication bias \citep{Lakens:etal:inpress}. %Last sentence may not be necessary

In the \citet{Anderson:etal:2010} meta-analysis, the authors applied one popular technique, the trim-and-fill procedure, to suggest bias-adjusted effect size estimates. This procedure yielded minimally-adjusted estimates, suggesting minimal bias. However, there are other ways to adjust for bias in meta-analysis. In the following section, we review some meta-analytic techniques for detecting and adjusting for bias, describing their properties, strengths, and weaknesses.

% I don't think fail-safe N is worth talking about anymore?
%\subsubsection{Fail-safe N}
%Fail-safe N, in theory, provides the number of studies %(subjects?)
%that would need to be hypothetically hiding in file-drawers in order to reduce an effect to non-significance. This technique is flawed in that it assumes an effect size of exactly zero in all censored studies, does not consider the possibility of flexible analysis (e.g. p-hacking), and does not provide an estimate of a bias-adjusted effect size. Application of fail-safe N often reports an unrealistic number of failed studies even in the presence of small effects and clear research bias. %citation needed
%It also seems flawed in that the larger and more biased a research literature, the more significant the meta-analytic effect will appear, and so the fail-safe N would be expected to increase, not decrease, as a function of publication bias. Thus, fail-safe N does not appear to have a straightforward interpretation as the robustness of an effect.

\subsubsection{Egger's regression test}
One simple test for research bias is Egger's regression test \citep{Egger:1997}. This test inspects the relationship between effect size and precision (or sample size) in reported studies. Because sample size does not typically cause effect size, an unbiased research literature is expected to have no relationship between effect size and precision. However, if studies must attain statistical significance to be published, such a relationship will be observed. Small-sample studies require large observed effect sizes to reach statistical significance, while large-sample studies can reach statistical significance with smaller observed effect sizes. Thus, in the presence of publication bias, there is an inverse relationship between effect size and precision. Egger's regression test inspects the degree and statistical significance of this relationship.

Note that, in some cases, sample size and effect size may be correlated for reasons other than bias. For example, experimental research tends to have smaller samples than correlational research and may reflect different true effect sizes. Alternatively, it may be possible that manipulations and measurements in small samples are more effective than in large samples. To represent these possibilities, a relationship between sample size and effect size is often called ``small-study effects'' rather than ``publication bias.'' Some of these possibilities can be excluded through practice; for example, conducting separate bias tests for correlational and experimental research can rule out paradigm as a potential cause of small-study effects.

One weakness of Egger's regression test is that, while it can detect bias, it does not suggest a bias-adjusted effect size. Thus, it is not possible to assess whether the meta-analytic estimate reflects a likely null value or some non-null but inflated value. The test has also been demonstrated to have poor statistical power. %Citation needed! Research needed!

Egger's regression test has been used repeatedly by skeptics to look for publication bias \citep[e.g.,][]{Ferguson:2007,Ferguson:Kilburn:2009}, but was not reported in the \citet{Anderson:etal:2010} meta-analysis. Thus, while Anderson and colleagues argue that the trim-and-fill estimates suggest minimal small-study effects, it is possible that an Egger's regression test might have detected some such effects.

\subsubsection{Funnel plots}
Because research bias is one potential cause of small-study effects, it is often useful to visually inspect meta-analytic data for small-study effects. The relationship between observed effect size and precision is often represented for this purpose in a funnel plot. In a funnel plot, effect size is plotted on the x-axis and precision on the y-axis. In the absence of small-study effects or heterogeneity, study results will form a symmetrical funnel shape, displaying substantial variance when sampling error is large but narrowing to a precise estimate when sampling error is small. Thus, when research is uncontaminated bias, some small-sample studies are expected to find null or even negative results due to sampling error. The funnel should fill evenly.

However, when there are small-study effects, the funnel plot is no longer symmetrical. In the case of publication bias, studies are missing from the lower portion of the funnel where results would not be statistically significant. Funnel-plot asymmetry can also be caused by flexible analysis and reporting. When samples are collected until a desired $p$-value attained, studies will move up and to the right of the funnel. When subgroups or experimental subgroups are dropped from report to highlight only a subgroup in which statistical significance was found, studies will move down and to the right. When outcomes are censored from report to highlight only the significant outcomes, studies will move to the right of the funnel.

Again, funnel plots have been presented by skeptics \citep[e.g.,][]{Ferguson:2007}, but the \citet{Anderson:etal:2010} meta-analysis did not provide any funnel plots. This makes it difficult for readers to appraise the strength of the data, inspect the distribution of study results, and determine whether the naive and trim-and-fill effect size estimates might be influenced by outliers.

% Trim-and-fill and how it's bad
\subsubsection{Trim and fill}
Another popular bias-adjustment technique, trim-and-fill \citep{Duval:Tweedie:2000}, attempts to detect and adjust for bias through inspection of the number of studies with extreme effect size estimates on either side of the meta-analytic mean estimate. Thus, if the funnel plot is asymmetrical, with many more highly-positive effects than null or negative effects, the procedure ``trims'' off the most extreme study and imputes a hypothetical censored study reflected around the funnel plot's axis of symmetry. Studies are trimmed and filled in this manner until the ranks are roughly equal. 

However intuitive, this is not an effective adjustment for bias, as the assumptions of trim-and-fill are unlikely to be met. Studies are not likely to be censored on the basis of the effect size, but rather, on the basis of their statistical significance. Accordingly, it is argued that trim-and-fill does a poor job of providing an adjusted effect size, adjusting too much when there is no bias and adjusting too little when there is bias \citep{Lakens:2014,Simonsohn:etal:2014b}. %These are the data colada blog post and laken's comment blog post
%(c.f. Duvall \& Tweedie, 2000, who argue that suppression via $p$-value is too simplistic and that there is little difference between the two)
Others are skeptical of trim-and-fill's imputation of studies. %citation needed?
Thus, trim-and-fill is most commonly suggested as a form of sensitivity analysis rather than a serious estimate of the unbiased effect size. When the naive meta-analytic estimate and the trim-and-fill-adjusted estimate differ only slightly, it is suggested that the research is largely unbiased.
\citet{Anderson:etal:2010} applied trim and fill in their meta-analysis as the only attempt to detect and adjust for small-study effects. Trim-and-fill yielded only slightly-adjusted effect sizes, and so the authors concluded minimal research bias.  %Although we are confused by their decision to divide by culture and not to perform trim-and-fill on the total sample -- the smaller the sample, the poorer the power to detect asymmetry, perhaps?
Some have characterized this as an extensive test for publication bias \citep[][pg. 51]{Bushman:Huesmann:2014} despite the weaknesses of the trim and fill procedure and the absence of funnel plots.

\subsubsection{PET-PEESE meta-regression}
A promising new tool in the detection of and adjustment for bias is meta-regression. Like Egger's test, meta-regression techniques for publication bias consider the relationship between effect size and precision. Under publication bias, larger samples yield smaller effects. Again, because sample size does not typically cause effect size, such a relationship between sample size and effect size suggests that studies were censored when not attaining statistical significance or that studies were flexibly analyzed in order to attain statistical significance. 

PET-PEESE meta-regression \citep{Stanley:Doucouliagos:2013} %See carter & McCullough for better citations
uses the relationship between precision and effect size to estimate the underlying effect. It does this in two steps: Precision-Effect Test (PET) and Precision-Effect Estimate with Standard Error (PEESE). 

In PET, a weighted {\em linear} regression is fit to describe the relationship between effect size and precision, then extrapolates to estimate what the ``true effect'' would be in a hypothetical study with perfect precision. This true effect corresponds to the estimated intercept in the metaregression equation describing effect size as a function of precision. That is, the intercept represents the estimated effect size after partialing out the linear effect of sample size on effect size.

When there is no true effect, published studies tend to lie on the boundary between statistical significance and nonsignificance, forming a linear relationship between sample size and precision. Thus, PET performs well at estimating effects when the null hypothesis is roughly true. However, when there is a true effect, small studies will be censored by publication bias, but most large studies will find statistical significance and be unaffected by bias. 

% Not sure how I feel about this given PET's questionable statistical power
Because PET assumes a constant level of publication bias throughout the funnel, it will underestimate the magnitude of a true nonzero effect.

Therefore, when the PET-estimated effect size is significantly different from zero, one is advised to move to the second step, PEESE. PEESE fits a weighted {\em quadratic} relationship between effect size and precision. The resulting curve models bias as being stronger in the lower part of the funnel but reduced as the studies become better-powered and less subject to bias. Because PEESE will overestimate the effect size when the null is true, it is suggested that the PEESE estimate be considered only if the PET estimate is statistically significantly different from zero. However, the statistical power of PET to detect an effect is unknown, and may be quite poor for sample sizes and effect sizes typical of psychology \citet{blogposts}. Given that a nonsignificant test result does not imply the truth of the null hypothesis, we are reluctant to privilege PET over PEESE. Thus, the present manuscript reports both PET and PEESE estimates for all meta-regressions. Readers are advised that if the null hypothesis is roughly true, PEESE will overestimate the true effect size, but that if the null hypothesis is false, PET will underestimate the true effect size.

% Desperately need to expand on this section
The efficacy of PET-PEESE metaregression is supported by a simulation study by \citet{Moreno:etal:2006}, 

This meta-regression technique has been previously applied by \citet{Carter:McCullough:2014} to inspect the amount of evidence for ``ego depletion,'' the phenomenon of fatigue in self-control. They found that after adjusting for small-study effects, PET-PEESE suggested an absence of evidence for the phenomenon. The authors therefore recommended a large-sample pre-registered replication effort, now supported by the American Psychological Society as the topic of the third Registered Replication Report (http://www.psychologicalscience.org/index.php/publications/observer/obsonline/aps-announces-third-replication-project.html).

% Explaining that I'm basically using Peters
One criticism of the Egger and PET-PEESE metaregression tests is that some effect size estimates have an inherent relationship between precision and effect size that is not caused by research bias. For example, given a single sample size, the precision of Cohen's $d$ increases as the effect size $d$ increases. A similar phenomenon holds for odds ratio. When these effect sizes are used, metaregression techniques risk misidentifying the inherent relationship between precision and effect size for a small-study effect. To avoid this problem, it has been suggested that one instead use precision estimates that are a function of the sample size alone. In the current report, we use as our effect size estimate Fisher's z with standard error $\frac{1}{\sqrt{N-3}}$. Because this standard error is not a function of the effect size, we avoid the problem of an inherent relationship between precision and effect size that might otherwise contaminate the metaregression.

\subsubsection{$p$-Curve}
Another novel technique for accounting for small-study effects is $p$-curve \citep{Simonsohn:etal:2014}. $p$-curve estimates the true effect size by inspecting the distribution of significant $p$-values. When the null hypothesis is true (i.e. $\delta$ = 0), the $p$-curve is flat: significant $p$-values are as likely to be between .00 and .01 as they are between .04 and .05. When the null hypothesis is false, the $p$-curve becomes right-skewed such that $p$-values between .00 and .01 are more common than $p$-values between .04 and .05. The degree of right skew is proportionate to the power of studies to detect an effect, such that increasing sample sizes or larger true effect sizes will yield greater degrees of right skew. By considering the $p$-values and sample sizes of significant studies, $p$-curve can be used to generate a maximum-likelihood estimate of the true effect size.

One weakness of $p$-curve is that, in the presence of questionable research practices such as sequential data analysis, conditional use of covariates, motivated treatment of outliers, and moderator munging, an excess of $p$-values will gather close to the $p$ = .05 threshold. This results in a flatter $p$-curve than would be found in more principled analysis, and thus $p$-curve will underestimate the true effect size in these circumstances.

\section{Methods}
We apply PET-PEESE meta-regression and $p$-curve effect size estimation to the \citet{Anderson:etal:2010} meta-analysis, using the meta-analytic data provided by those authors.\footnote{Since the publication of the \citet{Anderson:etal:2010} meta-analysis, a second meta-analysis has been published summarizing research published between 2009 and 2014 \citet{Greitemeyer:Mugge:2014}. We had originally planned to include this meta-analysis in the present manuscript, but in the course of our research, found a number of errors. These authors are currently working to correct their meta-analysis, at which time we will apply these techniques to that research as well.} 
Because the data were analyzed using Comprehensive Meta-Analysis with the intent of testing for moderators, many studies were entered with separate rows for different outcomes or subsamples within studies. However, our current models assume that entire studies are censored or re-analysed and thus that each study should constitute a single observation. In the event that multiple effect sizes were available for a particular study (e.g., effects on mean intensity and count of high intensity trials in the CRTT; separate simple effects for men and women), we aggregated these to form a single effect size for the study. For effects representing separate outcomes within a single sample, the outcomes were averaged. For effects representing separate subsamples within a study, the sample sizes were summed and a weighted average made of the subsample effect sizes. This parallels the behavior of the Comprehensive Meta-Analysis program used by \citet{Anderson:etal:2010}. $p$-values were calculated via $t$-test, first dividing Fisher's Z scores by their standard errors to generate a $t$-value, then using that $t$-value to get a two-tailed $p$-value.
%We consulted with the original authors as how best to aggregate rows within studies and reproduce their provided estimates. % I still need to email Anderson about this

We then followed the PET-PEESE procedure, fitting a weighted-least-squares regression model predicting effect size as a linear function of the standard error with weights inversely proportional to the square of the standard error. PEESE was also applied, predicting effect size as a quadratic function of the standard error. $p$-curve effect size estimates were generated using code provided by \citet{Simonsohn:etal:2014}, entering a $t$-value and degrees of freedom parameter for each relevant study.

PET or PEESE estimates are provided regardless of whether statistically significant bias was observed according to recommendations by \citet[p. 20-21]{Stanley:Doucouliagos:2013}: ``To be conservative, one should always use [the PET or PEESE estimate] even if there is insufficient evidence of publication selection because the Egger test [of publication bias] is known to have low power.'' Furthermore, simulations have suggested that the conditional application of meta-regression corrections (that is, applying them only when tests of bias attain statistical significance) tends to perform poorly compared to the unconditional application of such corrections, as a nonsignificant test result does not necessarily constitute firm evidence against publication bias \citep{Moreno:etal:2006}.

Within the analyses, all effect sizes were converted to Fischer's z so that they were normally distributed so as to fulfill the assumptions of the regression model. Effect sizes are converted back to Pearson $r$ for discussion. All meta-regressions were performed using the `metafor' package for {\bf R} \citep{Viechtbauer:2010}, using the {\tt rma()} function to fit a variance-weighted model with an additive error term.

Because PET-PEESE is a regression method, it is likely to perform poorly when there are few datapoints. Therefore, our analysis is restricted to effects and experimental paradigms with at least ten independent effect sizes. %citation needed from Carter & McCullough, 2014
Data and code have been made available online in the case that the reader nevertheless wants to generate PET-PEESE estimates for more sparse datasets or explore the impact of our inclusion and exclusion decisions.

In addition to our analysis of the full dataset as provided by Anderson and colleagues, we perform leave-one-out sensitivity analyses, removing each datapoint one at a time and refitting the PET-PEESE model. For each analysis, a supplementary tab-delimited spreadsheet is attached that lists the individual studies and the PET-PEESE coefficients and $p$-values when they are left out. \footnote{Initially, we had attempted a different sensitivity analysis in which we removed datapoints with a Cook's distance of more than 0.5 on the PET regression. In the case that several observations were excessively influential, we performed an iterative procedure, deleting the single most influential observation and checking again for influence until no observations had excessive influence. In practice, this tended to delete all datapoints that did not fit the PET regression well. This seemed unfair, excessively favoring the PET model over the available data.}

Two studies were removed from the meta-analysis in all analyses. First, \citet[study 1]{Matsuzaki:etal:2006} was removed because its entered effect sizes were unusually large for their precision (i.e., effects on aggressive behavior $r = .60$ and aggressive cognition $r = .53$), were highly influential on the meta-regression model, and most importantly could not be found as entered in the \citet{Anderson:etal:2010} dataset by inspection of the original article. % I have reached out to Anderson for comment on this.
Similarly, \citet{Panee:Ballard:2002} was removed because the study tested the effects of violent primes during gameplay and not the effects of violent gameplay itself; therefore, it does not provide a relevant test of the hypothesis. %might need to follow up on this

% Anderson et al. It would be nice to provide trim-and-fill estimates, maybe?
We reproduce estimates from \citet{Anderson:etal:2010} and apply $p$-curve effect size estimation and PET-PEESE metaregression to detect and adjust for small-study effects. Sufficient datapoints were available to re-analyze experimental studies of aggressive affect, aggressive behavior, aggressive cognition, and physiological arousal, as well as cross-sectional studies of aggressive affect, aggressive behavior, and aggressive cognition. Studies are further divided to create separate best-practices-only and all-studies estimates per \citet{Anderson:etal:2010} as sample sizes permit. 

% This is no longer accurate, at least until I hear from these two.
%We also attempt to reproduce estimates from \citet{Greitemeyer:Mugge:2014}. Effect sizes are inspected separately for aggressive affect, aggressive behavior, and aggressive cognition in experimental and cross-sectional research designs.

\section{Results}
% The whole results section really needs all the p-values and junk!
% Some useful stuff is in PETPEESE_results.xlsx but much of it is outdated (lm() instead of rma()).
Results for all performed meta-regressions are summarized in Table \ref{table:PETPEESE}. 
Funnel plots with overlaid PET-PEESE regression lines and curves are provided in Figure \ref{figure:funnelPlots}. We note that visual inspection of the funnel plot often reveals clear asymmetry, particularly in those subsets of studies that \citet{Anderson:etal:2010} selected as ``best-practices'' studies.
Below, we discuss these statistics and describe the results of sensitivity analyses.

%CIs would be nice
\subsection{Aggressive Affect}
\subsubsection{Experiments}
\paragraph{$p$-curve}
Among studies selected as best-practices, $p$-curve estimated the true effect size as $r = .16$, substantially smaller than the original naive estimate of $r = .29$. Among the full sample of best- and not-best studies, the estimate was again $r = .16$. Leave-one-out sensitivity analyses are presented in supplementary table XXX. 

\paragraph{PET-PEESE}
Among studies selected as best-practices, PET found no significant effect of violent games on aggressive affect. Small-study effects were clear ($p_{Egger} <.001$). The estimated effect size was $r = -.12, p = .198$.

When including both best- and not-best-practices studies, PET still found no significant effect of violent games on aggressive affect. Small-study effects were again clear ($p_{Egger} < .001$). The estimated effect size was $r = -.11, p = .055$.

In sensitivity analysis, it became apparent that one study \citep{Ballard:Weist:1996} had substantial influence over the meta-regression line. After removing this study, PET estimates were still not significant whether for best-practices or all studies. After this exclusion, the estimated effect size was almost exactly zero and small-study effects were still apparent (best-practices subsample: $r = -.01$, $p = .888$, $p_{Egger} = .002$; all studies: $r = -.05$, $p = .365$, $p_{Egger} < .001$). For the full spreadsheet of leave-one-out sensitivity analysis, consult supplementary file XXX.

\subsubsection{Cross-sectional research}
Insufficient studies were selected to conduct separate best-practices and full-sample analyses, so only the full sample was analyzed. $p$-curve estimate

PET-PEESE suggested that effects were significant ($p < .001$), contaminated by small-study effects ($p_{Egger} = .049$), and slightly adjusted from the naive estimate ($r = .14$).

In sensitivity analysis, it was found that several of the studies had substantial influence over the PET-PEESE model. The most influential of these was \citet{Uozumi:2006}; excluding this study caused the PET estimate to fall to nonsignificance and the effect size to be estimated as $r = .05$. Other influential observations (and the estimated effect size after their exclusion) included \citet[study 2, $r = .13$]{Matsuzaki:etal:2004}, and \citet[$r = .16$]{Yukawa:Sakamoto:2001}.

\subsection{Aggressive Behavior}
\subsubsection{Experiments}
Among studies selected as best-practices by \citet{Anderson:etal:2010}, significant small-study effects were detected ($p_{Egger}$ = .007), but a significant effect was not ($p$ = .126). PET estimated the effect as $r = .081$, substantially smaller than that of the naive or trim-and-fill estimates. No studies were observed to have substantial influence on the effect size estimate; at most, exclusion of \citet{Anderson:etal:2007} raised the estimate to $r = .11$.

Among all studies, PET found a significant effect of violent games on aggressive behavior ($p = .001$) and no significant small-study effects ($p_{Egger} = .322$). PEESE estimated the effect as $r = .157$, again smaller than that of the naive or trim-and-fill estimates. Again, no studies were found to be particularly influential, with $r$ ranging from $.15$ to $.17$.

\subsubsection{Cross-sectional research}
Cross-sectional associations were more robust and less contaminated by small-study effects. Among studies selected as best-practices, the PET estimate was significant ($p < .001$), and although small-study effects were detected ($p_{Egger} = .02$), PEESE recommended minimal adjustment ($r = .26$). Sensitivity analysis indicated that the estimate was largely robust to the inclusion or exclusion of single studies, with $r$ remaining between $.25$ and $.27$. 

Among the set of all cross-sectional studies, the PET estimate was again significant ($p < .001$), and PEESE recommended an only slightly-adjusted effect size ($r = .19$). Again, small-study effects were detected ($p_{Egger} < .001$). Sensitivity analysis suggested a number of influential studies, but even so, leave-one-out effect size estimates did not vary much, ranging from $r = .18$ to $r = .21$.

\subsection{Aggressive Cognition}
\subsubsection{Experiments}
Among experiments selected as best-practices, PET found neither a significant effect of violent games ($p = .055$) nor a significant small-study effect ($p_{Egger} = .086$). The effect size estimate was $r = .11$, much smaller than the naive estimate of $r = .22$. Because the $p$-value is very close to the critical threshold, one might consider the PEESE-adjusted effect size estimate of $r = .18$. Because the effect was very near significance, sensitivity analysis suggested some rather variable estimates, as the removal of a single study could cause the $p$-value to cross the significance threshold. For example, exclusion of \citet{Bushman:Anderson:2009} caused PET to reach significance, leading to a PEESE estimate of $r = .19$. In the other direction, exclusion of \citet{Anderson:Dill:2000} caused the effect size estimate to fall to $r = .06$.

Among all studies, PET found a significant effect of violent games on aggressive cognitions ($p = .003$) and no significant small-study effects ($p_{Egger} = .111$). PEESE estimated the effect as $r = .18$, again smaller than the naive or trim-and-fill estimates. Leave-one-out analysis did not detect much variability in estimates, with $r$ ranging from $.16$ to $.19$. 

\subsubsection{Cross-sectional research}
Among cross-sectional research selected as best-practices, PET found a significant effect of violent games ($p = .001$) and significant small-study effects ($p_{Egger} = .013$). The PEESE-adjusted effect size estimate was $r = .15$, slightly smaller than the naive estimate. Sensitivity analysis detected some variability in effect size estimates. Exclusion of \citet{Yukawa:Sakamoto:2001} caused the estimate to rise to $r = .17$, while exclusion of \citet{Funk:etal:2003} caused the PEESE estimate to fall to $r = .13$. When \citet{Anderson:etal:2004} was excluded, the PET estimate fell sharply, no longer reaching statistical significance and recommending $r = .06$.

Among the full sample of cross-sectional studies, PET again found significant associations with violent games ($p = .005$) and effects of sample size ($p_{Egger} < .001$). PEESE recommended an adjusted effect size of $r = .13$, moderately smaller than the naive estimate. Sensitivity analyses indicated two particularly influential observations: exclusion of \citet{Santisteban:etal:2007} caused the estimate to rise to $r = .15$, whereas exclusion of \citet{Funk:etal:2003} caused the PET estimate to no longer reach significance, yielding an estimated effect size of just $r = .04$.

\subsection{Physiological Arousal}
\subsubsection{Experiments}
Among the subset of best-practices experiments, PET detected neither an effect of violent games ($p = .227$) nor an effect of small studies ($p_{Egger} = .466$). PET recommended an adjusted effect size of $r = .13$, substantially smaller than the $r = .20$ estimate given by naive meta-analysis. Results in sensitivity analysis were quite variable, as might be expected of the small number of observations: estimates varied from $r = .08$ to $r = .27$.

In the total sample of all experiments, PET did not detect an effect of violent games ($p = .942$) but did detect small-study effects ($p_{Egger} = .039$). The PET estimate of the effect size was $r = -.01$. Sensitivity analysis revealed minimal influence from individual studies, with the estimated effect ranging from $r = -.02$ to $r = .02$.

% Discussion section is waaaay too long.
% It would also benefit from more rigorous organization via subheadings.
\section{Discussion}
Our findings suggest that the effects of violent video games in experimental research have been overestimated in the \citet{Anderson:etal:2010} meta-analysis, particularly among those studies selected as meeting best-practices criteria. After accounting for the relationship between sample size and effect size, we estimate that the best-practices experiments lack evidentiary value. The estimated effects on aggressive affect, aggressive cognitions, aggressive behaviors, and even physiological arousal were all very small and did not reach statistical significance in the PET-PEESE procedure. By contrast, when not restricting analsysis to best-practices sudies, we find some evidence for effects on aggressive behavior and cognitions. 

Correlational studies demonstrated greater evidence of association and less evidence of small-study effects on all aggressive outcomes. However, correlational research cannot speak to a causal association between violent games and aggressive outcomes. 

% This is a big sentence -- chop it up.
These PET-PEESE estimates stand in contrast to trim-and-fill results reported by \citet{Anderson:etal:2010}. In that report, trim-and-fill failed to detect bias in best-practices experiments of aggressive affect, suggested $r = .18$ for Western best-practices experiments of aggressive behavior, and adjusted best-practices experiments of aggressive cognition only slightly to $r = .20$. Again, it is suspected that trim-and-fill does a poorer job of adjusting for bias than does PET-PEESE, as results are more likely suppressed from publication or selection by $p$-value than by effect size. % Simonsohn blog post? Lakens blog post?

Contrary to the findings of \citet{Anderson:etal:2010}, we find that effects are equal or larger in not-best-practices experiments than in best-practices experiments after adjusting for small-study effects. It is possible that application of the best-practices criteria was flexible or otherwise flawed. If studies are included or excluded from the best-practices sample on the basis of their results, this selection bias would introduce further small-study effects. These small-study effects would then reduce the PET-PEESE estimates of the effect in the best-practices sample relative to the complete collection of studies.

The present meta-analysis is not conclusive evidence that the true effect size is so small. First, meta-analytic adjustments for research bias are imperfect and may not always provide a good estimation of the effect size. Second, flexibility in analysis or study selection may increase funnel plot asymmetry, leading to an {\em under}estimation of the effect. %Could cite Simonsohn, he said a similar thing about p-curve.
It is possible that there is some effect of violent video games which has been obscured by flexibility in the analysis of individual studies or by application of study inclusion criteria, particularly the best-practices inclusion criteria. These estimates must also be considered in light fo the limitations of PET-PEESE, described later.

Instead, the current results indicate potential problems in meta-analysis which may include publication bias, selection bias, and flexible analysis. As such, we cannot be sure of causal effects of violent video games on aggressive outcomes, as the available data do not provide a fair and rigorous test of the hypothesis. We recommend the use of large, pre-registered, open-data research projects, ideally with collaboration across neutral and even antagonistic research teams. 

On the topic of scientific transparency, we note that the clear and accessible archival of meta-analytic data is a considerable aid to research transparency. We commend Anderson and colleagues for sharing the data and for responding to questions as to how best reproduce their analyses. We suggest that future meta-analyses routinely include the data, funnel plots (in supplemental materials, if need be), and other supplementary materials. Meta-analyses that cannot be personally inspected or reproduced should be regarded with caution.

\subsection{Limitations of PET-PEESE}
That said, PET-PEESE meta-regression is a relatively new technique and its limitations may not yet be fully understood. For these reasons, we suggested that the provided estimates be considered possible bias-adjusted estimates rather than corrected estimates of the true effect. Also, PET-PEESE is a regression method, and like most regression methods, results can be misleading when few datapoints are available. PET-PEEESE also extrapolates outside the model, estimating the effect when standard error is zero when no datapoints have zero error. Such extrapolation can be misleading in regression. 
Furthermore, critics of PET-PEESE argue that, while it is unbiased, it is also inefficient, and so the results of any single PET-PEESE meta-regression may be quite inaccurate, even if many meta-regressions would perform well in the long run \citep{Reed:etal:WORKINGPAPER}. A similar simulation argues for the application of PET-PEESE despite observing similar variability and even a slight downward bias \citep{Moreno:etal:206}. These simulations, however, involve larger true effect sizes and sample sizes than are often found in psychology and less asymmetrical funnel plots than are found in the present study. When true effects are small (e.g. $\delta \le 0.5$), PET-PEESE still seems to perform fairly well. One might also reconcile PET-PEESE's slight downward bias with trim-and-fill's substantial upward bias, considering an adjusted estimate somewhere in between.

Another criticism of meta-regression is that small-study effects may be caused by phenomena besides publication bias or $p$-hacking. For example, a small survey might measure aggressive behavior thoroughly, with many questions, whereas a large survey can only afford to spare one or two questions. Similarly, sample sizes in experiments may be smaller, and effect sizes larger, than in cross-sectional surveys. The current report is able to partly address this concern by following the original authors' decision to analyze experimental and cross-sectional research separately.

However, the point is well taken that correcting for bias is something of a fool's errand.
Thus, rather than regard the present meta-analysis as having identified the true effect size, we instead interpret it as a warning sign that not all is right in scientific report. The observed funnel-plot asymmetry would be extremely unlikely if the research process in this field were unbiased and transparent. Thus, we call for increased transparency in the research process. 
%Preregistration, antagonistic collaboration, data-sharing, Simmons et al's 21 words, independent direct replication, large sample sizes, sensitivity analysis. 

% What went wrong with Anderson 2010?
% Practical considerations for meta-analysis / reducing bias in meta-analysis
\subsection{Selection Bias in Meta-Analysis} 
We observe some instances of flexible application of the best-practices criteria offered by \citet{Anderson:etal:2010}. Flexible application of the inclusion criteria may have lead to preferential selection of studies with significant results. This selection bias could explain why the best-practices studies had larger naive effect-size estimates but smaller PET-PEESE estimates.

$p$-curve estimates very similar effect sizes for both best-practices and all-studies samples. Recall that $p$-curve inspects only the studies that attained statistical significance. Inspection of the funnel plots reveals that the studies selected as best-practices are generally those studies attaining statistical significance; therefore, the studies considered by $p$-curve are mostly the same across the two samples.

%``Meta-analyses are known to suffer from the `junk in / junk out' phenomenon (which is unlikely to be fixed by `best-practices' efforts, when scholars may simply value their own junk higher than the junk of others.)'' \citep{Ferguson:Heene:2012} %I'm mad at Ferguson and don't want to give him the satisfaction of the citation right now.
\subsubsection{Content validity}
The first criterion is that the violent and nonviolent game must be sufficiently different in violent content. Application of this criterion was not consistent. In some cases, studies were excluded for having nonviolent games that contained very mild cartoon violence, while in others, nonviolent games containing substantial violence were included. Comparisons between the violent game {\em Mortal Kombat} and the nonviolent game {\em Sonic the Hedgehog} were discarded as not-best practices \citep[e.g.,][]{Cohn:1995; Hoffman:1994} because ``the nonviolent game contained violence'' \citep[supplementary materials]{Anderson:etal:2010}. Another study comparing a racing game {\em Moto Racer} against the violent game {\em Tekken 2} \citep{Brooks:2000} was excluded for similar reasons, but we were not able to find any violent content in {\em Moto Racer}. Meanwhile, other studies involving comparisons between violent and not-entirely-nonviolent games were included. \citet{Konijn:etal:2007} was included, although it used the game {\em Final Fantasy} as a nonviolent game. {\em Final Fantasy} appears to be as violent, or more violent, than {\em Sonic the Hedgehog}, so the simultaneous inclusion of this paradigm and exclusion of the {\em Sonic the Hedgehog} paradigm indicates inconsistency in the application of this criterion. Similarly, a study by \citet{Brady:Mathews:2006} was included as best-practices despite comparing the violent {\em Grand Theft Auto 3} to the purportedly-nonviolent game {\em Simpsons Hit and Run}. While lighter in tone and less explicit than {\em Grand Theft Auto 3}, {\em Simpsons Hit and Run} nonetheless allows the player to punch other characters, steal cars, and run over pedestrians. This content lead video game ratings boards to assign {\em Simpsons Hit and Run} a rating as appropriate for teens, not children. Thus, again, the application of this criterion seems much stricter for studies with significant results than for studies with nonsignificant results.
% Regarding Mathews 2006 vs. Brady & Mathews 2006, Craig says: "As I recall, the dissertation study was the same as the B & M 2006 paper. So, there is no real contradiction here, other than that the dissertation should have been listed differently in the supplemental materials." I'll have to check the estimates and ks.

Flexibility in the application of this criterion may have contributed to selection biases, inflating the naive meta-analytic estimate while reducing the PET-PEESE estimate. A better approach might be to have manipulations rated by research assistants naive to hypotheses or to study results, or to seek a statistical quantification of the difference in violence between games, such as a Cohen's $d$ describing a manipulation check.

\subsubsection{Measurement quality}
Selection bias may also have been facilitated by the application of best-practices criterion 5: The outcome measure could reasonably be expected to be influenced by the independent variable if the hypothesis were true. For an example of selection bias, see \citet[study 2]{Anderson:etal:2004}. In this study, participants were assigned to play a violent or nonviolent game, then complete a competitive reaction-time task measure of aggressive behavior with either an ambiguously or unambiguously provoking confederate. A significant effect was found amount the 90  subjects assigned to the ambiguous provocation condition ($r = .25$), but not among the 90 subjects assigned to the unambiguous provocation condition ($r = -.03$). These 90 subjects with a nonsignificant effect were dropped from both the best-practices and not-best-practices meta-analyses. 

When asked for comment, Anderson said ``Only the ambiguous provocation condition was used because we now know that the unambiguous (increasing) provocation version of the task is not as sensitive to a variety of independent variables as is the ambiguous provocation pattern. In other words, the increasing provocation conditions don't meet Criterion 5.'' While it is possible that only one form of the task is sensitive to the manipulation, the meta-analysis does not seek to model such fine-grained moderators; at the least, the full sample should have been included in the full-sample meta-analysis. We also suspect that the validity or invalidity of measurements cannot be determined on whether they provide the researcher with the desired $p<.05$ in an experiment. Finally, since a significant effect in either the ambiguous or unambiguous provocation group would be taken as evidence for an effect of violent video games, we are concerned that the selective exclusion of groups for not demonstrating such an effect risks introducing selection bias.

%  Selection bias in choice of effect sizes
% Here's the quote from Anderson et al (p. 158):
% For studies that reported multiple effects on the same conceptual outcome variable, we took one of two actions. In those cases where one measure was clearly better than the others, based on theoretical relevance (e.g., physical aggression is more relevant to violent video game effects than is verbal aggression), established validity (e.g., use of a well-validated multiple item measure of trait physical aggression vs. a new single item measure of trait aggression), or other empirical evidence offered in the study, we used the best measure. For example, if a study reported two new outcome measures of aggressive behavior, and only one of them correlated significantly with a third variable known to be related to physical aggression (e.g., trait irritability), we used that measure (e.g., Anderson & Dill, 2000, Study 2). In those cases in which there was not a clear best measure, we used the average effect size (Bartholow, Bushman, & Sestir, 2006). Note that we also repeated the main analyses, always using the average effect, and found essentially the same results. 
Selection bias may also influence which effect size among those reported was entered into analysis. As a general rule, it seems that \citet{Anderson:etal:2010} attempted to avoid subjectivity in effect size entry by averaging all reported effect sizes together. However, on several instances, effect sizes were not averaged together, but rather the single largest available effect size was selected. For example, in the aforementioned \citet[study 2]{Anderson:etal:2004}, the effect of violent games on the first trial of the CRTT was entered, but not the reported effect size on the other 24 trials of the CRTT. % insert effect sizes! This is not consistent with their above paragraph.
In another example, \citet[study 2]{Anderson:etal:2007} report effects of violent video games on violent behavior ($r = .35$), physical aggression ($r = .46$), and verbal aggression ($r = .25$). Only the largest of these was used as the effect size of violent games on aggressive behavior. % I guess this is consistent with their above paragraph, though.
Similarly, the effect of violent games on aggressive affect was entered as the effect on trait anger ($r = .23$) but not the effect on trait hostility ($r = .21$). Selection of the largest effects risks capitalizing on chance and systematically overestimating the true effect.

\subsubsection{Unfalsifiable predictions}
We note further selection bias in the interpretation of violent games on physiological arousal. As presented by \citet{Anderson:etal:2010}, violent games cause significant increases in physiological arousal, e.g. heart rate or blood pressure. However, in researching this meta-analysis, we became aware of studies in which null effects of violent games were excluded from meta-analysis. For example, in the best-practices studies by \citet{Carnagey:Anderson:2005}, the violent and nonviolent versions of the game were not found to effect players' physiological arousal. Rather than present these findings as null results of violent games on physiological arousal, the authors presented this result as evidence that the violent and nonviolent games were matched stimuli. We observe a similar treatment in the meta-analysis: the null results on physiological arousal were omitted from the meta-analysis investigating effects of violent games on physiological arousal. We find this approach to be too flexible and forbids falsification of the theory, as concordant results are taken as evidence for the theory, but discordant results are excluded from consideration.

In sum, it seems that the inclusion criteria were not effective in selecting an unbiased subset of best-practices studies. Instead, they may have provided some degrees of freedom with which studies with significant results could be included and studies with nonsignificant results excluded. % One more sentence to round this out?

\subsection{Possible Data-Entry Errors}
% Miscoded effect size in Carnagey & Anderson 2005?
% Did they run it again without the experimental condition, just looking at previous exposure and the outcome?? Ask Craig about this. Wouldn't expect it to change that much unless there was a failure of randomization...
Some null findings were censored or miscoded. %Censored? Do I have evidence of that?
For example, in the course of the experiment reported in \citet{Carnagey:Anderson:2005}, a nonexperimental assessment was also made of the effects of previous violent game exposure on aggressive outcomes. These nonexperimental effects were entered into the \citet{Anderson:etal:2010} meta-analysis, but were considerably changed from their report in the \citet{Carnagey:Anderson:2005} manuscript.  In the manuscript, nonsignificant effects of previous violent game exposure were reported for aggressive affect (study 1; $F(1, 66) = 0.78, r = -.11$), aggressive cognitions (study 2; $F(1, 57) = 0.02, r = .02$), and aggressive behavior (study 3; $F(1, 133) = 0.23, r = -.04$). However, as they appear in the meta-analysis, these were entered as two cross-sectional samples of violent game effects on aggressive behavior, this time with much larger effects than reported in the manuscript: $r = .33$ for studies 1 and 2 combined and $r = .23$ for study 3.


\subsection{Unpublished Materials}
% Does looking for unpublished studies help?
The \citep{Anderson:etal:2010} meta-analysis did make an attempt to collect and analyze unpublished studies (e.g., studies presented in dissertations or book chapters that did not undergo peer review). That the resulting analysis remained biased despite these attempts gives us concern that searching for unpublished studies may not actually alleviate bias in meta-analysis. 

% Maybe cut this paragraph of these two tripping over each other. But then, it is kind of funny.
Shortly after the publication of the \citet{Anderson:etal:2010} meta-analysis, there was some confusion as to the importance of unpublished research in meta-analysis. In a comment, \citet{Ferguson:Kilburn:2010} % "Much Ado About Nothing"
criticized the inclusion of unpublished research in the meta-analysis, arging that such work is sometimes of dubious quality. These authors further criticized the purportedly-selective inclusion of {\em not yet published} research, such as articles under review or in press, and publications not peer reviewed, such as book chapters. In their reply, \citet{Bushman:etal:2010} describe unpublished studies as ``studies not published in a peer-reviewed journal, although it could have been published in another outlet (e.g., book).'' While they quote a passage from \citet{Cooper:2009} stressing the importance of unpublished research as important to protection against confirmation bias and bias against the null hypothesis, the emphasis seemed to be nonetheless on book chapters and dissertations that were otherwise publicly available.

The unpublished research we are most often concerned about in meta-analysis are those studies that were conducted but never published in {\em any} form, whether journal article, dissertation, or book chapter. That is, we are concerned about ``publication'' in the most literal sense of {\em being made public.}  Because studies that do not yield significant effects are less likely to be submitted and accepted for publication, substantial parts of data may be missing from the scientific record. While \citet{Anderson:etal:2010} report having searched thoroughly for unpublished materials, we note that the meta-analysis contains almost entirely studies that were published in at least one form or another (e.g., book chapter or dissertation). Only two studies were found that were not published in any format. Given 20 years of research on a family of small effects, using small samples, it seems hard to believe that there would not be at least some unpublished studies languishing in file drawers.

One particularly interesting publication format is the doctoral dissertation. Department requirements generally dictate that dissertations be submitted and published in a dissertation database regardless of whether or not that dissertation is later published as a peer-reviewed journal article. Dissertations, then, provide us with a sample of reported studies relatively uncontaminated by publication biases favoring significant results. We count $XXX$ dissertations that did not later become journal articles. Effect sizes observed in $XXX/YYY$ of these dissertations are nonsignificant. Given the number of dissertations that did not go on to be published in journals, one might wonder how many non-dissertation studies have been similarly conducted but not made public. 

We note that few of these unpublished studies were accepted as best-practices research. Although we had hoped that the application of best-practices criteria would alleviate bias, recognizing well-performed research regardless of its results, it instead appears to have intensified bias. 

It is also possible that unpublished null findings were not available to be collected. Null findings are sometimes reanalyzed and massaged until they become positive research findings.



% Reducing bias in empirical research
\subsection{Improving Research Quality}
We need to restructure incentives so that failure to detect a significant effect is an accepted research outcome. %Obedient replication.
Part of the mess appears to be that researchers quickly took the basic phenomenon for granted and began to cast about for more sophisticated models that would advance theory. In some cases, these more sophisticated models led to attempted conceptual, rather than direct, replications. It has been pointed out that the results conceptual replications can be difficult to appraise: if significance is attained, the replication is considered a success, but if significance is not attained, the replication may be considered invalidated by the changes in its paradigm. In other cases, these more sophisticated models lead to the study of moderators and subgroups. When many moderators are tested, Type I error rates will rise substantially. Post-hoc exploratory analyses of moderators are, of course, important and valuable \citep[indeed, we present them ourselves in][]{Engelhardt:etal:2014}, but become hazardous when presented as confirmatory, or when patterns of statistical significance are taken to identify the validity or invalidity of the measures.

% Theoretical considerations
\subsection{Implications for Theory}
The estimated corrected effect is larger in less-controlled studies (cross-sectional vs experimental). Bias is also reduced in the cross-sectional research, as both sample sizes and the likely effect are both larger, more readily yielding statistical significance. 
% Barf at this section. Needs work.
We have wondered sometimes whether playing violent video games is itself a deviant behavior, as parents often prohibit it and polite society finds it unsavory. We also wonder at the confounding of game genre with game content, as many studies %CITATION NEEDED
measure violent game use not by measuring the violent content of games, but whether the games contain action or not.

We are also reluctant to apply PET-PEESE to longitudinal studies, as there are not enough data points to permit a good estimate. It is certainly possible, perhaps even likely, that there are small but noticeable longitudinal effects of hundreds of hours of gameplay over several years. This would seem more plausible than the prospect of a substantial and reliable effect obtained within fifteen to thirty minutes of gameplay. We echo the words of \citet[p. 51]{Bushman:Huesmann:2014}, ``In many ways it is quite impressive that playing a violent video game for just 15-30 min, on a single occasion can have significant and measurable effects on aggressive behavior.'' Our tone, however, is different.

We need to establish the existence of the phenomenon before attempting to elaborate on the effect, its moderators, and its broader implications. If the effect is indeed as small as we estimate here (r = .16) then identifying meaningful and reliable moderators of the effect will be challenging.  Moderators of the effect, if any, should be expected to have similarly small effects, and so may take impractically large samples to study. This may explain the counter-intuitive finding that effect sizes among children are not significantly larger than effects among adults: differences, if any, will be small, and are likely to be obscured by research bias. 

It may also be necessary to reevaluate our theories of aggressive behavior, specifically the General Aggression Model (GAM). According to GAM, aggressive feelings, thoughts, and behaviors are all closely related processes which feed into each other. Aggressive feelings are thought to inspire aggressive behavior, and aggressive thoughts increase the likelihood of aggressive behavior. The present analysis indicates that violent video games have minimal effect on aggressive feelings \citep[a finding paralleled by][]{Przybylski:etal:2014} but that, depending on inclusion criteria and the exclusion of influential data points, there may be some effect on aggressive thoughts and some smaller effect on aggressive behavior. If this is the case, changes in aggressive behavior caused by violent game exposure would seem to be more related to aggressive-thought accessibility than to aggressive feelings. In our own research, we have attempted to collect measures of aggressive thoughts, feelings, and behavior from participants within a single session and found them to correlate poorly \citep{Engelhardt:etal:2015}.

% This paragraph is way too pulpy. Take it down twelve notches.
We echo the astonishment registered by \citet[p. 62]{Warburton:2014}: Given the theories and evidence in the rest of social psychology and media psychology, ``Violent media can and must have some psychological impact on those who experience it, and probably does so via well-understood psychological processes.'' It is remarkable that, in the face of such strong theory, only middling and biased evidence could be obtained. 

We note that some theoretical justifications of violent video game effects have themselves come under scrutiny. For example, the idea of ``behavioral priming,'' e.g. that subliminaly activating a thought influences automatic behavior \citep{Bargh:etal:1996}, holds a substantial position in the General Aggression Model [CITATION NEEDED]. Observing or participating in video game violence, it is argued, activates aggressive thoughts, which then cause increased aggression in behavior, particularly automatic behavior. Moreover, it is hypothesized that repeated exposure to violent media could cause aggressive thoughts to be chronically primed [CITATION NEEDED], a hypothetical extension of the phenomenon that is unique to this literature. However, recent attempts to replicate the phenomena described by Bargh et al. have met with difficulty [CITATIONS NEEDED], and there is considerable skepticism about such direct priming effects in general. Nevertheless, proponents of violent game effects continue to cite Bargh's theory as support for video game effects without attention to the evidence against such mechanisms \citep{Prot:Anderson:2013,Anderson:etal:2015}. %The Anderson et al. comment to Bushman et al consensus paper.
 % Note that Bargh's behavioral priming work was a major influence on Anderson's ideas of how VVG effects work.

Continuing the previous quote from \citet[p. 62]{Warburton:2014}, % having determined a priori that the effect must exist, and never mind the data,
``Thus, for me, research in media violence no longer needs to establish whether such media can have a psychological and behavioral impact, but should instead rigorously examine the boundary conditions for such impacts.'' If the effects are indeed so small as we estimate, researchers will be hard-pressed to detect the boundary conditions. To detect r = .08 with 80\% power, one-tailed, would require 960 subjects. To detect the small moderators that reduce the effect to insignificance may require a staggering amount of data.

% Maybe the CRTT bias isn't showing up because everything, not just the CRTT, is biased. Would be nice to read Giancola & Parrott 2008. They show validity of 1st trial, mean, and count of intensity in 350+ subjects, but not of duration or product of duration stuff. Also, it's electricity, not noise.
% Who among us has not received an email from a collaborator suggesting that the means are in the right direction, and perhaps if we ran an additional forty subjects, the desired satistical significance might be attained?

% perhaps r=.20 is the ambient correlational effect size in social psych thanks to pub bias. Even if you're willing to believe that -only- half of published research findings are false \citep[c.f.,][]{Ioannidis:etal:2011}, that implies that the median effect size is roughly the effect size of nonsense and bias.

\newpage
\bibliographystyle{apacite}
\bibliography{database}

\end{document}

%Fun quotes
%Donnerstein, Strasburger, Bushman, comparing skepticism to holocaust denial
%Paul Broca proves that women's brains are smaller and that women are inferior
% Gustave Le Bon (1879), "In the most intelligent races, as among the Parisians, there are a large number of women whose brains are closer in size to those of gorillas than to the most developed male brains. This inferiority is so obvious that no one can contest it for a moment; only its degree is worth discussion. All psychologists who have studied the intelligence of women, as well as poets and novelists, recognize today that they represent the most inferior forms of human evolution and that they are closer to children and savages than to an adult, civilized man.


%Bushman & Huesmann 2014, p. 51, "Furthermore, the Anderson et al. (2010) meta-analysis included extensive testing for possible publication bias, and found none. [...] In many ways it is quite impressive that playing a violent video game for just 15-30 min, on a single occasion can have significant and measurable effects on aggressive behavior."
%Bushman & Huesmann 2014 cites Carlson, Marcus-Newhall, & Miller 1989 meta-analysis as demonstrating "impressive convergence across a wide range of laboratory aggression measures" and Anderson & Bushman 1997 meta-analysis as finding that "'real' and laboratory measures of aggression are influenced in similar ways by situational variables (e.g., alcohol, provocation, anonymity) and by individual difference variables (e.g., trait aggressiveness, participant sex, Type A personality)."
% Bushman & Huesmann emphasize that Anderson 2010 did not ask -anyone- for -unpublished- studies, and that some unpublished dissertations were found but had been published or had not met the inclusion criteria. [best-practices inclusion criteria??]
% Bushman & Huesmann suggest turning Pearson r into an odds ratio because it sounds more impressive. It is, maybe, if you are willing to assume even odds as priors.
% Parting shot, bagging on Ferguson for writing ``violent prose''.

% Warburton 2014 coming off the top rope talking about "Absolute truths in science are elusive'' but basically arguing that the research attains a reasonable degree of proof.
% "That is, unless there is a credible explanation as to why the effects of violent media (including video games) should be an exception to established findings from other media, or a valid reason why different psychological mechanisms would underlie those effects, it must be assumed that media violence effects are likely to follow patterns similar to those that have already been demonstrated." (maybe they're all bunk because psychologists are knuckleheads)
% Cites Greietemeyer a couple times as part of a ``growing research stream on positive media impacts'' which I aso think are rot based on their terrible power
% Warburton argues that other media does have impact, violence exposure factors have impact, and the processes must be the same as under other social psychological processess. He seems to decide a priori that "violent media can and must have some psychological impact on those who experience it, and probably does so via well-understood psychological processes. Thus, for me, research in media violence no longer needs to establish whether such media can have a psychological and behavioral impact, but should instead rigorously examine the boundary conditions for such impacts." (p 62)
% Warburton claims to have improved the hot sauce paradigm methodologically, Waburton Williams Carins, 2006; Warburton, 2014
% Adorable to see him mis-state p-value. "At law [on the balance of probabilities] is a lesser burden of proof. In science, many effects are tested in this way because statistically, the significance of a finding is usually measured in terms of the probability that it is erroneous (i.e., a Type I error). Most commonly there is a cut-off, a value at which the probability is too high that an effect found is simply due to an error in the study (e.g., a p-value of 5 %). Above the value it is thought that, on the balance of probabilities, an effect is not likely to be real. Below the value, it is thought that, on the balance of probabilities, an effect is likely to be real." Barf!
%Again argues publication bias claim ``strongly refuted'' but Anderson et al., and cite Rothstein & Bushman (2012) as published meta-analysis experts.
% I'm a little bothered by this idea that larger metas are always better. Anderson and Greitemeyer both seem to include a lot of studies and effects I'm not sure really belong in there. They argue selectivity based on their biased best-practices sample but argue massive sample size based on the no-criteria sample.
% See also Bushman, Rothstein, and Anderson about the pub bias argument.
% Warburton talks about the triangulation of experimental, cross-sectional, longitudinal, and brain-imaging research. I see a triangulation of research bias!
% Public policy statement 2000 from AAP, AACAP, APA, AMA, AAFP, APA.

% Barbara Krahe
% Cites Bargh, Chen, Burrows (1996) as definitive evidence of priming effects, lmao. People still believed this stuff in 2014?

% Bushman Rothstein Anderson 2010 Much Ado about Something
% very odd: "The term -unpublished study- means that the study was not published in a peer-reviewed journal, although it could have been published in another outlet (e.g., book)". Chiefly I am concerned about studies that never made it to any public attention (e.g., were swallowed due to $p > .05 %$)
% Cite a bunch of experts as emphasizing searching for books, book chapters, conference proceedings, dissertations, and other "gray" literature. So where is the gray literature?
% This whole article is gold, they harp endlessly upon how important it is that they sought out unpublished studies.
% Talking about "small study effects" but we really know what's going on here.

% For everybody else, everything else has been so consistently statistically significant that they can't fathom this NOT being statistically significant.
% I'm coming from the opposite direction: This is significant or at least exaggerated through dramatic publication and analytic bias. It is hard to believe that the foundational research of the 70s, 80s, 90s that we build upon is similarly biased. If we want to develop a research tool and argue for its efficacy, there are clear right and wrong answers. Research producing the right answer is more likely to be published and accepted than research producing the wrong answer.
% "Spun-glass theory of the mind" -- Meehl, 1973, Why I Do Not Attend Case Conferences