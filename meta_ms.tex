% Things to do:
%	Reframe manuscript to emphasize and interpret p-curve findings over PET-PEESE
%	See whether p-curve and PET or PEESE tend to agree
%	Bootstrapped 95 % CIs for all p-curve analyses
%	Word of warning re: trying to interpret 95 % CIs from PET-PEESE
%	Meditate on differences between additive and multiplicative error models.
%	Heterogeneity? Random effects?

% Note: Ferguson:2007a is "Evidence for publication bias"; Ferguson:2007b is "Good Bad and Ugly: Meta-analytic review"

% Bibtex is screwing up the AAP public policy statement citation. Also Konijn et al.

% It has also been widely cautioned that because trim and fill and some other techniques for assessing publication bias are based on an association between effect size and sample size, other explanations of this association should be considered. For example, effect sizes in experimental studies may be larger than those in crosssectional or longitudinal studies due to the reduced error variance that results from tight experimental controls; researchers may know this and therefore may intentionally plan to use larger sample sizes when conducting nonexperimental studies. Similarly, in some research contexts with very large sample sizes (e.g., national surveys) a researcher may have to use less precise measures (e.g., fewer items) that result in smaller effect sizes. In sum, it is possible that the effects in the studies with small samples really are larger than those in the studies with large samples (cf. Sterne and Egger, 2005). \citep[p. 152-153]{Anderson:etal:2010}
% Anderson et al also complain that \citet{Ferguson:2007a} used ``a very small set of available studies'' and ``For example, counter to widely accepted procedures for reducing the impact of publication bias, only published articles were included in the analyses and then procedures for addressing publication bias were misinterpreted. Also, studies published prior to 1995 were ignored and a large number of studies published since that time apparently were missed.'' \citep[p. 152]{Anderson:etal:2010}

\documentclass[man]{apa6}
%\documentclass{article}
\usepackage[natbibapa]{apacite}
\usepackage[longnamesfirst]{natbib}
\usepackage{pdflscape}

\rightheader{Bias in Violent Games Research}
\shorttitle{Bias in Violent Games Research}

\leftheader{Hilgard et al.}

\author{Joseph Hilgard, Christopher R. Engelhardt, and Jeffrey N. Rouder}
% Broad Consensus? Open the coffin? Much ado about something out of nothing?
%\title{There Should Not Be Broad Consensus: Bias in Violent Games Research}
%Does Second Look have too much reference to Greenwald's usage in the 90s. --JR

\title{A Second Look at Bias in Violent Games Research: A Reanalysis of Anderson et al. (2010)}

\affiliation{University of Missouri}

\authornote{
Joseph Hilgard, University of Missouri-Columbia.
Please direct correspondence regarding this article to Joseph Hilgard. E-mail: jhilgard@gmail.com

THIS MANUSCRIPT HAS NOT BEEN PEER-REVIEWED. DO NOT CITE OR DISSEMINATE WITHOUT THE PERMISSION OF THE CORRESPONDING AUTHOR.}

\abstract{Violent video games are theorized to be a significant cause of aggressive thoughts, feelings, and behaviors. A meta-analysis by Anderson and colleagues (2010) is thought by some to condense the research literature into robust and incontrovertible evidence that violent video games affect these outcomes in experimental, cross-sectional, and longitudinal research. In this meta-analysis, application of the trim-and-fill technique found minimal evidence of publication bias. However, there are now more sophisticated methods for the detection of, and adjustment for, publication bias.
In the present manuscript, we examine previous meta-analytic evidence and apply these new techniques for adjusting effect sizes in light of publication bias. 
Our conclusions differ from those of Anderson and colleagues in three salient ways. First, we detect significant publication bias in experimental research. Second, studies selected as being ``methodologically stronger'' do not find larger effects than other studies, but instead represent a subsample of the studies in which statistical significance was found. After adjusting for bias, there is no difference between the two estimates. Finally, after accounting for publication bias, effects of violent games on aggressive behavior in experimental research are found to be minimal.
That said, it is less clear that effects on aggressive affect and aggressive cognition in experiments have been overstated, as our adjustment techniques disagree as to the magnitude of bias in these studies. Furthermore, the cross-sectional literature appears relatively robust and unbiased.
We outline possible sources of bias and suggest directions for stronger future experimental research.
The results indicate the need for an open, transparent, and pre-registered research process to test the existence of the basic phenomenon.
}

\begin{document}
\maketitle


Do violent video games make their players more aggressive? Despite decades of research and hundreds of studies, the basic phenomena remain, at least for some, controversial.  For some authors, whom we term the {\em advocates}, the answer is definitively in the affirmative.  For advocates, the effects are large, obvious, robust, and nearly ubiquitous.  For others, whom we term the {\em skeptics}, the research is not as clean nor as obvious as has been presented.  Instead, skeptics point to a host of issues including construct validity, null findings, and publication bias as undermining the evidence for violent game effects.  In the writings of the skeptics, the evidence for the violent video game effects is not as solid as claimed; in fact, it is paper thin.

The advocates' primary thesis is advanced by a meta-analysis from \citep{Anderson:etal:2010}.  This meta-analysis covers 381 effect-size estimates based on 130,296 participants.  The main findings are that in experiments, there are sizable effects of video game violence on aggressive thoughts ($r = .22$),  aggressive feelings ($r = .29$), and aggressive behaviors ($r = .21$).  Moreover, these effects not limited to experiments but are also found in cross-sectional comparisons and even in longitudinal research designs. \citet{Bushman:etal:2010} and Huesmann (2010, 2014) call the evidence in this corpus of studies ``decisive."

Despite this meta-analysis, there are still skeptics of causal effects of violent video games on aggressive outcomes.  Perhaps there are two class of critiques: one is about the {\em evidentiary value} of the results; the second is about the {\em interpretation} of the results.   The evidentiary critiques are that the meta-analyses suffer from known difficulties including unaccounted publication biases and fortuitous selection criteria for inclusion.  Added to this are concerns that the studies themselves suffer from questionable research practices including selective reporting of dependent variables and strategic inclusion of moderating covariates.   The interpretation critiques are that violent video games differ from nonviolent games in more than violent content.  Such differences include overall arousal, exciting gameplay, and increased competition \citep{Adachi:Willoughby:2011,Elson:etal:2013}.  Therefore, it may be difficult to ascribe any effects to violent content rather than to these other differences.   

In this paper, we focus solely on the evidentiary critiques, which are presented subsequently in more detail.  We ask whether there is solid evidence for the effect of violent video games on aggressive outcomes.  We do not address the question of interpretation here, but see \citet{Elson:Ferguson:2013} and \citet{Bushman:Huesmann:2013} for discussion.   We ask if there is enough evidence for a violent-video-game effect with the explicit limitation that any such effect may or may not reflect the effect of violent content in specific.

Our approach is to reanalyze the meta-analysis of \citet{Anderson:etal:2010}, and we do so for the following reasons:  First, there are now new and more effective techniques for addressing the evidentiary critiques.  These new techniques, including PET \citep[Precision-effect test][]{Stanley:Doucouliagos:2013}, PEESE \citep[Precision-Effect Estimate with Standard Error][]{Stanley:Doucouliagos:2013}, and $p$-curve \citep{Simonsohn:etal:2014}[Note: not Tufano paper], provide for better control of publication bias and questionable research practices than those used in \citet{Anderson:etal:2010}.  Second, we along with others (cite Ferguson, 2010; Elson \& Ferguson, 2014?) remain concerned with the inclusion criterion.  Anderson et al. are admirably transparent in defining two working sets of studies---the full set of all studies, and an additional set of studies that meet {\em best practices} criteria.  As noted by Anderson et al., the best-practices set yields somewhat larger effects than the full set.  For example, the violent-video-game effect on aggressive affect is $r=.29$ for the best-practices experiments but only $r=.181$ for the full set.  

The fact that Anderson used a best-practices set or that the effects are larger with them than the full set is not in itself of concern.  What would be of concern, however, is if there is excessive publication bias and questionable research practices in the sets, and in particular, if there is more of these artifacts in the best-practices set than in the full set.  To answer these questions, we provide a reanalysis with funnel plots as well as the PET, PEESE, and $p$-curve estimators that correct for such biases.  What we find is indeed concerning---not only is there evidence for bias throughout, there is more bias with the best-practices set than the full set.  



\section{Concerns About Bias}
In recent years, psychology has experienced a crisis of confidence as researchers realize that many published research findings may be false. Using statistical techniques and reporting standards typical of social psychology, researchers have been able to provide experimental evidence for impossible phenomena such as extra-sensory precogition \citep[psi;][]{Bem:2011} and a song that makes its listeners younger \citep{Simmons:etal:2011}. Critics have pointed out that hypothesis-confirming results appear in the literature much more frequently than would be expected given reasonable estimates of statistical power. % Greg Francis? Arina K Bones? Uli Schimmack? John Ioannidis? Uli Schimmack?
It has even been suggested that the current ``publish or perish'' reward structure of academia encourages capitalization on Type I error, encouraging researchers to publish many studies with poor predictive value rather than publish few studies with substantial predictive power \citep{Bakker:etal:2011}. In this light, one might expect that there could be bias in violent games research, as there is in so many other disciplines. 

There are two specific types of bias we are concerned with in the meta-analytic context. The first, publication bias, is the phenomenon that studies with statistically significant (i.e., $p<.05$) findings are more likely to be submitted and accepted for publication. Publication bias is a problem that contributes to the overestimation of effect sizes and the propagation of Type I error. It is an especially dangerous problem for meta-analysis, as the selective reporting of studies that ``work'' (i.e., attain significance) leads to an overestimated effect size and may lead to conclusions of statistically and practically significant effects when there are none. The error introduced by publication bias is larger when research studies are comprised of smaller samples and are consequently underpowered.  For these small-sample studies, only those that overestimate the effect dramatically are able to reach the threshold of statistical significance.    Hence, small studies with large effects are perhaps the most suspect.  

The critical question is whether there is evidence for publication bias in the violent video-game literature.  Here, there is disagreement in the literature.  Anderson et al. claim that there is little evidence for publication bias.  Their claim follows from their attempt to account for such bias.  They used a  trim-and-fill procedure, which we discuss subsequently, to estimate bias-adjusted effect size estimates. This procedure yielded only a small adjustment suggesting minimal degree of publication bias. This claim strikes us as doubtful for two reasons. First, the authors found 16 dissertations which had found nonsigificant results and subsequently gone unpublished, but only one unpublished non-dissertation study. Given that dissertations likely represent a minority of all studies conducted on violent games, one might expect that there are more unpublished studies yet languish in file drawers. Second, the trim-and-fill correction is understood to be not particularly effective, as it corrects for bias when bias is absent and does not correct enough when bias is strong.   \citet{Ferguson:2007a}, in contrast, makes the case that publication bias is a difficult and pertinent problem in the violent-video-game literature and does so by noting that XXX (JOE).  In our view, the claim that there is minimal publication bias in violent media seems implausible given the prevalence of publication bias in research in general and in social psychology in particular.  On this basis, more detailed consideration of the possibility of bias in the Anderson et al. meta-analytic dataset is warranted.

The other bias of concern is from practices that inflate Type I error and overstate effect sizes in individual studies.  These practices go under many names including {\em questionable research practices}, {\em researchers' degrees of freedom}, and {\em $p$-hacking}.  A common example of such practices include so-called optional stopping where the decision to end a study is dependent on whether a significant effect has yet been found.  These practices are understood to exist in [social] psychology at non-trivial rates [CITATION NEEDED - SURVEY OF PREVALENCE OF QRP BEHAVIORS].

We suspect there are two specific $p$-hacking mechanisms in the violent-video game literature.  The first has to do with strategic use of dependent measures.  In this literature, it is common to collect several dependent measures.  For example, a researcher might measure aggressive behavior by studying both the duration and volume of a retaliatory noise burst.  There is considerable diversity in the way studies have combined these quantities, and it has been suggested that the diversity reflects the fact that some results attain statistical significance under one combination while other results attain significance under a different combination \citep{Elson:etal:2014}.  Overall, when researchers collect several dependent measures, there exists the possibility that there is some strategic selection among them.  
% need to elaborate on this -- strategic inclusion of covariates is one form of p-hacking and I don't have evidence for or against it. I do think an amount of subgroup analysis goes on vis-a-vis p-hacking.
A second mechanism goes by the strategic analysis and presentation of subgroups.  In some studies, for example, there are factors manipulated independently of game violence. For example, in \citet{Anderson:etal:2004}, study X, participants play a violent or nonviolent game and are then either clearly or ambiguously provoked by a confederate. In their meta-analysis, Anderson et al. include only those participants in the ambiguous-provocation condition. This selective inclusion was applied in both the best-practices and full-sample analyses. In personal correspondence, Anderson tells us ``Only the ambiguous provocation condition was used because we now know that the unambiguous (increasing) provocation version of the task is not as sensitive to a variety of independent variables as is the ambiguous provocation pattern.'' We find this approach risks capitalizing on chance, allowing twice the opportunities for an effect to be found.

\section{Assessing Bias in Meta-Analysis}
There are several approaches to assessing bias in meta-analysis, and some of these have been developed since the publication of \citet{Anderson:etal:2010}. We used several of the more recent tests and methods to provide a new perspective on the Anderson et al. meta-analysis.

A common theme of many of these methods is the relationship between effect size and precision (or sample size) in reported studies. Because sample size does not typically cause effect size, an unbiased research literature is expected to have no relationship between effect size and precision. However, such a relationship will be observed if studies must attain statistical significance to be published. Small-sample studies require large observed effect sizes to reach statistical significance, while large-sample studies can reach statistical significance with smaller observed effect sizes. Thus, in the presence of publication bias, there is an inverse relationship between effect size and precision. 

Note that, in some cases, sample size and effect size may be correlated for reasons other than bias. For example, experimental research tends to have smaller samples than correlational research and may reflect different true effect sizes. Alternatively, it may be possible that manipulations and measurements in small samples are more effective than in large samples. To represent these possibilities, a relationship between sample size and effect size is often called ``small-study effects'' rather than ``publication bias.'' Some of these possibilities can be excluded through practice; conducting separate bias tests for correlational and experimental research can rule out study design as a potential cause of small-study effects.

\subsubsection{Funnel plots}
A funnel plot summarizes the relationship between effect size and sample size, allowing for visual estimation of small-study effects. In a funnel plot, effect size is plotted on the x-axis and precision on the y-axis. In the absence of small-study effects or heterogeneity, study results will form a symmetrical funnel shape, displaying substantial variance when sampling error is large but narrowing to a precise estimate when sampling error is small. Thus, when research is not contaminated by bias, some small-sample studies are expected to find null or even negative results due to sampling error. The funnel should fill symmetrically. See Figure~\ref{funnels1}A for an example of a funnel plot of an unbiased research literature.

Such symmetry is not found in funnel plots of research contaminated with publication bias or $p$-hacking.  In the case of publication bias, studies are missing from the lower portion of the funnel where results would not reach statistical significance. See Figure~\ref{funnels1}B for such an asymmetrical funnel plot. Funnel-plot asymmetry can also be caused by flexibility in analysis and report. When samples are collected until a desired $p$-value is attained, published studies will increase in both precision and effect size, moving towards the upper-right edge of the funnel. When subgroups or experimental subgroups are dropped from report to highlight only a subgroup in which statistical significance was found, studies will lose precision and increase in effect size, moving towards the lower-right edge of the funnel. When outcomes are censored from report to highlight only the significant outcomes, the effect size increases, moving studies to the right of the funnel.   %JOE, WE NEED A FIGURE, AND I AM NOT SURE I BUY THIS.  CAN YOU RUN A SIM.  % Maybe I'll make a figure. I am not programming a bunch of simulations. We can talk about it if you need to.

Again, funnel plots have been presented by skeptics \citep[e.g.,][]{Ferguson:2007}, but the \citet{Anderson:etal:2010} meta-analysis did not provide any funnel plots. This makes it difficult for readers to appraise the strength of the data, inspect the distribution of study results, and determine whether the naive and trim-and-fill effect size estimates might be influenced by outliers.

% Trim-and-fill and how it's bad
\subsubsection{Trim and fill}
One popular bias-adjustment technique, trim-and-fill \citep{Duval:Tweedie:2000}, attempts to detect and adjust for bias through inspection of the number of studies with extreme effect size estimates on either side of the meta-analytic mean estimate. If the funnel plot is asymmetrical, with many more highly-positive effects than null or negative effects, the procedure ``trims'' off the most extreme study and imputes a hypothetical censored study reflected around the funnel plot's axis of symmetry (e.g., an imputed study with a much smaller or even negative effect size estimate). Studies are trimmed and filled in this manner until the ranks are roughly equal. See Figures~\ref{funnels1}C and ~\ref{funnels1}D for examples of trim-and-fill adjusted funnel plots of biased and unbiased literatures, respectively. 

However intuitive, this is not an especially effective adjustment for bias, as the assumptions of trim-and-fill are unlikely to be met. Studies are not likely to be censored on the basis of the effect size, but rather, on the basis of their statistical significance. Accordingly, it is argued that trim-and-fill does a poor job of providing an adjusted effect size, adjusting too much when there is no bias and adjusting too little when there is bias \citep{Lakens:2014,Simonsohn:etal:2014b}. (Indeed, our simulated datasets in Figures~\ref{funnels1}C and ~\ref{funnels1}D experience both these problems; however, they are single simulation runs and may not represent the long-run behavior of trim-and-fill.)%These are the data colada blog post and laken's comment blog post
%(c.f. Duvall \& Tweedie, 2000, who argue that suppression via $p$-value is too simplistic and that there is little difference between the two)
% I had Peters, 2007 as a potential citation elsewhere in the paper, too. And I could look at Moreno et al. to see how trim-and-fill performed relative to metaregression.
The imputation of additional effect sizes also must be regarded with caution, as it adds information to the dataset that does not necessarily exist (Higgins \& Green, Cochrane Handbook for Systematic Reviews of Interventions, March 2011, v5.1.0) %url: http://handbook.cochrane.org/chapter_10/10_4_4_2_trim_and_fill.htm

Thus, trim-and-fill is most commonly suggested as a form of sensitivity analysis rather than a serious estimate of the unbiased effect size. When the naive meta-analytic estimate and the trim-and-fill-adjusted estimate differ only slightly, it is suggested that the research is largely unbiased; when the difference is large, it suggests potential research bias.
\citet{Anderson:etal:2010} applied trim and fill in their meta-analysis as the only attempt to detect and adjust for small-study effects. Trim-and-fill yielded only slightly-adjusted effect sizes, and so the authors concluded minimal research bias.  %Although we are confused by their decision to divide by culture and not to perform trim-and-fill on the total sample -- the smaller the sample, the poorer the power to detect asymmetry, perhaps?
Some have characterized this as an extensive test for publication bias \citep[][pg. 51]{Bushman:Huesmann:2014} despite the weaknesses of the trim-and-fill procedure and the absence of funnel plots or other tests for bias.

\subsubsection{Egger's regression test}
Egger's regression test \citep{Egger:1997} is a simple check for bias which inspects the degree and statistical significance of the relationship between sample size and effect size. A significant test statistic suggests that the observed funnel plot would be unusually asymmetrical were the collected literature unbiased. This test is sometimes helpful in reducing the subjectivity in visually inspecting a funnel plot for asymmetry. Figures~\ref{funnels1}E and ~\ref{funnels1}F show unbiased and biased research literatures with overlaid Egger regression lines. In the case of the unbiased literature, the slope is not statistically significant, but in the case of the biased literature, the slope is statistically significant, indicating the presence of bias.

One weakness of Egger's regression test is that, while it can detect bias, it does not provide a bias-adjusted effect size. The test is also known to have poor statistical power when bias is moderate or studies are few, limiting the strength of conclusions that can be drawn through application of the test (Sterne, Gavaghan, and Egger, 2000).

Egger's regression test has been used repeatedly by skeptics to look for publication bias \citep[e.g.,][]{Ferguson:2007,Ferguson:Kilburn:2009}, but was not reported in the \citet{Anderson:etal:2010} meta-analysis. Thus, while Anderson and colleagues argue that their analysis contains minimal publication bias, an Egger's regression test might have found significant bias.

\subsubsection{PET-PEESE meta-regression}
A promising new tool in the detection of and adjustment for bias is meta-regression. Meta-regression estimates a bias-adjusted effect size by considering the relationship between effect size and precision, then estimating the underlying effect size that would be found with perfect precision. Two meta-regression estimators are the Precision-Effect Test (PET) and Precision-Effect Estimate with Standard Error (PEESE) \citep{Stanley:Doucouliagos:2013}. %See carter & McCullough for better citations 

In PET, a weighted {\em linear} regression is fit to describe the relationship between effect size and precision, much like the Egger regression test. Unlike Egger's test, however, PET then extrapolates from this regression to estimate what the ``true effect'' would be in a hypothetical study with perfect precision. When there is minimal bias, there is minimal adjustment (see Figure~\ref{funnels2}A). When there is no true effect, published studies tend to lie on the boundary between statistical significance and nonsignificance, forming a linear relationship between sample size and precision. Thus, PET performs well at estimating effects when the null hypothesis is roughly true (see Figure~\ref{funnels2}C). However, when there is a true effect, small studies will be censored by publication bias, but most large studies will find statistical significance and be unaffected by bias. PET will fail to model this nuance and risks underestimating the size of true effects (see Figure~\ref{funnels2}B).

A second meta-regression estimator, PEESE, is intended to address this problem. PEESE fits a weighted {\em quadratic} relationship between effect size and precision. The resulting curve models bias as being stronger in the lower part of the funnel but reduced as the studies become better-powered and less subject to bias. Again, in the absence of bias, adjustment is minimal (see Figure~\ref{funnels2}D). PEESE is less likely than PET to underestimate nonzero effects (Figure~\ref{funnels2}E), but risks overestimating the size of null effects (Figure~\ref{funnels2}F).

Because PET underestimates nonzero effects and PEESE overestimates null effects, sometimes PET and PEESE are combined as a two-step conditional PET-PEESE procedure. If PET detects a significant effect, the PEESE estimate is used; if PET does not detect a significant effect, the PET estimate is used. Although this approach would seem to make use of the estimators' complementary strengths and weaknesses, this approach may be exceedingly conservative, as PET has questionable statistical power for the detection of effects. When PET's power is poor, conditional PET-PEESE tends to underestimate effects, as only PET is ever applied. 

This meta-regression technique has been previously applied by \citet{Carter:McCullough:2014} to inspect the amount of evidence for ``ego depletion,'' the phenomenon of fatigue in self-control. They found that after adjusting for small-study effects, PET-PEESE suggested an absence of evidence for the phenomenon. The authors therefore recommended a large-sample pre-registered replication effort, now supported by the American Psychological Society as the topic of the third Registered Replication Report (http://www.psychologicalscience.org/index.php/publications/observer/obsonline/aps-announces-third-replication-project.html).

% Explaining that I'm basically using Peters
One criticism of the Egger and PET-PEESE metaregression tests is that some effect size estimates have an inherent relationship between precision and effect size that is not caused by research bias. For example, given a single sample size, the precision of Cohen's $d$ increases as the effect size $d$ increases. A similar phenomenon holds for odds ratio. When these effect sizes are used, metaregression techniques risk misidentifying the inherent relationship between precision and effect size for a small-study effect. To avoid this problem, it has been suggested that one instead use precision estimates that are a function of the sample size alone (Peters, Sutton, Jones, Abrams, \& Rushton, 2006). In the current report, we use as our effect size estimate Fisher's Z with standard error $\frac{1}{\sqrt{N-3}}$, consistent with the original analysis of Anderson and colleagues. Because this standard error is not a function of the effect size, we avoid the problem of an inherent relationship between precision and effect size that might otherwise contaminate the metaregression.

\subsubsection{$p$-Curve}
Another novel technique for accounting for small-study effects is $p$-curve \citep{Simonsohn:etal:2014}. $p$-curve estimates the true effect size by inspecting the distribution of significant $p$-values. When the null hypothesis is true (i.e. $\delta$ = 0), the $p$-curve is flat: significant $p$-values are as likely to be between .00 and .01 as they are between .04 and .05. When the null hypothesis is false, the $p$-curve becomes right-skewed such that $p$-values between .00 and .01 are more common than are $p$-values between .04 and .05. The degree of right skew is proportionate to the power of studies to detect an effect such that increasing sample sizes or larger true effect sizes will yield greater degrees of right skew. By considering the $p$-values and sample sizes of significant studies, $p$-curve can be used to generate a maximum-likelihood estimate of the true effect size.

One weakness of $p$-curve is that, in the presence of questionable research practices, an excess of $p$-values will gather just under the $p$ = .05 threshold. This results in a flatter $p$-curve than would be found if studies had been reported without $p$-hacking, and thus $p$-curve will underestimate the true effect size in these circumstances. That aside, simulation work suggests that $p$-curve is quite effective at estimating true effect sizes [CITATION NEEDED].

In summary, we will apply a number of meta-analytic techniques for detecting and adjusting for publication bias. Of these, $p$-curve seems the most promising, but the Egger test and meta-regression estimators also add value.


%JOE, I WOULD CUT THE NEXT SECTION IN ITS ENTIRETY
%% Maybe cut this paragraph of these two tripping over each other. But then, it is kind of funny.
%The above techniques describe statistical techniques for inspecting publication bias in research -- ways to look for the influence of unpublished research and its influence on the estimate. We now describe the importance of unpublished research and a novel way to estimate its prevalence.
%
%Shortly after the publication of the \citet{Anderson:etal:2010} meta-analysis, there was some confusion as to the importance of unpublished research in meta-analysis. In a comment, \citet{Ferguson:Kilburn:2010} % "Much Ado About Nothing"
%criticized the inclusion of unpublished research in the meta-analysis, arguing that such work is sometimes of dubious quality. These authors further criticized the purportedly-selective inclusion of {\em not yet published} research, such as articles under review or in press, and publications not peer reviewed, such as book chapters. In their reply, \citet{Bushman:etal:2010} described unpublished studies as ``studies not published in a peer-reviewed journal, although it could have been published in another outlet (e.g., book).'' Although they quoted a passage from \citet{Cooper:2009} stressing the importance of unpublished research as important to protection against bias, the emphasis seemed to be nonetheless on book chapters and dissertations that were otherwise publicly available.
%
%In our view, Drs. Ferguson and Bushman have both misinterpreted what is meant by ``unpublished research.'' The unpublished research we are most often concerned about in meta-analysis are those studies that were conducted but never published in {\em any} form, whether journal article, dissertation, or book chapter. That is, we are concerned about ``publication'' in the most literal sense of {\em being made public.}  Because studies that do not yield significant effects are less likely to be written, submitted, and accepted for publication, substantial parcels of data may be missing from the scientific record. While \citet{Anderson:etal:2010} report having searched thoroughly for unpublished materials, we note that the meta-analysis contains almost entirely studies that were published in at least one form or another (e.g., journal article, book chapter, or dissertation). Only two studies were found that were not published in any format. Given 20 years of research on a family of small effects, using small samples, it seems likely that there are more unpublished studies languishing in file drawers.

\subsection{Unpublished Materials}
Publication bias, in which journals tend to publish only significant findings, is a chief source of overestimated effect sizes in meta-analysis. Nonsignificant results can be difficult to retrieve for meta-analysis as they often go unpublished and forgotten. However, one publication format is largely immune to these publication pressures: the doctoral dissertation. Department requirements generally dictate that dissertations be submitted and published in a dissertation database regardless of whether or not that dissertation is later published as a peer-reviewed journal article.  Another advantage of dissertations is that they are typically thorough, reporting all outcomes and manipulations, whereas published journal articles may instead highlight only the significant results.  Dissertations, then, provide us with a sample of reported studies relatively uncontaminated by publication biases favoring significant results. In our analyses, we highlight unpublished dissertations, their patterns of statistical significance, and how they fared in meeting best-practices criteria.

\section{Method}
We perform a reanalysis of \citet{Anderson:etal:2010} meta-analysis using the same data in the original meta-analysis as provided by the study's first author.  We augment the trim and fill approach with funnel plots, PET-PEESE analysis, and $p$-curve effect-size estimation. We use the original authors' separation of studies by study design (experimental, cross-sectional, longitudinal) and by study outcome (affect, behavior, cognition, arousal) in our presentation.

%JOE, WHAT, I AM LOST HERE, What is Comprehensive Meta-Analysis?
Because the data were analyzed using the software ``Comprehensive Meta-Analysis'' with the intent of testing for moderators, many studies were entered with separate rows for different outcomes or subsamples within studies. However, our current models of publication bias assume that entire studies are censored or re-analysed per their statistical significance; thus, each study should constitute a single observation. Thus, in the event that multiple effect sizes were entered for a particular study (e.g., effects on mean intensity and count of high intensity trials in the CRTT; separate simple effects for men and women), we aggregated these to form a single effect size for the study. For effects representing separate outcomes within a single sample, the outcomes were averaged. For effects representing separate subsamples within a study, the sample sizes were summed and a weighted average made of the subsample effect sizes. This parallels the behavior of the software used in the original analysis. $p$-values were calculated via $t$-test, first dividing Fisher's Z scores by their standard errors to generate a $t$-value, then using that $t$-value to get a two-tailed $p$-value.
%We consulted with the original authors as how best to aggregate rows within studies and reproduce their provided estimates. % Anderson has lost patience with me, I doubt we will be consulting anytime soon.

JOE, THERE IS A LOT OF DETAILED CRAP HERE.  CAN WE BE MORE DIRECT AND LESS DEFENSIVE?  IT FEELS LIKE OVER-DEFENDED OR GRANDSTANDING OR SOMETHING.  PLEASE BE MORE DIRECT AND LESS TANGENTIAL.  NO QUOTES IN METHOD SECTIONS PLEASE.

We then applied the meta-analytic adjustments. PET was performed by fitting a weighted-least-squares regression model predicting effect size as a linear function of the standard error with weights inversely proportional to the square of the standard error. Similarly, PEESE was also applied, predicting effect size as a quadratic function of the standard error and using similar weights. Finally, $p$-curve effect size estimates were generated using code provided by \citet{Simonsohn:etal:2014}, entering a $t$-value and degrees of freedom parameter for each relevant study.

%PET and PEESE estimates are provided regardless of whether statistically significant bias was observed according to recommendations by \citet[p. 20-21]{Stanley:Doucouliagos:2013}: ``To be conservative, one should always use [the PET or PEESE estimate] even if there is insufficient evidence of publication selection because the Egger test [of publication bias] is known to have low power.'' Furthermore, simulations have suggested that the conditional application of meta-regression corrections (that is, applying them only when tests of bias attain statistical significance) tends to perform poorly compared to the unconditional application of such corrections, as a nonsignificant test result does not necessarily constitute firm evidence against publication bias \citep{Moreno:etal:2006}. It is appropriate to apply the bias correction even if the observed bias is not significant.

%For similar reasons, we provide both PET and PEESE estimates regardless of the significance of the PET estimator. The power of PET to detect true effects seems questionable, and a nonsignificant PET result does not constitute strong evidence of the absence of a trueeffect. The reader is encouraged to consider together the $p$-curve, PET, PEESE, and naive estimates in the context of the provided funnel plots and ongoing research into the efficacy of meta-analytic adjustments for bias.

Within the meta-regressions, all effect sizes were converted to Fischer's Z so as to fulfill the regression model's assumptions of normally-distributed effect sizes. Effect sizes are converted back to Pearson $r$ for tables and discussion. %All meta-regressions were performed using the `metafor' package for {\bf R} \citep{Viechtbauer:2010}, using the {\tt rma()} function to fit a variance-weighted model with an additive error term. 
Meta-regression estimates were fit with a multiplicative error term.
$p$-curve estimates were similarly converted from Cohen's $d$ to Pearson $r$ for consistency of presentation.

Both $p$-curve and PET-PEESE are likely to perform poorly when there are few datapoints. Therefore, our analysis is restricted to effects and experimental paradigms with at least ten independent effect sizes. %citation needed from Carter & McCullough, 2014
Our code has been made available online at (GITHUB URL) in the case that the reader nevertheless wants to generate estimates for more sparse datasets or explore the impact of our inclusion and exclusion decisions. The data are available upon request from Dr. Anderson. % Need Craig's permission.

In addition to our analysis of the full dataset as provided by Anderson and colleagues, we perform leave-one-out sensitivity analyses, removing each datapoint one at a time and making all adjusted estimates. For each analysis, a supplementary tab-delimited spreadsheet is attached that lists the individual studies and the estimates when they are left out.\footnote{Initially, we had attempted a different sensitivity analysis in which we removed datapoints with a Cook's distance of more than 0.5 on the PET regression. In the case that several observations were excessively influential, we performed an iterative procedure, deleting the single most influential observation and checking again for influence until no observations had excessive influence. In practice, this tended to delete all datapoints that did not fit the PET regression well. This seemed to distastefully and unfairly favor the PET model over the available data; therefore, we eschewed this approach.}

Two studies were removed from the meta-analysis in all analyses. First, \citet[study 1]{Matsuzaki:etal:2006} was removed because its entered effect sizes were unusually large for their precision (i.e., effects on aggressive behavior $r = .60$ and aggressive cognition $r = .53$), were highly influential on the meta-regression model, and most importantly could not be found as entered in the \citet{Anderson:etal:2010} dataset by inspection of the original article. \footnote{We asked Dr. Anderson for comment. He replied, ``The Japanese team reported additional results for a number of their papers, in those cases in which the initial paper didn't have what was needed. This was true for several other papers as well. For example, if an original paper reported only some composite measure of aggressive personality but had more specific data on physical aggressiveness, we tried to get the more appropriate measure.'' It seems unlikely to us that such a large effect would be found on a single most-appropriate measure and nevertheless would go unreported in favor of a smaller composite effect. However, it is certainly possible. Without recourse to the raw data, we omit this study as an outlier and probable error of data entry. This footnote is provided for the benefit of the reader so that she may judge our decision.}
Similarly, \citet{Panee:Ballard:2002} was removed because the study tested the effects of violent primes on in-game behaviors and not the effects of violent gameplay itself; therefore, it does not provide a relevant test of the hypothesis. %might need to follow up on this

% Anderson et al. It would be nice to provide trim-and-fill estimates, maybe?
We reproduce estimates from \citet{Anderson:etal:2010} and apply $p$-curve effect size estimation and PET-PEESE metaregression to detect and adjust for small-study effects. Sufficient datapoints were available to re-analyze experimental studies of aggressive affect, aggressive behavior, aggressive cognition, and physiological arousal, as well as cross-sectional studies of aggressive affect, aggressive behavior, and aggressive cognition. Studies are further divided to create separate best-practices-only and all-studies estimates per \citet{Anderson:etal:2010} as sample sizes permit. 

\section{Results}
Results for all performed $p$-curves and meta-regressions are summarized in Table \ref{table:PETPEESE}. 
Funnel plots with overlaid PET-PEESE regression lines and curves are provided in Figure \ref{figure:funnelPlots}. We note that visual inspection of the funnel plot often reveals clear asymmetry, particularly in those subsets of studies that \citet{Anderson:etal:2010} selected as ``best-practices'' studies.
Below, we discuss these statistics and describe the results of sensitivity analyses.

\subsection{Egger's regression test}
Results of the Egger's regression tests are supplied in Table XXX. The regression test was statistically significant in several subsets of the data: best-practices and full-sample experiments of aggressive affect, best-practices experiments of aggressive behavior, the full sample of cross-sectional studies of aggressive affect, the full sample (but not best-practices subsample) of experiments of physiological arousal, the best-practices subsample and full sample of cross-sectional studies of aggressive behavior, and the best-practices subsample and full sample of cross-sectional studies of aggressive cognition. The best-practices subsample of experiments of aggressive cognition was also very nearly statistically significant ($p = .055$).

These results indicate that small-study effects are likely present in studies of violent game effects. However, they do not indicate how severe the small-study effects are, or what the true effect sizes may be underlying such small-study effects. We pursue these questions in the next section.

\subsection{Adjusted effect sizes}
Results of the $p$-curve and PET-PEESE analyses are supplied in Table XXX alongside naive fixed-effects and random-effects meta-analytic effect size estimates. Again, our in-progress simulation work suggests that $p$-curve may be the least biased and most efficient of these estimators. However, a weighted combination of several estimators often outperforms any single estimator. Therefore, we suggest that the reader consider all five estimates and apply her own weights in deciding for herself what seems the most likely true effect in each subsample.  

Contrary to the conclusions of the original authors' naive estimates, $p$-curve does not think that best-practices studies measure a larger true effect than do not-best-practices studies. In all cases save one, best-practices and not-best-practices studies received similar adjusted estimates; in the case of correlational studies of aggressive behavior, best-practices studies were estimated as measuring a slightly larger effect. 

Because PEESE is thought to be an unbiased estimator of true nonzero effects, one might think that the PEESE estimate approximates an upper bound on the true effect size -- an estimate that is accurate if there is indeed a nonzero effect. However, in many cases, the $p$-curve estimate exceeds the PEESE estimate. 

There is one notable case in which $p$-curve and PET-PEESE seem to agree on the estimate. When inspecting effects on aggressive behavior in experiments, both techniques estimated that the true effects were very small and likely not meaningfully different from zero. Notably, these estimates are highly consistent with some recent reports by the new generation of violent-media researchers \citep{Engelhardt:etal:2015,Przybylski:etal:2014}.

%This section still needs work!
\subsection{Sensitivity analysis}
Leave-one-out sensitivity analyses are presented in a supplementary Excel spreadsheet. We summarize the results below.

\subsubsection{Aggressive Affect: Experiments} Among experiments of aggressive affect, it was apparent that one study \citep{Ballard:Weist:1996} had substantial influence over the meta-regression line, having an extremely large effect size estimate measured with modest precision. After removing this study, the small-study effects were still apparent (best practices, $p_{Egger} = .002$; all studies, $p_{Egger} < .001$), but meta-regression estimates rose such that PET estimated a more sensible null effect rather than a negative effect (best-practices: PET $r = -.01$, PEESE $r = .17$; full sample: PET $r = -.05$, PEESE $r = .08$). $p$-curve was not influenced much by this exclusion, recommending $r = .13$ for best-practices and $r = .14$ for full sample. 

\subsubsection{Aggressive Affect: Correlational} Among cross-sectional studies of aggressive affect, it was found that several of the studies had substantial influence over the PET-PEESE model. The most influential of these was \citet{Uozumi:2006}; excluding this study caused the PET estimate to fall to nonsignificance and the effect size to be estimated as $r = .05$. Other influential observations (and the estimated effect size after their exclusion) included \citet[study 2, $r = .13$]{Matsuzaki:etal:2004}, and \citet[$r = .16$]{Yukawa:Sakamoto:2001}.

\subsubsection{Aggressive Behavior: Experiments}Among experimental studies of aggressive behavior, leave-one-out sensitivity analysis did not indicate major influence of any particular study in the best-practices or full samples. At most, exclusion of \citet{Anderson:etal:2007} sometimes raised the estimate a bit, as one might expect given that it is the study with the largest sample and the smallest effect size.

\subsubsection{Aggressive behavior: Correlational} Among cross-sectional studies of aggressive behavior, sensitivity analysis indicated that the estimate was largely robust to the inclusion or exclusion of single studies, with $r$ remaining between $.25$ and $.27$ for best-practices and between $r = .18$ and $r = .21$ for full-sample.

\subsubsection{Aggressive cognition: Experiments}[THIS NEEDS TO BE REWRITTEN BECAUSE I'M TRYING TO GET AWAY FROM THE NHST IN PET-PEESE.] Because the effect was very near significance, sensitivity analysis suggested some rather variable estimates, as the removal of a single study could cause the $p$-value to cross the significance threshold. For example, exclusion of \citet{Bushman:Anderson:2009} caused PET to reach significance, leading to a PEESE estimate of $r = .19$. In the other direction, exclusion of \citet{Anderson:Dill:2000} caused the effect size estimate to fall to $r = .06$.

Among all studies, $p$-curve agreed with the original analysis that the effect was $r = .21$. PET found a significant effect of violent games on aggressive cognitions ($p = .003$) and no significant small-study effects ($p_{Egger} = .111$). PEESE estimated the effect as $r = .18$, again smaller than the naive or trim-and-fill estimates. Leave-one-out analysis did not detect much variability in estimates, with $r$ ranging from $.16$ to $.19$. 

\subsubsection{Aggressive Behavior: Correlational}Among best-practices cross-sectional studies of aggressive cognition, exclusion of \citet{Yukawa:Sakamoto:2001} caused the estimate to rise to $r = .17$, while exclusion of \citet{Funk:etal:2003} caused the PEESE estimate to fall to $r = .13$. When \citet{Anderson:etal:2004} was excluded, the PET estimate fell sharply, no longer reaching statistical significance and recommending $r = .06$. In the full sample, sensitivity analyses indicated two particularly influential observations: exclusion of \citet{Santisteban:etal:2007} caused the estimate to rise to $r = .15$, whereas exclusion of \citet{Funk:etal:2003} caused the PET estimate to no longer reach significance, yielding an estimated effect size of just $r = .04$.

\subsubsection{Physiological Arousal: Experiments}In the best-practices subsample, results were highly sensitive to the inclusion or exclusion of single studies, as might be expected of the small number of observations: estimates varied from $r = .08$ to $r = .27$. In the full sample, sensitivity analysis revealed minimal influence from individual studies, with the estimated effect ranging from $r = -.02$ to $r = .02$. Again, $p$-curve estimates were very different, suggesting an effect {\em larger} than that of naive meta-analysis, $r = .27$.

\subsection{Unpublished dissertations}
Funnel plots highlighting the unpublished dissertations are provided in Figure YYY. As one might expect given publication bias, the unpublished dissertations generally populate the left side of the funnel plot. 

We applied chi-square tests to examine two relationships: first, the relationship between statistical significance and publication status, and second, the relationship between publication status and selection as meeting best-practices criteria. Frequencies are given in Table XXX. The liberal counts assume independence of each entered effect size, while the conservative counts aggregate all effect sizes within each study.

Chi-square tests were highly significant for all tests. The relationship between statistical significance and publication status was highly significant such that unpublished dissertations were much less likely to have found statistical significance than published studies (liberal and conservative tests, $p$ < .001).
%$p = 3.94 \times 10^{-6}$; conservative test, $p = 4.02 \times 10^{-6}$). 
Similarly, the relationship between publication status and best-practices inclusion was highly significant such that unpublished dissertations were far less likely to be included as best-practices than published studies (liberal test, $p$ < .001; conservative test, $p$ = .002).
%(liberal test, $p = 2.17 \times 10^{-8}$; conservative test, $p = .002$). 
Although we had hoped that the application of best-practices criteria would alleviate bias, recognizing well-performed research regardless of its results, it instead appears to have intensified bias. 

\section{Discussion}
Our findings differ from those of \citet{Anderson:etal:2010} in three important ways. First, we find evidence of publication bias where the original authors argued bias was minimal. Second, the original meta-analysis claimed that methodologically strong studies found larger effects than did methodologically weak studies. Instead, we find that best-practices studies yield estimates comparable to the full set of studies. Division of studies into best- and not-best-practices exacerbated funnel-plot asymmetry, leading to higher naive estimates but comparable adjusted estimates. Third, the original meta-analysis argued that all outcomes were statistically and practically significant. In our analysis, we find instead that the effect of violent video games on aggressive behavior in experiments is likely very small ($r$ = .05--.10). That said, effects on aggressive affect and aggressive cognition in experimental and cross-sectional research seem stronger and more robust, although $p$-curve and PET-PEESE often disagree about the strength of the effect.

Currently, we believe that $p$-curve is the stronger meta-analytic technique. Although PET-PEESE is intuitive, easy to visualize, and draws upon more studies than just the statistically significant ones, the power of PET to detect a true effect is questionable, particularly in sample sizes typical of social psychology. Thus, PET's significance test does not do much to tell us whether PET or PEESE is the better estimator. Nevertheless, we feel that the PET-PEESE estimates add value by representing possible effect size estimates. Future research will be necessary to know how accurate each estimator is.

Although we believe that effect sizes have been overestimated in research, this is not to say that the true effect sizes are precisely as we estimate. First, if the measures and manipulations used by psychologists are ineffective, there may be a true relationship that is not detected. It is possible that 15-minute gameplay experiments are insufficient to observe and test the effects of violent games. Although brief-session experiments of violent game exposure may not detect substantial effects, it is quite plausible that the accumulated effect of many hours of violent gameplay is relevant and detectable, as reported in longitudinal research efforts (citation needed). Second, $p$-curve will underestimate a true effect in the presence of $p$-hacking. Thus, it is possible that the true effect is substantial but our estimates are biased downwards by $p$-hacking in one or more studies. Third, while we find meta-analytic adjustments for research bias useful, we find prospective meta-analysis still more useful. A transparent and pre-registered collaborative replication effort would be ideal.

On the topic of scientific transparency, we note that the clear and accessible archival of meta-analytic data is a tremendous boon to research transparency. We commend Anderson and colleagues for sharing the data and for responding to questions as to how best reproduce their analyses. We suggest that future meta-analyses routinely include the data, funnel plots (in supplemental materials, if need be), and other supplementary materials \citep{Lakens:etal:2015}. Meta-analyses that cannot be inspected or reproduced should be regarded with concern.

Having detected bias in the meta-analysis, we turn now to possible causes of said bias.

% What went wrong with Anderson 2010?
% Practical considerations for meta-analysis / reducing bias in meta-analysis
\subsection{Selection Bias in Meta-Analysis} 
We observe some instances of flexible application of the best-practices criteria offered by \citet{Anderson:etal:2010}. Flexible application of the inclusion criteria may have lead to preferential selection of studies with significant results. This selection bias could explain why the best-practices studies had larger naive effect-size estimates but comparable adjusted estimates.

$p$-curve estimates very similar effect sizes for both best-practices and all-studies samples. Recall that $p$-curve inspects only the studies that attained statistical significance. Inspection of the funnel plots reveals that the studies selected as best-practices are generally those studies attaining statistical significance; therefore, the studies considered by $p$-curve are mostly the same across the two samples. We now discuss specific instances of criteria application that may be responsible for selection bias.

\subsubsection{Content validity}
The first best-practices criterion is that the violent and nonviolent game must be sufficiently different in violent content. Application of this criterion was not consistent. In some cases, studies were excluded for having nonviolent games that contained very mild cartoon violence, while in others, nonviolent games containing substantial violence were included. For example, comparisons between the violent game {\em Mortal Kombat} and the nonviolent game {\em Sonic the Hedgehog} were discarded as not-best practices \citep[e.g.,][]{Cohn:1995; Hoffman:1994} because ``the nonviolent game contained violence'' \citep[supplementary materials]{Anderson:etal:2010}. Another study comparing a racing game {\em Moto Racer} against the violent game {\em Tekken 2} \citep{Brooks:2000} was excluded for similar reasons, but we were not able to find any violent content in {\em Moto Racer}. (At worst, the player can bump into another driver in such a way that both drivers fall off their bikes; neither driver is injured, and the player suffers a time penalty.) 

Meanwhile, other studies involving comparisons between violent and not-entirely-nonviolent games were included. \citet{Konijn:etal:2007} was included although it used the game {\em Final Fantasy} as a nonviolent game. {\em Final Fantasy} appears to be as violent, or more violent, than {\em Sonic the Hedgehog}, so the simultaneous inclusion of this paradigm and exclusion of the {\em Sonic the Hedgehog} paradigm indicates inconsistency in the application of this criterion. Similarly, a study by \citet{Brady:Mathews:2006} was included as best-practices despite comparing the violent {\em Grand Theft Auto 3} to the purportedly-nonviolent game {\em Simpsons Hit and Run}. While lighter in tone and less explicit than {\em Grand Theft Auto 3}, {\em Simpsons Hit and Run} nonetheless allows the player to punch other characters, steal cars, and run over pedestrians. This content lead video game ratings boards to assign {\em Simpsons Hit and Run} a rating as appropriate for teens, not children. Thus, again, the application of this criterion seems to favor the inclusion of significant results and the exclusion of nonsignificant results.
% Regarding Mathews 2006 vs. Brady & Mathews 2006, Craig says: "As I recall, the dissertation study was the same as the B & M 2006 paper. So, there is no real contradiction here, other than that the dissertation should have been listed differently in the supplemental materials." I'll have to check the estimates and ks.

Flexibility in the application of this criterion may have contributed to selection biases, inflating the naive meta-analytic estimate relative to the adjusted estimate. A better approach might be to have manipulations rated by research assistants naive to hypotheses or to study results, or to seek a statistical quantification of the difference in violence between games, such as a Cohen's $d$ describing a manipulation check.

\subsubsection{Measurement quality}
Selection bias may also have been facilitated by the application of best-practices criterion 5: The outcome measure could reasonably be expected to be influenced by the independent variable if the hypothesis were true. For an example of selection bias, see \citet[study 2]{Anderson:etal:2004}. In this study, participants were assigned to play a violent or nonviolent game, then complete a competitive reaction-time task measure of aggressive behavior with either an ambiguously or unambiguously provoking confederate. A significant effect was found amount the 90  subjects assigned to the ambiguous provocation condition ($r = .25$), but not among the 90 subjects assigned to the unambiguous provocation condition ($r = -.03$). These 90 subjects with a nonsignificant effect were dropped from both the best-practices and not-best-practices meta-analyses. 

When asked for comment, Anderson said ` In other words, the increasing provocation conditions don't meet Criterion 5.'' While it is possible that only one form of the task is sensitive to the manipulation, the meta-analysis does not seek to model such fine-grained moderators; at the least, the full sample should have been included in the full-sample meta-analysis. Furthermore, the validity or invalidity of measurements cannot be determined on whether they provide the researcher with the desired $p < .05$ in an experiment. Finally, since a significant effect in either the ambiguous or unambiguous provocation group would be taken as evidence for an effect of violent video games, we are concerned that the selective exclusion of groups for not demonstrating such an effect risks introducing selection bias.

%  Selection bias in choice of effect sizes
% Here's the quote from Anderson et al (p. 158):
% For studies that reported multiple effects on the same conceptual outcome variable, we took one of two actions. In those cases where one measure was clearly better than the others, based on theoretical relevance (e.g., physical aggression is more relevant to violent video game effects than is verbal aggression), established validity (e.g., use of a well-validated multiple item measure of trait physical aggression vs. a new single item measure of trait aggression), or other empirical evidence offered in the study, we used the best measure. For example, if a study reported two new outcome measures of aggressive behavior, and only one of them correlated significantly with a third variable known to be related to physical aggression (e.g., trait irritability), we used that measure (e.g., Anderson & Dill, 2000, Study 2). In those cases in which there was not a clear best measure, we used the average effect size (Bartholow, Bushman, & Sestir, 2006). Note that we also repeated the main analyses, always using the average effect, and found essentially the same results. 
Selection bias may also influence which effect size among those reported was entered into analysis. As a general rule, it seems that \citet{Anderson:etal:2010} attempted to avoid subjectivity in effect size entry by averaging all reported effect sizes together. However, on several instances, effect sizes were not averaged together, but rather the single largest available effect size was selected. Returning again to \citet[study 2]{Anderson:etal:2004}, the effect of violent games on the first trial of the CRTT was entered (mean difference = 1.07), but not the reported effect size on the other 24 trials of the CRTT (trials 2-9, mean difference = 0.08; trials 10-17, mean difference = 0.04; trials 18-25, mean difference = 0.19). 
Again, Anderson and colleagues may think that this first-trial-only measure is the most appropriate measurement, at least for this particular study. We are less certain. Selection of the largest effects risks capitalizing on chance and systematically overestimating the true effect. There may be some flexibility involved in the decision to select one trial from a set of twenty-five, to be reported in only one half of the total sample. As \citet{Elson:etal:2014} point out, not every study uses 1st-trial-only CRTT behavior as the outcome; perhaps the decision to use this particular outcome is contingent on its statistical significance.

%\subsubsection{Unfalsifiable predictions}
%We note further selection bias in the interpretation of violent games on physiological arousal. As presented by \citet{Anderson:etal:2010}, violent games cause significant increases in physiological arousal, e.g. heart rate or blood pressure. However, in researching this meta-analysis, we became aware of studies in which null effects of violent games were excluded from meta-analysis. For example, in the best-practices studies by \citet{Carnagey:Anderson:2005}, the violent and nonviolent versions of the game were not found to effect players' physiological arousal. Rather than present these findings as null results of violent games on physiological arousal, the authors presented this result as evidence that the violent and nonviolent games were matched stimuli. We observe a similar treatment in the meta-analysis: the null results on physiological arousal were omitted from the meta-analysis investigating effects of violent games on physiological arousal. We find this approach to be too flexible and forbids falsification of the theory, as concordant results are taken as evidence for the theory, but discordant results are excluded from consideration.

%That said, although PET-PEESE estimates negligible effects on arousal relative to a non-violent game, p-curve does estimate substantial effects. Because we suspect $p$-curve gives better estimates than PET-PEESE, we suppose that there are substantial effects of violent games on physiological arousal. Still, it would be helpul if it could be clarified when arousal is an inevitable consequence of violent games and when arousal is a confound that can be controlled.

In sum, it seems that the inclusion criteria were not effective in selecting an unbiased subset of best-practices studies. Instead, they may have provided some degrees of freedom with which studies with significant results could be included and studies with nonsignificant results excluded. % One more sentence to round this out?

\subsection{Omissions}
% Miscoded effect size in Carnagey & Anderson 2005?
% Did they run it again without the experimental condition, just looking at previous exposure and the outcome?? Ask Craig about this. Wouldn't expect it to change that much unless there was a failure of randomization...
Some null findings were not entered for analysis. %Censored? Do I have evidence of that?
In the course of the experiment reported in \citet{Carnagey:Anderson:2005}, a nonexperimental assessment was also made of the effects of previous violent game exposure on aggressive outcomes. In the manuscript, nonsignificant effects of previous violent game exposure were reported for aggressive affect (study 1; $F(1, 66) = 0.78, r = -.11$), aggressive cognitions (study 2; $F(1, 57) = 0.02, r = .02$), and aggressive behavior (study 3; $F(1, 133) = 0.23, r = -.04$). These nonsignificant results were not entered for analysis.

\subsection{Unpublished Studies}
The \citep{Anderson:etal:2010} meta-analysis did make an attempt to collect and analyze unpublished studies (e.g., studies presented in dissertations or book chapters that did not undergo peer review). That the resulting analysis remained biased despite these attempts gives us concern that searching for unpublished studies may not actually alleviate bias in meta-analysis. 

This is not a criticism of the original authors' meta-analytic effort. Unpublished results are extremely challenging to gather. There is no public record, so database searches will not find them. Many have not been written up, so researchers may not have summary statistics to share with the meta-analyst. Such projects are often forgotten (sometimes deliberately), so even if the meta-analyst asks researchers for unpublished data, it may not be yielded. Finally, null results are sometimes reanalyzed and massaged until they become positive research findings, again censoring null results from public report.

Our inspection of unpublished dissertations suggests that there may be more unpublished non-dissertation studies than just the two found by Anderson and colleagues. This, in accord with our adjustments for small-study effects, suggests that the naive meta-analytic estimate is overestimated by publication bias, and indicates the need for publication of all competent research, not just the research finding significant effects. 

% Reducing bias in empirical research
%\subsection{Improving Research Quality}
%Finally, we must register some skepticism of the more complex, interactive models of aggressive behavior that have been developed through this research literature. It seems that researchers quickly took the basic phenomenon for granted and began to cast about for more sophisticated models that would advance theory. In some cases, these more sophisticated models led to attempted conceptual, rather than direct, replications. It has been pointed out that the results conceptual replications can be difficult to appraise: if significance is attained, the replication is considered a success, but if significance is not attained, the replication may be considered invalidated by the changes in its paradigm. 

%In other cases, these more sophisticated models lead to the study of moderators and subgroups. When many moderators are tested, Type I error rates will rise substantially due to the problem of multiple comparisons. Post-hoc exploratory analyses of moderators are, of course, important and valuable (indeed, we have presented them ourselves in the past), but become hazardous when presented as confirmatory or when patterns of statistical significance are taken to identify the validity or invalidity of the measures. We note that replication attempts of such interactions are exceedingly rare. Furthermore, if the main effect is as small as we estimate here, and if the moderating effects are on a similarly small scale, such tests of the interactions could be woefully underpowered, providing little positive predictive value and mostly generating Type I error.

% For future days?
%\subsection{The Competitive Reaction-Time Task}
%One popular measure of aggressive behavior, the Competitive Reaction-Time Task (CRTT), has been the topic of much discussion. While this measure is often used, it is rarely quantified the same way twice. It has been suggested that this variability in quantification is a form of $p$-hacking used by researchers to find larger, more significant effects \citep{Elson:etal:2014}. Anderson and colleagues point out that studies using the CRTT find smaller, not larger, effects, suggesting that CRTT results are not inflated.
% % Now speculating, to be tested with data soon.
%[The following is my hypothesis and needs to be tested, perhaps not in this manuscript.] Our analysis finds that studies using the CRTT also feature larger sample sizes. It is possible, then, that the analysis and report of the CRTT is as biased or more biased than that of other measures, but that less bias is needed to reach statistical significance with these larger sample sizes. Thus, we maintain that there is clear need for validation of the noise-blast CRTT and for preregistration of the CRTT quantification that will be used in confirmatory research projects.

\subsection{Limitations}
There are some limitations to the analyses we present. The meta-analytic adjustments used are novel and their limitations may not yet be fully understood. In informal simulations [cite blog posts], $p$-curve tends to perform well. However, it is hard to understand why $p$-curve would estimate effects of violent games on physiological arousal to be larger than would naive meta-analysis. Perhaps some research projects find large effects on physiological arousal but do not report them, as the findings may be considered ``too obvious'' for publication. Alternatively, perhaps samples are small enough that estimates have substantial imprecision, or we have violated some assumption of the model.

Similarly, PET-PEESE has its own limitations. Although PET seems to perform well when the null is true, and PEESE seems to perform well when the null is not true, the hybrid PET-PEESE technique has questionable power to detect when the null is not true. Thus, PET and PEESE might be thought of as presenting lower and upper bounds on the effect, respectively, rather than identifying the true effect size. 

Another criticism of meta-regression is that small-study effects may be caused by phenomena besides publication bias or $p$-hacking. For example, a small survey might measure aggressive behavior thoroughly, with many questions, whereas a large survey can only afford to spare one or two questions. Similarly, sample sizes in experiments may be smaller, and effect sizes larger, than in cross-sectional surveys. The current report is able to partly address this concern by following the original authors' decision to analyze experimental and cross-sectional research separately. Still, there may be genuine theoretical and methodological reasons that larger studies find smaller effects than do smaller studies. %For example, the largest study of aggressive behavior contains samples of both teens and children \citep{Anderson:etal:2007}. Children were made to play a violent or non-violent E-rated game. These games may not have sufficient violent content to present an effect. Granted, no effect was found among the teens who played T-rated violent games, so this account may not hold, either.

% Theoretical considerations
\subsection{Ways Forward}
Although the analyses we present attempt to account for publication and analytic bias, they do not account for validity. Even these adjusted estimates may still overestimate the true effect size due to the influence of confounds. Although it is often claimed that the observed effects are due to violent content alone \citep[][e.g.,]{Anderson:etal:2004}, the evidence for this claim is sometimes weak. Pilot studies are often used to argue that a violent and nonviolent game are equivalent in all other dimensions, but sample sizes are often too small to support this claim \citep{Hilgard:etal:2015}. Application of confounds in analysis of covariance is a more promising approach, but this is also sometimes controversial \citep{Miller:Chapman:2001}. When covariates are measured with error (e.g., with single-item Likert measures), substantial residual variance may be left behind and mistaken for variance associated with violence. Thus, insofar as effects remain after adjustment for small-study effects, they may still be contaminated to some degree by confounds. For these reasons, we favor modified-game paradigms for experimental research (Elson \& Quandt, 2014; see Engelhardt, Hilgard, \& Bartholow, 2015, Engelhardt et al., 2015, Elson et al., 2015, Kneer, Elson, \& Knapp, 2015), which manipulate violent content while preserving the content of gameplay (rules, controls, level design, etc.). One criticism of these paradigms is that the non-violent conditions may not be perfectly nonviolent. We suggest the strengths and weaknesses of these manipulations be the subject of future discussion and study.

We have abstained from inspection of longitudinal studies as there are not enough data points to permit a good estimate. It is possible and even likely that there are detectable longitudinal effects of many hours of gameplay over time. Nonetheless, researchers conducting longitudinal studies should be careful to maintain a transparent research process and to publish results regardless of their significance lest the longitudinal research literature be found to suffer from similar weaknesses.
%This would seem more plausible than the prospect of a substantial and reliable effect obtained within fifteen to thirty minutes of gameplay. We echo the words of \citet[p. 51]{Bushman:Huesmann:2014}, ``In many ways it is quite impressive that playing a violent video game for just 15-30 min, on a single occasion can have significant and measurable effects on aggressive behavior.'' Our tone, however, is different. Such an effect would be impressive, in the sense that it would be surprising and require substantial evidence to support. Our analysis suggests that the strength of evidence is not sufficient to support such a conclusion.

One line of thought is that the basic phenomenon is certain and that research should be focused on elaborating on the model by exploring moderators of the effect. This perspective is most strongly enunciated by \citet[p. 62]{Warburton:2014}, who writes, % having determined a priori that the effect must exist, and never mind the data,
``Violent media can and must have some psychological impact on those who experience it, and probably does so via well-understood psychological processes. [\ldots] Thus, for me, research in media violence no longer needs to establish whether such media can have a psychological and behavioral impact, but should instead rigorously examine the boundary conditions for such impacts.'' 

We disagree with this perspective regarding the effects on behavior in experiments. We feel that it is most important to establish the existence of the basic phenomenon before attempting to elaborate on possible moderators. If the effects are indeed so small as we estimate, researchers will be hard-pressed to detect the boundary conditions. If $p$-curve is correct and the true effect size in a well-designed experiment is $r = .07$, then 1257 samples are necessary to achieve $80\%$ one-tailed power. To detect the small moderators that reduce the effect to insignificance may require a staggering amount of data.

At present, researchers may feel that they know a lot about the moderators that influence the effect of violent video games on aggressive behavior, as many studies report significant interactions of violent game content by individual differences such as trait anger or gender. However, the diversity of reported moderators and the paucity of replication of these moderations suggest possible weaknesses in the literature. When many moderators are tested, Type I error rates will rise substantially due number of tests conducted. Furthermore, if the main effect is as small as we estimate here, and if the moderating effects are on a similarly small scale, such tests of the interactions could be woefully underpowered, providing little positive predictive value and mostly generating Type I error. Post-hoc exploratory analyses of moderators are valuable (indeed, we have presented them ourselves in the past), but become hazardous when presented as confirmatory or when patterns of statistical significance are taken to identify the validity or invalidity of the measures. 

In sum, the research literature as analyzed by \citep{Anderson:etal:2010} seems to contain greater publication bias than their trim-and-fill analyses and conclusions indicated. This is especially true of those studies which were selected as using best practices, as the application of best-practices criteria seemed to be influenced sometimes by the results of the study. Effects in experiments seem to be overestimated, particularly those of violent video game effects on aggressive behavior, which appeared to be very close to zero. 

Rather than accept these estimates as the ``true'' effect sizes, we recommend instead a preregistered collaborative research effort and prospective meta-analysis. In this research effort, preregistration and collaboration will both be indispensable. In the absence of preregistration and collaboration, the two well-defined camps of proponents and skeptics may each find results that support their conclusions and refuse to believe the results of the other camp. If we are to advance the debate over violent game effects, we must do it not by silencing the other party, but by getting each party to sit down together with a disinterested third party, design an experiment, and say in writing for all to see, ``I agree that this is the appropriate research design. My theory predicts that the result shall be this; his theory predicts that the result shall be that. Together, let us see who is right, and move on.''

% This paragraph is way too pulpy. Take it down twelve notches.
%We echo the astonishment registered by \citet[p. 62]{Warburton:2014}: Given the theories and evidence in the rest of social psychology and media psychology, ``'' 

%The theories and evidence in the rest of social psychology and media psychology may be similarly weak. For example, the idea of ``behavioral priming,'' e.g. that subliminally activating a thought influences automatic behavior \citep{Bargh:etal:1996}, holds a substantial position in the General Aggression Model [CITATION NEEDED]. Observing or participating in video game violence, it is argued, activates aggressive thoughts, which then cause increased aggression in behavior, particularly automatic behavior. Moreover, it is hypothesized that repeated exposure to violent media could cause aggressive thoughts to be chronically primed [CITATION NEEDED], a hypothetical extension of the phenomenon that is unique to this literature. However, recent attempts to replicate the phenomena described by Bargh et al. have met with difficulty [CITATIONS NEEDED], and there is considerable skepticism about such direct priming effects in general. Nevertheless, proponents of violent game effects continue to cite Bargh's theory as support for video game effects without attention to the evidence against such mechanisms \citep{Prot:Anderson:2013,Anderson:etal:2015}. %The Anderson et al. comment to Bushman et al consensus paper.
 % Note that Bargh's behavioral priming work was a major influence on Anderson's ideas of how VVG effects work.




% Maybe the CRTT bias isn't showing up because everything, not just the CRTT, is biased. Would be nice to read Giancola & Parrott 2008. They show validity of 1st trial, mean, and count of intensity in 350+ subjects, but not of duration or product of duration stuff. Also, it's electricity, not noise.
% Who among us has not received an email from a collaborator suggesting that the means are in the right direction, and perhaps if we ran an additional forty subjects, the desired satistical significance might be attained?

% perhaps r=.20 is the ambient correlational effect size in social psych thanks to pub bias. Even if you're willing to believe that -only- half of published research findings are false \citep[c.f.,][]{Ioannidis:etal:2011}, that implies that the median effect size is roughly the effect size of nonsense and bias.

\newpage
\bibliographystyle{apacite}
\bibliography{database}

\begin{figure}
	\includegraphics{funnels_1.png}
	\caption{Funnel plots, trim-and-fill, and Egger's test. Effect size Fisher's $z$ is on the x-axis, while standard error of Fisher's $z$ is on the y-axis. The true effect size $z = .2$ is indicated by the dashed line. Panels A and B show funnel plots for unbiased and biased literatures, respectively. The solid line indicates the naive meta-analytic estimate. Panels C and D show the results of trim-and-fill adjustments to these literatures, with the white points representing imputed ``filled'' studies. The solid line indicates the trim-and-fill-adjusted estimate. Panels E and F show an overlaid Egger's regression line. The slope is statistically significant in F but not in E.}
	\label{funnels1}
\end{figure}

% i don't think landscape is doing anything yet
\begin{figure}
%\begin{landscape}
	\includegraphics[width = \textwidth, keepaspectratio]{funnels_2.png}
	\caption{PET and PEESE meta-regression. Again, Fisher's $z$ is on the x-axis, standard error is on the y-axis, and the true effect size is indicated by the dashed line. Bias-adjusted estimates are indicated by the dotted vertical line. Panels A and B indicate the PET technique applied to unbiased and biased literatures of a nonzero effect. PET underestimates the nonzero effect in the presence of bias. Panel C indicates the PET technique applied to a biased literature of a null effect; PET does quite well in estimating the null effect. Panels D and E show PEESE applied to unbiased and biased literatures of a nonzero effect. Panel F shows PEESE applied to a biased literature of a null effect. PEESE does well at estimating the nonzero effect, but overestimates the null effect.}
	\label{funnels2}
%\end{landscape}
\end{figure}


\end{document}



%Fun quotes
%Donnerstein, Strasburger, Bushman, comparing skepticism to holocaust denial
%Paul Broca proves that women's brains are smaller and that women are inferior
% Gustave Le Bon (1879), ``In the most intelligent races, as among the Parisians, there are a large number of women whose brains are closer in size to those of gorillas than to the most developed male brains. This inferiority is so obvious that no one can contest it for a moment; only its degree is worth discussion. All psychologists who have studied the intelligence of women, as well as poets and novelists, recognize today that they represent the most inferior forms of human evolution and that they are closer to children and savages than to an adult, civilized man.''


%Bushman & Huesmann 2014, p. 51, "Furthermore, the Anderson et al. (2010) meta-analysis included extensive testing for possible publication bias, and found none. [...] In many ways it is quite impressive that playing a violent video game for just 15-30 min, on a single occasion can have significant and measurable effects on aggressive behavior."
%Bushman & Huesmann 2014 cites Carlson, Marcus-Newhall, & Miller 1989 meta-analysis as demonstrating "impressive convergence across a wide range of laboratory aggression measures" and Anderson & Bushman 1997 meta-analysis as finding that "'real' and laboratory measures of aggression are influenced in similar ways by situational variables (e.g., alcohol, provocation, anonymity) and by individual difference variables (e.g., trait aggressiveness, participant sex, Type A personality)."
% Bushman & Huesmann emphasize that Anderson 2010 did not ask -anyone- for -unpublished- studies, and that some unpublished dissertations were found but had been published or had not met the inclusion criteria. [best-practices inclusion criteria??]
% Bushman & Huesmann suggest turning Pearson r into an odds ratio because it sounds more impressive. It is, maybe, if you are willing to assume even odds as priors.
% Parting shot, bagging on Ferguson for writing ``violent prose''.

% Warburton 2014 coming off the top rope talking about "Absolute truths in science are elusive'' but basically arguing that the research attains a reasonable degree of proof.
% "That is, unless there is a credible explanation as to why the effects of violent media (including video games) should be an exception to established findings from other media, or a valid reason why different psychological mechanisms would underlie those effects, it must be assumed that media violence effects are likely to follow patterns similar to those that have already been demonstrated." (maybe they're all bunk because psychologists are knuckleheads)
% Cites Greietemeyer a couple times as part of a ``growing research stream on positive media impacts'' which I aso think are rot based on their terrible power
% Warburton argues that other media does have impact, violence exposure factors have impact, and the processes must be the same as under other social psychological processess. He seems to decide a priori that "violent media can and must have some psychological impact on those who experience it, and probably does so via well-understood psychological processes. Thus, for me, research in media violence no longer needs to establish whether such media can have a psychological and behavioral impact, but should instead rigorously examine the boundary conditions for such impacts." (p 62)
% Warburton claims to have improved the hot sauce paradigm methodologically, Waburton Williams Carins, 2006; Warburton, 2014
% Adorable to see him mis-state p-value. "At law [on the balance of probabilities] is a lesser burden of proof. In science, many effects are tested in this way because statistically, the significance of a finding is usually measured in terms of the probability that it is erroneous (i.e., a Type I error). Most commonly there is a cut-off, a value at which the probability is too high that an effect found is simply due to an error in the study (e.g., a p-value of 5 %). Above the value it is thought that, on the balance of probabilities, an effect is not likely to be real. Below the value, it is thought that, on the balance of probabilities, an effect is likely to be real." Barf!
%Again argues publication bias claim ``strongly refuted'' but Anderson et al., and cite Rothstein & Bushman (2012) as published meta-analysis experts.
% I'm a little bothered by this idea that larger metas are always better. Anderson and Greitemeyer both seem to include a lot of studies and effects I'm not sure really belong in there. They argue selectivity based on their biased best-practices sample but argue massive sample size based on the no-criteria sample.
% See also Bushman, Rothstein, and Anderson about the pub bias argument.
% Warburton talks about the triangulation of experimental, cross-sectional, longitudinal, and brain-imaging research. I see a triangulation of research bias!
% Public policy statement 2000 from AAP, AACAP, APA, AMA, AAFP, APA.

% Barbara Krahe
% Cites Bargh, Chen, Burrows (1996) as definitive evidence of priming effects, lmao. People still believed this stuff in 2014?

% Bushman Rothstein Anderson 2010 Much Ado about Something
% very odd: "The term -unpublished study- means that the study was not published in a peer-reviewed journal, although it could have been published in another outlet (e.g., book)". Chiefly I am concerned about studies that never made it to any public attention (e.g., were swallowed due to $p > .05 %$)
% Cite a bunch of experts as emphasizing searching for books, book chapters, conference proceedings, dissertations, and other "gray" literature. So where is the gray literature?
% This whole article is gold, they harp endlessly upon how important it is that they sought out unpublished studies.
% Talking about "small study effects" but we really know what's going on here.

% For everybody else, everything else has been so consistently statistically significant that they can't fathom this NOT being statistically significant.
% I'm coming from the opposite direction: This is significant or at least exaggerated through dramatic publication and analytic bias. It is hard to believe that the foundational research of the 70s, 80s, 90s that we build upon is similarly biased. If we want to develop a research tool and argue for its efficacy, there are clear right and wrong answers. Research producing the right answer is more likely to be published and accepted than research producing the wrong answer.
% "Spun-glass theory of the mind" -- Meehl, 1973, Why I Do Not Attend Case Conferences