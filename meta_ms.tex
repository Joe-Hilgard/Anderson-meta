
Publication bias is a problem that contributes to the overestimation of effect sizes and the propagation of Type I error.

Meta-analytic techniques for assessing and correcting for publication bias have been applied to this literature, but these are of questionable utility. Fail-safe N, in theory, provides the number of studies %(subjects?)
that would need to be hypothetically hiding in file-drawers in order to reduce an effect to non-significance. This technique is flawed in that it assumes an effect size of exactly zero in all censored studies, does not consider the possibility of flexible analysis (e.g. p-hacking), and does not provide an estimate of a bias-adjusted effect size.

Another popular bias-adjustment technique, trim-and-fill \citep[Duvall:Tweedie:19XX} attempts to detect and adjust for bias through inspection of the funnel plot. If the funnel plot is asymmetrical, the procedure "trims" off the most extreme study and imputes a hypothetical censored study reflected around the funnel plot's axis of symmetry. Studies are trimmed and filled until the funnel plot is no longer significantly asymmetrical. This is not an effective adjustment for bias, as the assumptions of trim-and-fill are unlikely to be met. Studies are not likely to be censored on the basis of the effect size, but rather, on the basis of their statistical significance. It is argued that trim-and-fill does a poor job of providing an adjusted effect size, adjusting too much when there is no bias and adjusting too little when there is bias. %Could cite data colada post

Meta-regression techniques instead consider the relationship between effect size and precision. In an asymmetrical funnel plot, larger samples yield smaller effects, as would be expected if studies were censored when not attaining statistical significance or when studies are flexibly analyzed in order to attain statistical significance. The PET-PEESE meta-regression \citep{Stanley:Doucouliagos:20XX} uses the published literature to estimate the relationship between precision and effect size, then extrapolates to estimate what the "true effect" would be if measured with perfect precision. This meta-regression technique has been previously applied by \citet{Carter:McCullough:2014} to inspect the amount of evidence for "ego depletion", the phenomenon of fatigue in self-control. They found that after adjusting for publication bias, PET-PEESE suggested an absence of evidence for the phenomenon, and therefore recommended a large-sample pre-registered replication effort.



We apply PET-PEESE meta-regression to two recent meta-analyses of violent video game effects, one covering research up until 2009 \citep{Anderson:etal:2010} and the other research between 2009 and 2013 \citep{Greitemeyer:Mugge:2014}. Both meta-analyses argued for statistically and practically significant effects of violent video games on aggressive outcomes. Moreover, both applied trim-and-fill and found little evidence or adjustment for research bias. However, visual inspection of the funnel plot often reveals alarming asymmetry, particularly in the case of those studies which \citet{Anderson:etal:2010} call "best-practices" studies.

% Methods
We acquired data from \citet{Anderson:etal:2010} and \citet{Greitemeyer:Mugge:2014}. Because the data were analyzed using Comprehensive Meta-Analysis, many studies were entered with separate rows for different outcomes or subsamples within studies. We consulted with the original authors as how best to aggregate rows within studies and reproduce their provided estimates.

We then followed the PET-PEESE procedure, fitting a weighted-least-squares regression model predicting effect size (Pearson's r converted to Fisher's z) as a linear function of the standard error, with weights inversely proportional to the square of the standard error. In the case that the PET regression found a statistically significant effect after accounting for publication bias, PEESE was applied, predicting effect size as a quadratic function of the standard error. PET or PEESE estimates are provided regardless of whether statistically significant bias was observed according to recommendations by \citet[p. 20-21]{Stanley:Doucouliagos:20XX}: ``To be conservative, one should always use [the PET or PEESE estimate] even if there is insufficient evidence of publication selection because the Egger test [of publication bias] is known to have low power.''

Because PET-PEESE is a regression method, it is likely to perform poorly when there are few datapoints. Therefore, our analysis is restricted to effects and experimental paradigms with at least fifteen %twenty?
independent effect sizes. Data and code have been made available online in the case that the reader nevertheless wants to generate PET-PEESE estimates for more sparse datasets.

For sensitivity analysis, we remove datapoints with a Cook's distance of more than 0.5, as these may have excessive influence over the slope of the regression line. Estimates are provided with and without these influential observations.

% Results
% Anderson et al.
We reproduce estimates from \citet{Anderson:etal:2010} and apply PET-PEESE. Sufficient datapoints were available to re-analyze experimental studies of aggressive affect, aggressive behavior, aggressive cognition, and physiological arousal, as well as cross-sectional studies of aggressive affect, aggressive behavior, and aggressive cognition. Studies are further divided into ``best-practices'' and ``not best-practices'' studies per \citet{Anderson:etal:2010} as sample sizes permit.


\section{Experiments}
\subsection{Aggressive affect}

It is interesting to note that, contrary to the findings of \citet{Anderson:etal:2010}, we find that effects are larger in not-best-practices experiments than in best-practices experiments after adjusting for selection and publication bias. This is likely because the best-practices criteria are flawed or flexibly applied. An example of a flawed criterion is their recommendation that games be matched in pilot testing. Many studies which did pilot-test their games used such small samples that a lack of statistical significance did not necessarily constitute evidence for the null. % For more see Hilgard Engelhardt Bartholow & Rouder
Flexible application of the criterion includes % the exclusion/inclusion nonsense with Sonic vs Mortal Kombat being bad but GTA3 vs Simpsons Hit-n-Run being OK.

% Gone from predicting 4.4 % of variance to 1.4 %

% Discussion
PET-PEESE meta-regression is a relatively new technique and its limitations may not yet be fully understood. For these reasons, we suggested that the provided estimates be considered possible bias-adjusted estimates rather than corrected estimates of the true effect. Also, PET-PEESE is a regression method, and like most regression methods, results can be misleading when few datapoints are available. PET-PEEESE also extrapolates outside the model, estimating the effect when standard error is zero when no datapoints have zero error. Thus, rather than regard the present meta-analysis as having identified the true effect size, we instead suggest it as food for meta-analytic thought. Clearly, publication bias, meta-analytic bias, and perhaps flexible analysis are problems in this literature as they are in so many other literatures. We recommend the use of large, pre-registered, open-data research projects, ideally with collaboration across antagonistic research teams. We also feel that, in light of the current results, there is reason enough for skepticism. Attempts to establish consensus or to research the science of denialism may be premature. We hope that the years ahead foster a civil, unprejudiced, and transparent application of the scientific method. 

