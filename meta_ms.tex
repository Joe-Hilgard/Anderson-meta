\documentclass[jou]{apa6}
%\documentclass{article}
\usepackage[natbibapa]{apacite}
\usepackage[longnamesfirst]{natbib}

\rightheader{Bias in Violent Games Research}
\shorttitle{Bias in Violent Games Research}

\leftheader{Hilgard et al.}

\author{Joseph Hilgard, Christopher R. Engelhardt, Bruce D. Bartholow, and Jeffrey N. Rouder}
% Broad Consensus? Open the coffin? Much ado about something out of nothing?
\title{There Should Not Be Broad Consensus: Bias in Violent Games Research}

\affiliation{University of Missouri}

\note{\begin{flushleft}

\vspace{1in}
Joseph Hilgard\\
jhilgard@gmail.com
\end{flushleft}
}

\abstract{}

\begin{document}
\maketitle

%1. It is believed by many that violent games cause violent outcomes and that the experimental research is convincing, but others are less persuaded

%2. Sources of controversy include failures to replicate and speculation about biased analysis, reporting, and meta-analysis of research.

%3. Results from naive meta-analysis and consequent certainty among researchers. Nailing the coffin shut on doubts (Huesmann, 2010), Why is it so hard to convince / new media old problems (Donnerstein, Strasburger, Bushman, 2014), There is broad consensus (Bushman et al), how do we silence denialists (Anderson et al)

% What is publication bias and small-study effects?
Publication bias is the phenomenon that studies with statistically significant ($p<.05$) findings are more likely to be submitted and accepted for publication. Publication bias is a problem that contributes to the overestimation of effect sizes and the propagation of Type I error. It is an especially pernicious problem for meta-analysis, as the selective reporting of studies that ``work'' (i.e. attain significance) leads to an overestimated effect size. For this reason, a crucial part of meta-analysis is to attempt to inspect and adjust for the degree of publication bias.


\begin{quote}
Despite thousands of research studies on media effects, many people simply refuse to believe them. Some academics may contribute to this because they like to "buck the establishment," which is an easy way to promote themselves and their research. Of course, many people still believe that President Obama wasn't born in the United States, President Kennedy wasn't assassinated, men didn't walk on the moon, and the Holocaust didn't occur. \citep[p. 572]{Strasburger:etal:2014}
\end{quote}
%end blockquote

So confident are some believers of violent-media effects that any remaining skepticism seems to be due to unfathomable stubbornness. 
Sufficiently satisfied that the phenomenon has been demonstrated time and again in laboratory research, some authors have moved on to advancing theories as to why skepticism remains, suggesting that skeptics are influenced by psychological reactance or cognitive dissonance. Certain skeptics are compared to moon-landing deniers (or worse), described as being incapable to persuade by any amount of evidence \citep{Strasburger:etal:2014}) Researchers have begun to propose ways to ``advance the debate'' by speaking directly to the public to avoid skeptics within academia \citep{Strasburger:Donnerstein:2014}. Claims of consensus have been advanced \citep{Bushman:etal:2014}, and attempts have been made to separate ``true media violence scientists'' who believe in the effect from less-expert sources motivated to deny evidence \citep{Anderson:etal:2014}.

% Fail-safe N stinks
Meta-analytic techniques for assessing and correcting for publication bias have been applied to this literature, but these are of questionable utility. Fail-safe N, in theory, provides the number of studies %(subjects?)
that would need to be hypothetically hiding in file-drawers in order to reduce an effect to non-significance. This technique is flawed in that it assumes an effect size of exactly zero in all censored studies, does not consider the possibility of flexible analysis (e.g. p-hacking), and does not provide an estimate of a bias-adjusted effect size.

% Trim-and-fill and how it's bad
Another popular bias-adjustment technique, trim-and-fill \citep{Duval:Tweedie:2000} attempts to detect and adjust for bias through inspection of the funnel plot. If the funnel plot is asymmetrical, the procedure ``trims'' off the most extreme study and imputes a hypothetical censored study reflected around the funnel plot's axis of symmetry. Studies are trimmed and filled until the funnel plot is no longer significantly asymmetrical. This is not an effective adjustment for bias, as the assumptions of trim-and-fill are unlikely to be met. Studies are not likely to be censored on the basis of the effect size, but rather, on the basis of their statistical significance. It is argued that trim-and-fill does a poor job of providing an adjusted effect size, adjusting too much when there is no bias and adjusting too little when there is bias. %Could cite data colada post

Meta-regression techniques instead consider the relationship between effect size and precision. In an asymmetrical funnel plot, larger samples yield smaller effects, as would be expected if studies were censored when not attaining statistical significance or when studies are flexibly analyzed in order to attain statistical significance. The PET-PEESE meta-regression \citep{Stanley:Doucouliagos:2013} %See carter & McCullough for better citations
uses the published literature to estimate the relationship between precision and effect size. A weighted regression is fit to describe how the effect size changes with increasing experimental precision, then extrapolates to estimate what the ``true effect'' would be in a hypothetical study with perfect precision. This meta-regression technique has been previously applied by \citet{Carter:McCullough:2014} to inspect the amount of evidence for ``ego depletion'', the phenomenon of fatigue in self-control. They found that after adjusting for publication bias, PET-PEESE suggested an absence of evidence for the phenomenon, and therefore recommended a large-sample pre-registered replication effort.

We apply PET-PEESE meta-regression to two recent meta-analyses of violent video game effects, one covering research up until 2009 \citep{Anderson:etal:2010} and the other research between 2009 and 2013 \citep{Greitemeyer:Mugge:2014}. Both meta-analyses argued for statistically and practically significant effects of violent video games on aggressive outcomes. Moreover, both applied trim-and-fill and found little evidence or adjustment for research bias. However, visual inspection of the funnel plot often reveals alarming asymmetry, particularly in the case of those studies which \citet{Anderson:etal:2010} call ``best-practices'' studies.

% Methods
We acquired data from \citet{Anderson:etal:2010} and \citet{Greitemeyer:Mugge:2014}. Because the data were analyzed using Comprehensive Meta-Analysis, many studies were entered with separate rows for different outcomes or subsamples within studies. We consulted with the original authors as how best to aggregate rows within studies and reproduce their provided estimates.

We then followed the PET-PEESE procedure, fitting a weighted-least-squares regression model predicting effect size (Pearson's r converted to Fisher's z) as a linear function of the standard error, with weights inversely proportional to the square of the standard error. In the case that the PET regression found a statistically significant effect after accounting for publication bias, PEESE was applied, predicting effect size as a quadratic function of the standard error. PET or PEESE estimates are provided regardless of whether statistically significant bias was observed according to recommendations by \citet[p. 20-21]{Stanley:Doucouliagos:20XX}: ``To be conservative, one should always use [the PET or PEESE estimate] even if there is insufficient evidence of publication selection because the Egger test [of publication bias] is known to have low power.''

Because PET-PEESE is a regression method, it is likely to perform poorly when there are few datapoints. Therefore, our analysis is restricted to effects and experimental paradigms with at least fifteen %twenty?
independent effect sizes. Data and code have been made available online in the case that the reader nevertheless wants to generate PET-PEESE estimates for more sparse datasets.

For sensitivity analysis, we remove datapoints with a Cook's distance of more than 0.5, as these may have excessive influence over the slope of the regression line. Estimates are provided with and without these influential observations.

% Results
% Anderson et al.
We reproduce estimates from \citet{Anderson:etal:2010} and apply PET-PEESE. Sufficient datapoints were available to re-analyze experimental studies of aggressive affect, aggressive behavior, aggressive cognition, and physiological arousal, as well as cross-sectional studies of aggressive affect, aggressive behavior, and aggressive cognition. Studies are further divided into ``best-practices'' and ``not best-practices'' studies per \citet{Anderson:etal:2010} as sample sizes permit.


\section{Experiments}
\subsection{Aggressive affect}

It is interesting to note that, contrary to the findings of \citet{Anderson:etal:2010}, we find that effects are larger in not-best-practices experiments than in best-practices experiments after adjusting for selection and publication bias. This is likely because the best-practices criteria are flawed or flexibly applied. An example of a flawed criterion is their recommendation that games be matched in pilot testing. Many studies which did pilot-test their games used such small samples that a lack of statistical significance did not necessarily constitute evidence for the null. % For more see Hilgard Engelhardt Bartholow & Rouder
Flexible application of the criterion includes % the exclusion/inclusion nonsense with Sonic vs Mortal Kombat being bad but GTA3 vs Simpsons Hit-n-Run being OK.

% Gone from predicting 4.4\% of variance to 1.4\%

% Discussion
Our findings suggest that the effects of violent video games on aggressive thoughts, feelings, and behaviors have been overestimated. 

Clearly, publication bias, meta-analytic bias, and perhaps flexible analysis are problems in this literature as they are in so many other literatures. We recommend the use of large, pre-registered, open-data research projects, ideally with collaboration across antagonistic research teams. We also feel that, in light of the current results, there is reason enough for skepticism. Attempts to establish consensus, disseminate research findings to the public, or develop strategies for thwarting denialism may be premature. We hope that the years ahead foster a civil, unprejudiced, and transparent application of the scientific method. 

That said, PET-PEESE meta-regression is a relatively new technique and its limitations may not yet be fully understood. For these reasons, we suggested that the provided estimates be considered possible bias-adjusted estimates rather than corrected estimates of the true effect. Also, PET-PEESE is a regression method, and like most regression methods, results can be misleading when few datapoints are available. PET-PEEESE also extrapolates outside the model, estimating the effect when standard error is zero when no datapoints have zero error. Thus, rather than regard the present meta-analysis as having identified the true effect size, we instead suggest it as food for meta-analytic thought. 

% Problems with "consensus"
Scientific consensus is probably a good thing in certain contexts, but here I see it as stifling. I don't think Ferguson's research is great \citet[and we have even criticized his research as using too small of samples, see ][]{Hilgard:etal:2015}, but it seems that the reason he arrives at different results than other small studies is that he is less averse to the null. We urge researchers not to fall into the trap of "obedient replication, where investigators feel that the prevailing school of thought is so dominant that finding consistent results is perceived as a sign of being a good scientist and there is no room for dissenting results and objections." \citep{Ioannidis:2012} %page number needed, http://www.jpsychores.com/article/S0022-3999%2812%2900263-2/abstract

Research may have been too hasty in establishing a consensus, formulating strategies to exclude skeptics from public debate, serving as expert court witnesses, issuing public policy statements. The present results highlight the importance of transparent and unbiased research and meta-analysis, the archival and sharing of meta-analytic data for further scrutiny and analysis, 

% What went wrong with Anderson 2010?
% Practical considerations for meta-analysis / reducing bias in meta-analysis
Selection bias in meta-analysis. 
``Meta-analyses are known to suffer from the `junk in / junk out' phenomenon (which is unlikely to be fixed by `best-practices' efforts, when scholars may simply value their own junk higher than the junk of others.)'' \citep{Ferguson:Heene:2012}
We observe some instances of flexible application of the best-practices criteria offered by \citet{Anderson:etal:2010}. One criterion is that the violent and nonviolent game must be sufficiently different in violent content. Application of this criterion was inconsistent. Comparisons between the violent game {\em Mortal Kombat} and the nonviolent game {\em Sonic the Hedgehog} were discarded as not-best practices \citep[e.g.,][]{CITATION:NEEDED:2000} because ``the nonviolent game contained violence'' \citep[supplementary materials]{Anderson:etal:2010}. Another study comparing a racing game {\em Moto Racer} against the violent game \textbf{I DONT REMEMBER WHICH} \citep{Brooks:1996} was excluded for similar reasons, but we were not able to find any violent content in {\em Moto Racer}. Meanwhile, other studies involving comparisons between violent and not-entirely-nonviolent games were included. \citet{Konijn:etal:2007} was included, even though the game {\em Final Fantasy} is as violent or more violent than {\em Sonic the Hedgehog}. \citet{Brady:Mathews:2006} was included as best-practices despite comparing the violent {\em Grand Theft Auto 3} to the T-rated not-so-nonviolent game {\em Simpsons Hit and Run}. 
% The study by \citet{Brady:Matthews:2006} is a particularly interesting example, as the same study was included in its dissertation form \citep{Brady:2006} and listed as not-best-practices. 
% Craig: "As I recall, the dissertation study was the same as the B & M 2006 paper. So, there is no real contradiction here, other than that the dissertation should have been listed differently in the supplemental materials." I'll have to check the estimates and ks.
Flexibility in the application of this criterion may have contributed to selection biases, inflating the naive meta-analytic estimate while reducing the PET-PEESE estimate. A better approach might be to have manipulations rated by research assistants naive to hypotheses or to study results, or to seek a statistical quantification of the difference in violence between games, such as a Cohen's $d$ describing a manipulation check.

Selection bias may also have been facilitated by the application of best-practices criterion 5: The outcome measure could reasonably be expected to be influenced by the independent variable if the hypothesis were true. For an example of selection bias, see \citet[study 2]{Anderson:etal:2004}. In this study, participants were assigned to play a violent or nonviolent game, then complete a competitive reaction-time task measure of aggressive behavior with either an ambiguously or unambiguously provoking confederate. A significant effect was found amount the 90  subjects assigned to the ambiguous provocation condition ($r = .25$), but not among the 90 subjects assigned to the unambiguous provocation condition ($r = -.03$). These 90 subjects with a nonsignificant effect were dropped from both the best-practices and not-best-practices meta-analyses. When asked for comment, the authors said ``Only the ambiguous provocation condition was used because we now know that the unambiguous (increasing) provocation version of the task is not as sensitive to a variety of independent variables as is the ambiguous provocation pattern. In other words, the increasing provocation conditions don't meet Criterion 5.'' While it is possible that only one form of the task is sensitive to the manipulation, the meta-analysis does not seek to model such fine-grained moderators. We are also dismayed by the prospect that the validity or invalidity of measurements can be determined on whether they provide the researcher with the desired $p<.05$ in an experiment. Since a significant effect in either the ambiguous or unambiguous provocation group would be taken as evidence for an effect of violent video games, we feel that the selective exclusion of groups for not demonstrating such an effect risks introducing selection bias.

The \citep{Anderson:etal:2010} meta-analysis did make an attempt to collect and analyze unpublished studies. That the resulting analysis remained biased despite these attempts gives us concern that searching for unpublished studies may not actually alleviate bias in meta-analysis. We note that few of these unpublished studies were accepted as best-practices research. It is also possible that some null findings were ultimately re-analyzed until significant findings were obtained, and so the null findings appear in neither the unpublished nor published research literature.

In all cases, the clear and accessible archival of meta-analytic data is a considerable aid to research transparency. We commend Anderson and colleagues for sharing the data and for responding to questions as to how best reproduce their analyses. The present analyses would not have been possible without their assistance. We suggest that future meta-analyses routinely include the data, funnel plots (in supplemental materials, if need be), and other supplementary materials. Meta-analyses that cannot be personally inspected or reproduced should be regarded with caution.

% Reducing bias in empirical research
We need to restructure incentives so that failing to reproduce an effect is an accepted research outcome. %Obedient replication.
Part of the mess appears to be that researchers quickly took the basic phenomenon for granted and began to cast about for more sophisticated models that would advance theory. In some cases, these more sophisticated models led to attempted conceptual, rather than direct, replications. In other cases, these more sophisticated models required some amount of post-hoc moderator munging. Suppose we conduct a basic two-cell violent and nonviolent game manipulation. Results reveal no significant effect of violent games, an unacceptable and unpublishable result. Thus, we begin to test post-hoc moderator after moderator: perhaps the effects hold for males, but not females? Those high in trait anger, but not those low in trait anger? Morning experiments, or night experiments? Such post-hoc exploratory analyses are, of course, important and valuable \citep[indeed, we present them ourselves in][]{Engelhardt:etal:2014}, but become dangerous when presented as confirmatory, or when patterns of statistical significance are taken to identify the validity or invalidity of the measures.

% Theoretical considerations
Note that the effect is larger in less-controlled studies (cross-sectional vs experimental). We are also reluctant to apply PET-PEESE to longitudinal studies, as there are not enough data points to permit a good estimate. It is possible that some causal association exists in the real world on the scale of hundreds of hours of gameplay. However, the current results indicate that it is unlikely that a substantial and reliable effect can be obtained from having an undergraduate play a game for fifteen minutes, then assign noise-blasts to a confederate.

We need to establish the existence of the phenomenon before attempting to elaborate on the effect, its moderators, and its broader implications. If the effect is indeed as small as we estimate here (r = .07 or generously .16) then identifying meaningful and reliable moderators of the effect will be challenging. To test this main effect with 80\% power, one-tailed, would require between 120 and 630 subjects per cell. Moderators of the effect, if any, should be expected to be on a similar scale, and so may take about 800 subjects to detect with 80\% power (for effect size f=.10, cite GPOWER). This may explain the counter-intuitive finding that effect sizes among children are not significantly larger than effects among adults: differences, if any, will be small, and are likely to be obscured by research bias. 

It may also be necessary to reevaluate our theories of aggressive behavior, specifically the General Aggression Model (GAM). According to GAM, aggressive feelings, thoughts, and behaviors are all closely related processes which feed into each other. Aggressive feelings are thought to inspire aggressive behavior, and aggressive thoughts increase the likelihood of aggressive behavior. The present analysis indicates that violent video games have minimal effect on aggressive feelings \citep[a finding paralleled by][]{Przybylski:etal:2014} but that, depending on inclusion criteria and the exclusion of influential data points, there may be some effect on aggressive thoughts and some smaller effect on aggressive behavior. If this is the case, changes in aggressive behavior caused by violent game exposure would seem to be more related to aggressive-thought accessibility than to aggressive feelings. In our own research, we have attempted to collect measures of aggressive thoughts, feelings, and behavior from participants within a single session and found them to correlate poorly \citep{Engelhardt:etal:2015}.

\end{document}