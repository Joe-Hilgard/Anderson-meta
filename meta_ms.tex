\documentclass[jou]{apa6}
%\documentclass{article}
\usepackage[natbibapa]{apacite}
\usepackage[longnamesfirst]{natbib}

\rightheader{Bias in Violent Games Research}
\shorttitle{Bias in Violent Games Research}

\leftheader{Hilgard et al.}

\author{Joseph Hilgard, Christopher R. Engelhardt, Bruce D. Bartholow, and Jeffrey N. Rouder}
% Broad Consensus? Open the coffin? Much ado about something out of nothing?
\title{There Should Not Be Broad Consensus: Bias in Violent Games Research}

\affiliation{University of Missouri}

\note{\begin{flushleft}

\vspace{1in}
Joseph Hilgard\\
jhilgard@gmail.com
\end{flushleft}
}

\abstract{}

\begin{document}
\maketitle

%1. It is believed by many that violent games cause violent outcomes and that the experimental research is convincing, but others are less persuaded

%2. Sources of controversy include failures to replicate and speculation about biased analysis, reporting, and meta-analysis of research.

%3. Results from naive meta-analysis and consequent certainty among researchers. Nailing the coffin shut on doubts (Huesmann, 2010), Why is it so hard to convince / new media old problems (Donnerstein, Strasburger, Bushman, 2014), There is broad consensus (Bushman et al), how do we silence denialists (Anderson et al)

% What is publication bias and small-study effects?
Publication bias is the phenomenon that studies with statistically significant ($p<.05$) findings are more likely to be submitted and accepted for publication. Publication bias is a problem that contributes to the overestimation of effect sizes and the propagation of Type I error. It is an especially pernicious problem for meta-analysis, as the selective reporting of studies that ``work'' (i.e. attain significance) leads to an overestimated effect size. For this reason, a crucial part of meta-analysis is to attempt to inspect and adjust for the degree of publication bias.


\begin{quote}
Despite thousands of research studies on media effects, many people simply refuse to believe them. Some academics may contribute to this because they like to "buck the establishment," which is an easy way to promote themselves and their research. Of course, many people still believe that President Obama wasn't born in the United States, President Kennedy wasn't assassinated, men didn't walk on the moon, and the Holocaust didn't occur. \citep[p. 572]{Strasburger:etal:2014}
\end{quote}
%end blockquote

So confident are some believers of violent-media effects that any remaining skepticism seems to be due to unfathomable stubbornness. 
Sufficiently satisfied that the phenomenon has been demonstrated time and again in laboratory research, some authors have moved on to advancing theories as to why skepticism remains, suggesting that skeptics are influenced by psychological reactance or cognitive dissonance. Certain skeptics are compared to moon-landing deniers (or worse), described as being incapable to persuade by any amount of evidence \citep{Strasburger:etal:2014}) Researchers have begun to propose ways to ``advance the debate'' by speaking directly to the public to avoid skeptics within academia \citep{Strasburger:Donnerstein:2014}. Claims of consensus have been advanced \citep{Bushman:etal:2014}, and attempts have been made to separate ``true media violence scientists'' who believe in the effect from less-expert sources motivated to deny evidence \citep{Anderson:etal:2014}.

% Fail-safe N stinks
Meta-analytic techniques for assessing and correcting for publication bias have been applied to this literature, but these are of questionable utility. Fail-safe N, in theory, provides the number of studies %(subjects?)
that would need to be hypothetically hiding in file-drawers in order to reduce an effect to non-significance. This technique is flawed in that it assumes an effect size of exactly zero in all censored studies, does not consider the possibility of flexible analysis (e.g. p-hacking), and does not provide an estimate of a bias-adjusted effect size.

% Trim-and-fill and how it's bad
Another popular bias-adjustment technique, trim-and-fill \citep{Duval:Tweedie:2000} attempts to detect and adjust for bias through inspection of the funnel plot. If the funnel plot is asymmetrical, the procedure ``trims'' off the most extreme study and imputes a hypothetical censored study reflected around the funnel plot's axis of symmetry. Studies are trimmed and filled until the funnel plot is no longer significantly asymmetrical. This is not an effective adjustment for bias, as the assumptions of trim-and-fill are unlikely to be met. Studies are not likely to be censored on the basis of the effect size, but rather, on the basis of their statistical significance. It is argued that trim-and-fill does a poor job of providing an adjusted effect size, adjusting too much when there is no bias and adjusting too little when there is bias. %Could cite data colada post

Meta-regression techniques instead consider the relationship between effect size and precision. In an asymmetrical funnel plot, larger samples yield smaller effects, as would be expected if studies were censored when not attaining statistical significance or when studies are flexibly analyzed in order to attain statistical significance. The PET-PEESE meta-regression \citep{Stanley:Doucouliagos:2013} %See carter & McCullough for better citations
uses the published literature to estimate the relationship between precision and effect size. A weighted regression is fit to describe how the effect size changes with increasing experimental precision, then extrapolates to estimate what the ``true effect'' would be in a hypothetical study with perfect precision. This meta-regression technique has been previously applied by \citet{Carter:McCullough:2014} to inspect the amount of evidence for ``ego depletion'', the phenomenon of fatigue in self-control. They found that after adjusting for publication bias, PET-PEESE suggested an absence of evidence for the phenomenon, and therefore recommended a large-sample pre-registered replication effort.

We apply PET-PEESE meta-regression to two recent meta-analyses of violent video game effects, one covering research up until 2009 \citep{Anderson:etal:2010} and the other research between 2009 and 2013 \citep{Greitemeyer:Mugge:2014}. Both meta-analyses argued for statistically and practically significant effects of violent video games on aggressive outcomes. Moreover, both applied trim-and-fill and found little evidence or adjustment for research bias. However, visual inspection of the funnel plot often reveals alarming asymmetry, particularly in the case of those studies which \citet{Anderson:etal:2010} call ``best-practices'' studies.

% Methods
We acquired data from \citet{Anderson:etal:2010} and \citet{Greitemeyer:Mugge:2014}. Because the data were analyzed using Comprehensive Meta-Analysis, many studies were entered with separate rows for different outcomes or subsamples within studies. However, the assumption of the current PET-PEESE model is that entire studies are censored or re-analysed, and thus each study should constitute a single observation. We consulted with the original authors as how best to aggregate rows within studies and reproduce their provided estimates.

We then followed the PET-PEESE procedure, fitting a weighted-least-squares regression model predicting effect size (Pearson's r converted to Fisher's z) as a linear function of the standard error, with weights inversely proportional to the square of the standard error. In the case that the PET regression found a statistically significant effect after accounting for publication bias, PEESE was applied, predicting effect size as a quadratic function of the standard error. PET or PEESE estimates are provided regardless of whether statistically significant bias was observed according to recommendations by \citet[p. 20-21]{Stanley:Doucouliagos:20XX}: ``To be conservative, one should always use [the PET or PEESE estimate] even if there is insufficient evidence of publication selection because the Egger test [of publication bias] is known to have low power.''

Because PET-PEESE is a regression method, it is likely to perform poorly when there are few datapoints. Therefore, our analysis is restricted to effects and experimental paradigms with at least ten independent effect sizes. %citation needed from Carter & McCullough, 2014
Data and code have been made available online in the case that the reader nevertheless wants to generate PET-PEESE estimates for more sparse datasets.

For sensitivity analysis, we remove datapoints with a Cook's distance of more than 0.5, as these may have excessive influence over the slope of the regression line. Estimates are provided with and without these influential observations.

Two studies were removed from the meta-analysis. \citet[study 1]{Matsuzaki:etal:2006} was removed because its entered effect sizes were unusually large for their precision (i.e., effects on aggressive behavior $r = .60$ and aggressive cognition $r = .53$), highly influential, and could not be found as entered by inspection of the original article. Similarly, \citet{Panee:Ballard:2002} was removed because the study tested the effects of violent primes during gameplay and not the effects of violent gameplay itself. %might need to follow up on this

% Results


% Anderson et al.
We reproduce estimates from \citet{Anderson:etal:2010} and apply PET-PEESE. Sufficient datapoints were available to re-analyze experimental studies of aggressive affect, aggressive behavior, aggressive cognition, and physiological arousal, as well as cross-sectional studies of aggressive affect, aggressive behavior, and aggressive cognition. Studies are further divided into ``best-practices'' and ``not best-practices'' studies per \citet{Anderson:etal:2010} as sample sizes permit.


\section{Experiments}
\subsection{Aggressive affect}
Among studies selected as best-practices, PET found no significant effect of violent games on aggressive affect. An effect of bias was clear.

When including both best- and not-best-practices studies, PET still found no significant effect of violent games on aggressive affect. 
We proceeded to explore the analysis by excluding observations with excessive influence (e.g. Cook's distance $>$ .5). After removing \citet{Ballard:Weist:1996}, PET estimates were still not significant whether for best-practices or all studies. No other data points had excessive influence. 

%nonexp?

\subsection{Aggressive behavior}
Among studies selected as best-practices, PET found both a significant effect of violent games on aggressive behavior and a significant effect of bias. Because the intercept was significant, PEESE was then applied. PEESE estimated the effect as $r = .16$, substantially smaller than that of the naive or trim-and-fill estimates.

Among all studies, PET found a significant effect of violent games on aggressive behavior and no significant effect of bias. PEESE estimated the effect as $r = .16$, again smaller than that of the naive or trim-and-fill estimates. The closely matched effect sizes suggest that effects are not larger in best-practices research, just more biased.

We proceeded to explore the analysis by assessing the influence of individual studies. The largest study in the best-practices sample, \citet[Study 1]{Anderson:etal:2007}, had a Cook's distance just above .5. Excluding this influential observation increased the PEESE estimate ($r=.19$) and naive estimate ($r = .24$). The decision to include or exclude this study will surely be a point of some contention given its influence over the PEESE estimate. On the other hand, it is the largest study by far in the sample, more than twice as large as the next-largest study. 

\subsection{Aggressive cognition}
Among studies selected as best-practices, PET found neither a significant effect of violent games or a significant effect of bias. The effect size estimate was $r = .11$, much smaller than the naive estimate of $r = .22$.

Among all studies, PET found a significant effect of violent games on aggressive cognitions ($p = .049, r = .14$). PEESE estimated the effect as $r = .18$, again smaller than the naive or trim-and-fill estimates. 

No observations had excessive distance. 

\subsection{Physiological Arousal}


\section{Discussion}
% Gone from predicting 4.4\% of variance to 1.4\%
Our findings suggest that the effects of violent video games on aggressive thoughts, feelings, and behaviors have been overestimated. After accounting for the relationship between sample size and effect size, we estimate that violent games have no significant effect on aggressive feelings, very small effects on aggressive cognitions, and small effects on aggressive behavior. Effects on aggressive behavior were the only significant effect among those investigated, but effects were estimated as rather smaller than that originally reported by \citet{Anderson:etal:2010}. To put into perspective the difference between $r = .21$ and $r = .16$, we consider a power analysis for a hypothetical experiment with 80\% power, one-tailed. In such an experiment, $r = .21$ requires a sample size of $n = 136$, while $r = .16$ requires a substantially larger $n = 237$. %It may be necessary to demand larger sample sizes from game-effects research, as samples of less than 100 subjects are clearly underpowered. % Fuck Greitemeyer.

Clearly, publication bias, meta-analytic bias, and perhaps flexible analysis are problems in this literature as they are in so many other literatures. We recommend the use of large, pre-registered, open-data research projects, ideally with collaboration across antagonistic research teams. We also feel that, in light of the current results, there is reason enough for skepticism. Attempts to establish consensus, disseminate research findings to the public, or develop strategies for thwarting denialism may be premature. We hope that the years ahead foster a civil, unprejudiced, and transparent application of the scientific method. 

It is interesting to note that, contrary to the findings of \citet{Anderson:etal:2010}, we find that effects are equal or larger in not-best-practices experiments than in best-practices experiments after adjusting for selection and publication bias. This is likely because the best-practices criteria are flawed or flexibly applied. An example of a flawed criterion is their recommendation that games be matched in pilot testing. Many studies which did pilot-test their games used such small samples that a lack of statistical significance did not necessarily constitute evidence for the null. % For more see Hilgard Engelhardt Bartholow & Rouder


That said, PET-PEESE meta-regression is a relatively new technique and its limitations may not yet be fully understood. For these reasons, we suggested that the provided estimates be considered possible bias-adjusted estimates rather than corrected estimates of the true effect. Also, PET-PEESE is a regression method, and like most regression methods, results can be misleading when few datapoints are available. PET-PEEESE also extrapolates outside the model, estimating the effect when standard error is zero when no datapoints have zero error. Thus, rather than regard the present meta-analysis as having identified the true effect size, we instead suggest it as food for meta-analytic thought. 

% Problems with "consensus"
Scientific consensus is probably a good thing in certain contexts, but here I see it as stifling. I don't think Ferguson's research is great \citet[and we have even criticized his research as using too small of samples, see ][]{Hilgard:etal:2015}, but it seems that the reason he arrives at different results than other small studies is that he is less averse to the null. We urge researchers not to fall into the trap of "obedient replication, where investigators feel that the prevailing school of thought is so dominant that finding consistent results is perceived as a sign of being a good scientist and there is no room for dissenting results and objections." \citep{Ioannidis:2012} %page number needed, http://www.jpsychores.com/article/S0022-3999%2812%2900263-2/abstract

Research may have been too hasty in establishing a consensus, formulating strategies to exclude skeptics from public debate, serving as expert court witnesses, issuing public policy statements. The present results highlight the importance of transparent and unbiased research and meta-analysis, the archival and sharing of meta-analytic data for further scrutiny and analysis, and the continued tolerance of public debate and skepticism.

% What went wrong with Anderson 2010?
% Practical considerations for meta-analysis / reducing bias in meta-analysis
\subsection{Selection bias in meta-analysis} 
``Meta-analyses are known to suffer from the `junk in / junk out' phenomenon (which is unlikely to be fixed by `best-practices' efforts, when scholars may simply value their own junk higher than the junk of others.)'' \citep{Ferguson:Heene:2012}
We observe some instances of flexible application of the best-practices criteria offered by \citet{Anderson:etal:2010}. One criterion is that the violent and nonviolent game must be sufficiently different in violent content. Application of this criterion was inconsistent. Comparisons between the violent game {\em Mortal Kombat} and the nonviolent game {\em Sonic the Hedgehog} were discarded as not-best practices \citep[e.g.,][]{CITATION:NEEDED:2000} because ``the nonviolent game contained violence'' \citep[supplementary materials]{Anderson:etal:2010}. Another study comparing a racing game {\em Moto Racer} against the violent game \textbf{I DONT REMEMBER WHICH} \citep{Brooks:1996} was excluded for similar reasons, but we were not able to find any violent content in {\em Moto Racer}. Meanwhile, other studies involving comparisons between violent and not-entirely-nonviolent games were included. \citet{Konijn:etal:2007} was included, even though the game {\em Final Fantasy} is as violent or more violent than {\em Sonic the Hedgehog}. \citet{Brady:Mathews:2006} was included as best-practices despite comparing the violent {\em Grand Theft Auto 3} to the T-rated not-so-nonviolent game {\em Simpsons Hit and Run}. 
% The study by \citet{Brady:Matthews:2006} is a particularly interesting example, as the same study was included in its dissertation form \citep{Brady:2006} and listed as not-best-practices. 
% Craig: "As I recall, the dissertation study was the same as the B & M 2006 paper. So, there is no real contradiction here, other than that the dissertation should have been listed differently in the supplemental materials." I'll have to check the estimates and ks.
Flexibility in the application of this criterion may have contributed to selection biases, inflating the naive meta-analytic estimate while reducing the PET-PEESE estimate. A better approach might be to have manipulations rated by research assistants naive to hypotheses or to study results, or to seek a statistical quantification of the difference in violence between games, such as a Cohen's $d$ describing a manipulation check.

Selection bias may also have been facilitated by the application of best-practices criterion 5: The outcome measure could reasonably be expected to be influenced by the independent variable if the hypothesis were true. For an example of selection bias, see \citet[study 2]{Anderson:etal:2004}. In this study, participants were assigned to play a violent or nonviolent game, then complete a competitive reaction-time task measure of aggressive behavior with either an ambiguously or unambiguously provoking confederate. A significant effect was found amount the 90  subjects assigned to the ambiguous provocation condition ($r = .25$), but not among the 90 subjects assigned to the unambiguous provocation condition ($r = -.03$). These 90 subjects with a nonsignificant effect were dropped from both the best-practices and not-best-practices meta-analyses. When asked for comment, the authors said ``Only the ambiguous provocation condition was used because we now know that the unambiguous (increasing) provocation version of the task is not as sensitive to a variety of independent variables as is the ambiguous provocation pattern. In other words, the increasing provocation conditions don't meet Criterion 5.'' While it is possible that only one form of the task is sensitive to the manipulation, the meta-analysis does not seek to model such fine-grained moderators. We are also dismayed by the prospect that the validity or invalidity of measurements can be determined on whether they provide the researcher with the desired $p<.05$ in an experiment. Since a significant effect in either the ambiguous or unambiguous provocation group would be taken as evidence for an effect of violent video games, we feel that the selective exclusion of groups for not demonstrating such an effect risks introducing selection bias.

% This turned out to be less exciting than I'd thought
A similar selection influences the results as entered from \citet{Carnagey:Anderson:2005}. Participants played one of three versions of the game {\em Carmageddon 2}: in one version, players gained points for violence against pedestrians and other drivers (violent-reward condition); in another version, players lost points for violence against pedestrians and other drivers (violent-punishment condition); in a last condition, pedestrians and other drivers were removed from the game so that players could not perpetrate any violence (nonviolent condition). Because the research question asks whether violent game content increases aggressive outcomes, the appropriate effect size would contrast the violent conditions (whether violent-reward or violent-punishment) against the nonviolent game condition. Instead, the violent-punishment condition is dropped and effect sizes entered for the comparison of violent-reward and nonviolent games. % One could easily make the counter-argument that violent-reward is the only form that parallels real-world violent gameplay.

%  Selection bias in choice of effect sizes
Selection bias may also influence which effect size among those reported was entered into analysis. As a general rule, it seems that \citet{Anderson:etal:2010} attempted to avoid subjectivity in effect size entry by averaging all reported effect sizes together. However, on several instances, effect sizes were not averaged together, but rather the single largest available effect size was selected. For example, in the aforementioned \citet[study 2]{Anderson:etal:2004}, the effect of violent games on the first trial of the CRTT was entered, but not the reported effect size on the other 24 trials of the CRTT. In another example, \citet[study 2]{Anderson:etal:2007} report effects of violent video games on violent behavior ($r = .35$), physical aggression ($r = .46$), and verbal aggression ($r = .25$). Only the largest of these was used as the effect size of violent games on aggressive behavior. Similarly, the effect of violent games on aggressive affect was entered as the effect on trait anger ($r = .23$) but not the effect on trait hostility ($r = .21$). Selection of the largest effects risks capitalizing on chance and systematically overestimating the true effect.

We note further selection bias in the interpretation of violent games on physiological arousal. As presented by \citet{Anderson:etal:2010}, violent games cause significant increases in physiological arousal, e.g. heart rate or blood pressure. However, in researching this meta-analysis, we became aware of studies in which null effects of violent games were excluded from meta-analysis. For example, in the best-practices studies by \citet{Carnagey:Anderson:2005}, the violent and nonviolent versions of the game were not found to effect players' physiological arousal. Rather than present these findings as null results of violent games on physiological arousal, the authors presented this result as evidence that the violent and nonviolent games were matched stimuli. We observe a similar treatment in the meta-analysis: the null results on physiological arousal were omitted from the meta-analysis investigating effects of violent games on physiological arousal. We find this approach to be too flexible and forbids falsification of the theory, as concordant results are taken as evidence for the theory, but discordant results are excluded from consideration.

Some null findings were censored or miscoded. For example, in the course of the experiment reported in \citet{Carnagey:Anderson:2005}, a nonexperimental assessment was also made of the effects of previous violent game exposure on aggressive outcomes. Nonexperimental effects were entered into the \citet{Anderson:etal:2010} meta-analysis but were much larger (and more statistically significant) than reported in the \citet{Carnagey:Anderson:2005} manuscript.

The \citep{Anderson:etal:2010} meta-analysis did make an attempt to collect and analyze unpublished studies. That the resulting analysis remained biased despite these attempts gives us concern that searching for unpublished studies may not actually alleviate bias in meta-analysis. We note that few of these unpublished studies were accepted as best-practices research. It is also possible that some null findings were ultimately re-analyzed until significant findings were obtained, and so the null findings appear in neither the unpublished nor published research literature.

In all cases, the clear and accessible archival of meta-analytic data is a considerable aid to research transparency. We commend Anderson and colleagues for sharing the data and for responding to questions as to how best reproduce their analyses. The present analyses would not have been possible without their assistance. We suggest that future meta-analyses routinely include the data, funnel plots (in supplemental materials, if need be), and other supplementary materials. Meta-analyses that cannot be personally inspected or reproduced should be regarded with caution.

% Reducing bias in empirical research
We need to restructure incentives so that failing to reproduce an effect is an accepted research outcome. %Obedient replication.
Part of the mess appears to be that researchers quickly took the basic phenomenon for granted and began to cast about for more sophisticated models that would advance theory. In some cases, these more sophisticated models led to attempted conceptual, rather than direct, replications. In other cases, these more sophisticated models required some amount of post-hoc moderator munging. Suppose we conduct a basic two-cell violent and nonviolent game manipulation. Results reveal no significant effect of violent games, an unacceptable and unpublishable result. Thus, we begin to test post-hoc moderator after moderator: perhaps the effects hold for males, but not females? Those high in trait anger, but not those low in trait anger? Morning experiments, or night experiments? Such post-hoc exploratory analyses are, of course, important and valuable \citep[indeed, we present them ourselves in][]{Engelhardt:etal:2014}, but become dangerous when presented as confirmatory, or when patterns of statistical significance are taken to identify the validity or invalidity of the measures.

% Theoretical considerations
Note that the effect is larger in less-controlled studies (cross-sectional vs experimental). We are also reluctant to apply PET-PEESE to longitudinal studies, as there are not enough data points to permit a good estimate. It is possible that some causal association exists in the real world on the scale of hundreds of hours of gameplay. However, the current results indicate that it is unlikely that a substantial and reliable effect can be obtained from having an undergraduate play a game for fifteen minutes, then assign noise-blasts to a confederate.

We need to establish the existence of the phenomenon before attempting to elaborate on the effect, its moderators, and its broader implications. If the effect is indeed as small as we estimate here (r = .07 or generously .16) then identifying meaningful and reliable moderators of the effect will be challenging. To test this main effect with 80\% power, one-tailed, would require between 120 and 630 subjects per cell. Moderators of the effect, if any, should be expected to be on a similar scale, and so may take about 800 subjects to detect with 80\% power (for effect size f=.10, cite GPOWER). This may explain the counter-intuitive finding that effect sizes among children are not significantly larger than effects among adults: differences, if any, will be small, and are likely to be obscured by research bias. 

It may also be necessary to reevaluate our theories of aggressive behavior, specifically the General Aggression Model (GAM). According to GAM, aggressive feelings, thoughts, and behaviors are all closely related processes which feed into each other. Aggressive feelings are thought to inspire aggressive behavior, and aggressive thoughts increase the likelihood of aggressive behavior. The present analysis indicates that violent video games have minimal effect on aggressive feelings \citep[a finding paralleled by][]{Przybylski:etal:2014} but that, depending on inclusion criteria and the exclusion of influential data points, there may be some effect on aggressive thoughts and some smaller effect on aggressive behavior. If this is the case, changes in aggressive behavior caused by violent game exposure would seem to be more related to aggressive-thought accessibility than to aggressive feelings. In our own research, we have attempted to collect measures of aggressive thoughts, feelings, and behavior from participants within a single session and found them to correlate poorly \citep{Engelhardt:etal:2015}.

\end{document}