% Note: Ferguson:2007a is "Evidence for publication bias"; Ferguson:2007b is "Good Bad and Ugly: Meta-analytic review"

% It has also been widely cautioned that because trim and fill and some other techniques for assessing publication bias are based on an association between effect size and sample size, other explanations of this association should be considered. For example, effect sizes in experimental studies may be larger than those in crosssectional or longitudinal studies due to the reduced error variance that results from tight experimental controls; researchers may know this and therefore may intentionally plan to use larger sample sizes when conducting nonexperimental studies. Similarly, in some research contexts with very large sample sizes (e.g., national surveys) a researcher may have to use less precise measures (e.g., fewer items) that result in smaller effect sizes. In sum, it is possible that the effects in the studies with small samples really are larger than those in the studies with large samples (cf. Sterne and Egger, 2005). \citep[p. 152-153]{Anderson:etal:2010}
% Anderson et al also complain that \citet{Ferguson:2007a} used ``a very small set of available studies'' and ``For example, counter to widely accepted procedures for reducing the impact of publication bias, only published articles were included in the analyses and then procedures for addressing publication bias were misinterpreted. Also, studies published prior to 1995 were ignored and a large number of studies published since that time apparently were missed.'' \citep[p. 152]{Anderson:etal:2010}

\documentclass[man]{apa6}
%\documentclass{article}
\usepackage[natbibapa]{apacite}
\usepackage[longnamesfirst]{natbib}

\rightheader{Bias in Violent Games Research}
\shorttitle{Bias in Violent Games Research}

\leftheader{Hilgard et al.}

\author{Joseph Hilgard, Christopher R. Engelhardt, Bruce D. Bartholow, and Jeffrey N. Rouder}
% Broad Consensus? Open the coffin? Much ado about something out of nothing?
\title{There Should Not Be Broad Consensus: Bias in Violent Games Research}

\affiliation{University of Missouri}

\authornote{
Joseph Hilgard, University of Missouri-Columbia.
Please direct correspondence regarding this article to Joseph Hilgard. E-mail: jhilgard@gmail.com
}

\abstract{
Violent video games are thought to be a significant cause of aggressive thoughts, feelings, and behaviors.
In the past year, debates have reached a head as proponents have begun to search for ways to establish a consensus among ``true'' experts who believe in the effect and exclude skeptics and other ``false'' experts from public engagement with the lay audience.
The wisdom of this gambit depends on the strength of the evidence. If the evidence is incontrovertibly strong, remaining skepticism would reflect an unwillingness to learn from evidence, perhaps caused by conflicts of interest, biased assimilation of evidence, or other bad practices and bad faith. On the other hand, if the evidence is fallible, attempts by proponents to quash debate are premature and represent a threat to academic freedom.
In the present manuscript, we examine previous meta-analytic evidence and apply PET-PEESE meta-regression, a statistical meta-analytic technique that tests and adjusts for small-study effects including (but not limited to) publication bias and flexible analysis. We find that the evidence for effects of violent games on aggressive outcomes in best-practices experiments have been overestimated through publication, selection, or analytic bias.
We conclude that the extant evidence is contaminated by bias and is not sufficient for decisive conclusions. We encourage an open, transparent, and pre-registered research process to test the existence of the basic phenomenon.
}

\begin{document}
\maketitle

%1. It is believed by many that violent games cause violent outcomes and that the experimental research is convincing, but others are less persuaded
Do violent games make their players more aggressive? Despite decades of research and hundreds of studies, there remains scientific debate. Results have been aggregated in several meta-analyses, the most-cited and most comprehensive of which \citep{Anderson:etal:2010} claimed decisive evidence for effects on every outcome in every study design: experimental, cross-sectional, and longitudinal. 
This has been hailed by some as decisive evidence \citep{Bushman:etal:2010,Huesmann:2010,Huesmann:2014}.


%2. Results from naive meta-analysis and consequent certainty among researchers. Nailing the coffin shut on doubts (Huesmann, 2010), Why is it so hard to convince / new media old problems (Donnerstein, Strasburger, Bushman, 2014), There is broad consensus (Bushman et al), how do we silence denialists (Anderson et al)

% and yet, still skeptics
Despite this purportedly-decisive evidence, there are still skeptics of the association.
Some conduct their own meta-analyses and find smaller effects \citep{Ferguson:2007a,Ferguson:2007b,Ferguson:InPress,Sherry:2001}, although these are sometimes criticized for containing fewer studies than the \citet{Anderson:etal:2010} meta-analysis. % Bushman much ado about something citation?
Some skeptics conduct experiments and fail to detect significant effects of violent game content \citep{Adachi:Willoughby:2011,Elson:etal:2013,Ferguson:etal:2008,Valadez:Ferguson:2012}. Some of these experiments provide empirical support for the null, but others provide minimal evidence or even some evidence for the effect \citep{Hilgard:etal:2014}.

Some suggest that the obtained effects are due to confounds \citep{Adachi:Willoughby:2011b;Elson:etal:2013}. Others argue that the public impact of the research has been overstated. This argument questions the external validity of laboratory measures and posits that causal changes observed in laboratory measures may not reflect real-world causal changes in real-world aggressive behavior \citep{Elson:Ferguson:2014}. %potential ecological fallacy, . arguing that society is becoming less aggressive despite increased use of video games 



%2. Sources of controversy include failures to replicate and speculation about biased analysis, reporting, and meta-analysis of research.


However, the debate is not limited to theoretical concerns. A more pervasive and subtle suggestion from skeptics is that something is amiss in the research process. Perhaps researchers are not being completely honest with their hypotheses and data. % This seems out of place here if I talk 
Skeptics have suggested that the literature is contaminated by publication bias, leading to overestimated effects \citep{Ferguson:2007a}. 
Others suspect publication bias \citep{Ferguson:2009?}. Others suggest that obtained study data are flexibly analyzed until the desired research conclusion is reached \citep{Elson:etal:2014;Ferguson:CITATION:NEEDED}. 

% Time is running out for academic freedom
\begin{quote}
Despite thousands of research studies on media effects, many people simply refuse to believe them. Some academics may contribute to this because they like to "buck the establishment," which is an easy way to promote themselves and their research. Of course, many people still believe that President Obama wasn't born in the United States, President Kennedy wasn't assassinated, men didn't walk on the moon, and the Holocaust didn't occur. \citep[p. 572]{Strasburger:etal:2014}
\end{quote}
%end blockquote

So confident are some believers of violent-media effects that any remaining skepticism seems to be due to unfathomable stubbornness. 
Sufficiently satisfied that the phenomenon has been demonstrated time and again in laboratory research, some authors have moved on to advancing theories as to why skepticism remains, suggesting that skeptics are influenced by psychological reactance or cognitive dissonance. Certain skeptics are compared to moon-landing deniers (or worse), described as being incapable to persuade by any amount of evidence \citep{Strasburger:etal:2014}) Researchers have begun to propose ways to ``advance the debate'' by speaking directly to the public to avoid skeptics within academia \citep{Strasburger:Donnerstein:2014}. Claims of consensus have been advanced \citep{Bushman:etal:2014}, and attempts have been made to separate ``true media violence scientists'' who believe in the effect from less-expert sources motivated to deny evidence \citep{Anderson:etal:2014}.

We are alarmed by these efforts to discredit skeptics and squelch debate. Skeptics provide a crucial role in science. They bring shortcomings in evidence to awareness so that they can be tested and addressed. They advance testable alternative explanations for observed phenomena.

On the other hand, undue skepticism threatens to misrepresent research findings and mislead public policy and debate. The frustrations of violent-game-effect proponents are understandable--from their perspective, the evidence is ironclad, and any disagreement comes from a stubborn refusal to learn from evidence. In the current manuscript, we test whether the evidence is indeed ironclad by inspecting the degree of bias in research results.

% Social Psychologists are Knuckleheads Theory
% citations: Bakker et al, 2011, Rules of the Game Known as Psych Science; Simmons et al., 2011, flexible research; Ioannidis, 2011, Most Published Research Findings Are False; Bones:2012, We Knew the Future All Along
We find it a futile endeavor to pick apart at theory on the basis of patterns of moderation or nonsignificance found in tiny underpowered studies. Trying to describe the boundary conditions of the effect based on such small samples will likely represent an overfitting of theory to data, as there is little evidential value to such small studies \citep{Hilgard:etal:2014}.
On the balance of the available data, the evidence for effects of violent games is clear.
However, what if the available data represent only a fraction or distortion of the total data? 
It is well known that publication bias is a persistent and tenacious problem in research, as is the flexible analysis of data.

% What is publication bias and small-study effects?
Publication bias is the phenomenon that studies with statistically significant ($p<.05$) findings are more likely to be submitted and accepted for publication. Publication bias is a problem that contributes to the overestimation of effect sizes and the propagation of Type I error. It is an especially pernicious problem for meta-analysis, as the selective reporting of studies that ``work'' (i.e. attain significance) leads to an overestimated effect size and may lead to conclusions of statistically significant effects when there are none. For this reason, a crucial part of meta-analysis is to attempt to inspect and adjust for the degree of publication bias.

% Fail-safe N stinks
Meta-analytic techniques for assessing and correcting for publication bias have been applied to this literature, particularly in the decisive \citet{Anderson:etal:2010} meta-analysis, but these techniques are of questionable utility. Fail-safe N, in theory, provides the number of studies %(subjects?)
that would need to be hypothetically hiding in file-drawers in order to reduce an effect to non-significance. This technique is flawed in that it assumes an effect size of exactly zero in all censored studies, does not consider the possibility of flexible analysis (e.g. p-hacking), and does not provide an estimate of a bias-adjusted effect size.

% Trim-and-fill and how it's bad
Another popular bias-adjustment technique, trim-and-fill \citep{Duval:Tweedie:2000}, attempts to detect and adjust for bias through inspection of the funnel plot. If the funnel plot is asymmetrical, the procedure ``trims'' off the most extreme study and imputes a hypothetical censored study reflected around the funnel plot's axis of symmetry. Studies are trimmed and filled until the funnel plot is no longer significantly asymmetrical. This is not an effective adjustment for bias, as the assumptions of trim-and-fill are unlikely to be met. Studies are not likely to be censored on the basis of the effect size, but rather, on the basis of their statistical significance. It is argued that trim-and-fill does a poor job of providing an adjusted effect size, adjusting too much when there is no bias and adjusting too little when there is bias. %Could cite data colada post
Applying trim-and-fill in their meta-analysis, \citet{Anderson:etal:2014} found only minimal adjustments to the effect size due to apparent bias. %Although we are confused by their decision to divide by culture and not to perform trim-and-fill on the total sample -- the smaller the sample, the poorer the power to detect asymmetry, perhaps?
Some have characterized this as an ``extensive'' test for publication bias \citep{some-chucklehead}, but we are not satisfied. In examining the funnel plots behind the study's main findings, we found naked, obvious symmetry.

Meta-regression techniques instead consider the relationship between effect size and precision. In an asymmetrical funnel plot, larger samples yield smaller effects, as would be expected if studies were censored when not attaining statistical significance or when studies are flexibly analyzed in order to attain statistical significance. The PET-PEESE meta-regression \citep{Stanley:Doucouliagos:2013} %See carter & McCullough for better citations
uses the published literature to estimate the relationship between precision and effect size. A weighted regression is fit to describe how the effect size changes with increasing experimental precision, then extrapolates to estimate what the ``true effect'' would be in a hypothetical study with perfect precision. This meta-regression technique has been previously applied by \citet{Carter:McCullough:2014} to inspect the amount of evidence for ``ego depletion'', the phenomenon of fatigue in self-control. They found that after adjusting for publication bias, PET-PEESE suggested an absence of evidence for the phenomenon, and therefore recommended a large-sample pre-registered replication effort.

We apply PET-PEESE meta-regression to two recent meta-analyses of violent video game effects, one covering research up until 2009 \citep{Anderson:etal:2010} and the other research between 2009 and 2013 \citep{Greitemeyer:Mugge:2014}. Both meta-analyses argued for statistically and practically significant effects of violent video games on aggressive outcomes. Moreover, both applied trim-and-fill and found little evidence or adjustment for research bias. However, visual inspection of the funnel plot often reveals alarming asymmetry, particularly in the case of those studies which \citet{Anderson:etal:2010} call ``best-practices'' studies.

% Methods
We acquired data from \citet{Anderson:etal:2010} and \citet{Greitemeyer:Mugge:2014}. Because the data were analyzed using Comprehensive Meta-Analysis, many studies were entered with separate rows for different outcomes or subsamples within studies. However, the assumption of the current PET-PEESE model is that entire studies are censored or re-analysed, and thus each study should constitute a single observation. We consulted with the original authors as how best to aggregate rows within studies and reproduce their provided estimates.

We then followed the PET-PEESE procedure, fitting a weighted-least-squares regression model predicting effect size as a linear function of the standard error, with weights inversely proportional to the square of the standard error. In the case that the PET regression found a statistically significant effect after accounting for publication bias, PEESE was applied, predicting effect size as a quadratic function of the standard error. PET or PEESE estimates are provided regardless of whether statistically significant bias was observed according to recommendations by \citet[p. 20-21]{Stanley:Doucouliagos:20XX}: ``To be conservative, one should always use [the PET or PEESE estimate] even if there is insufficient evidence of publication selection because the Egger test [of publication bias] is known to have low power.'' Within the analyses, all effect sizes were converted to Fischer's z so that they were normally distributed so as to fulfill the assumptions of the regression model. Effect sizes are converted back to Pearson $r$ for discussion.

Because PET-PEESE is a regression method, it is likely to perform poorly when there are few datapoints. Therefore, our analysis is restricted to effects and experimental paradigms with at least ten independent effect sizes. %citation needed from Carter & McCullough, 2014
Data and code have been made available online in the case that the reader nevertheless wants to generate PET-PEESE estimates for more sparse datasets or explore the impact of our inclusion and exclusion decisions.

For sensitivity analysis, we remove datapoints with a Cook's distance of more than 0.5, as these may have excessive influence over the slope of the regression line. Estimates are provided with and without these influential observations.

Two studies were removed from the meta-analysis. \citet[study 1]{Matsuzaki:etal:2006} was removed because its entered effect sizes were unusually large for their precision (i.e., effects on aggressive behavior $r = .60$ and aggressive cognition $r = .53$), highly influential, and could not be found as entered by inspection of the original article. Similarly, \citet{Panee:Ballard:2002} was removed because the study tested the effects of violent primes during gameplay and not the effects of violent gameplay itself. %might need to follow up on this

% Anderson et al.
We reproduce estimates from \citet{Anderson:etal:2010} and apply PET-PEESE. Sufficient datapoints were available to re-analyze experimental studies of aggressive affect, aggressive behavior, aggressive cognition, and physiological arousal, as well as cross-sectional studies of aggressive affect, aggressive behavior, and aggressive cognition. Studies are further divided into ``best-practices'' and ``not best-practices'' studies per \citet{Anderson:etal:2010} as sample sizes permit. 

%
We also attempt to reproduce estimates from \citet{Greitemeyer:Mugge:2014}. Effect sizes are inspected separately for aggressive affect, aggressive behavior, and aggressive cognition in experimental and cross-sectional research designs.

In the event that multiple effect sizes were available for a particular study (e.g. effects on mean intensity and count of high intensity trials in the CRTT; separate simple effects for men and women), we aggregated these to form a single effect size for the study.

\section{Results}
\subsection{Aggressive affect}
\paragraph{Experiments}
Among studies selected as best-practices, PET found no significant effect of violent games on aggressive affect. An effect of bias was clear. The estimated effect size was $r = -.12$.

When including both best- and not-best-practices studies, PET still found no significant effect of violent games on aggressive affect. An effect of bias was again clear. The estimated effect size was $r = -.11$.

We proceeded to explore the analysis by excluding observations with excessive influence (e.g. Cook's distance $>$ .5). After removing one study \citep{Ballard:Weist:1996}, PET estimates were still not significant whether for best-practices or all studies. After this exclusion, the estimated effect size was almost exactly zero. No other data points had excessive influence. 

\paragraph{Cross-sectional Research}

\subsection{Aggressive behavior}
\paragraph{Experiments}
Among studies selected as best-practices, PET found both a significant effect of violent games on aggressive behavior and a significant effect of bias. Because the intercept was significant, PEESE was then applied. PEESE estimated the effect as $r = .16$, substantially smaller than that of the naive or trim-and-fill estimates.

Among all studies, PET found a significant effect of violent games on aggressive behavior and no significant effect of bias. PEESE estimated the effect as $r = .16$, again smaller than that of the naive or trim-and-fill estimates. The closely matched effect sizes suggest that effects are not larger in best-practices research, just more biased.

We proceeded to explore the analysis by assessing the influence of individual studies. The largest study in the best-practices sample, \citet[Study 1]{Anderson:etal:2007}, had a Cook's distance just above .5. Excluding this influential observation increased the PEESE estimate ($r=.19$) and naive estimate ($r = .24$). The decision to include or exclude this study will surely be a point of some contention given its influence over the PEESE estimate. On the other hand, it is the largest study by far in the sample, more than twice as large as the next-largest study. 

\paragraph{Cross-sectional research}

\subsection{Aggressive cognition}
\paragraph{Experiments}
Among studies selected as best-practices, PET found neither a significant effect of violent games or a significant effect of bias. The effect size estimate was $r = .11$, much smaller than the naive estimate of $r = .22$.

Among all studies, PET found a significant effect of violent games on aggressive cognitions ($p = .049, r = .14$). PEESE estimated the effect as $r = .18$, again smaller than the naive or trim-and-fill estimates. 

No observations had excessive distance. 

\paragraph{Cross-sectional research}


\subsection{Physiological Arousal}
\paragraph{Experiments}

\section{Discussion}
% Gone from predicting 4.4\% of variance to 1.4\%
Our findings suggest that the effects of violent video games as causal predictors of aggressive thoughts, feelings, and behaviors have been overestimated. After accounting for the relationship between sample size and effect size, we estimate that violent games have no significant effect on aggressive feelings, very small effects on aggressive cognitions, and small effects on aggressive behavior. Effects on aggressive behavior were the only significant effect among those investigated, but effects were estimated as rather smaller than that originally reported by \citet{Anderson:etal:2010}. To put into perspective the difference between $r = .21$ and $r = .16$, we consider a power analysis for a hypothetical experiment with 80\% power, one-tailed. In such an experiment, $r = .21$ requires a sample size of $n = 136$, while $r = .16$ requires a substantially larger $n = 237$. %It may be necessary to demand larger sample sizes from game-effects research, as samples of less than 100 subjects are clearly underpowered. % Fuck Greitemeyer.

These PET-PEESE estimates stand in contrast to trim-and-fill results reported by citet{Anderson:etal:2010}. In this report, trim-and-fill failed to detect bias in best-practices experiments of aggressive affect, suggested $r = .18$ for Western best-practices experiments of aggressive behavior, and adjusted best-practices experiments of aggressive cognition only slightly to $r = .20$.

Clearly, publication bias, meta-analytic bias, and perhaps flexible analysis are problems in this literature as they are in so many other literatures. We recommend the use of large, pre-registered, open-data research projects, ideally with collaboration across antagonistic research teams. We also feel that, in light of the current results, there is reason enough for skepticism. Attempts to establish consensus, disseminate research findings to the public, or develop strategies for thwarting denialism may be premature. We hope that the years ahead foster a civil, unprejudiced, and transparent application of the scientific method. 

It is interesting to note that, contrary to the findings of \citet{Anderson:etal:2010}, we find that effects are equal or larger in not-best-practices experiments than in best-practices experiments after adjusting for selection and publication bias. This is possibly because the best-practices criteria are flawed or flexibly applied. An example of a flawed criterion is their recommendation that games be matched in pilot testing. Many studies which did pilot-test their games used such small samples that a lack of statistical significance did not necessarily constitute evidence for the null. % For more see Hilgard Engelhardt Bartholow & Rouder
However, it is possible that best-practices studies are simultaneously better-designed and more biased in their analysis and report. 

That said, PET-PEESE meta-regression is a relatively new technique and its limitations may not yet be fully understood. For these reasons, we suggested that the provided estimates be considered possible bias-adjusted estimates rather than corrected estimates of the true effect. Also, PET-PEESE is a regression method, and like most regression methods, results can be misleading when few datapoints are available. PET-PEEESE also extrapolates outside the model, estimating the effect when standard error is zero when no datapoints have zero error. 
Critics of PET-PEESE argue that, while it is unbiased, it is also inefficient, and so the results of any single PET-PEESE meta-regression may be quite inaccurate, even if many meta-regressions would perform well in the long run \citep{Reed:etal:WORKINGPAPER}. Simulations presented in this criticism, however, involve larger true effect sizes than are often found in psychology, and less asymmetrical funnel plots than are found in the present study. When true effects are small (e.g. $\delta \le 0.5$), PET-PEESE still seems to perform fairly well. However, the point is well taken that correcting for bias is something of a fool's errand.
Thus, rather than regard the present meta-analysis as having identified the true effect size, we instead interpret it as a warning sign that not all is right in scientific report. The observed funnel-plot asymmetry would be extremely unlikely if the research process in this field were unbiased and transparent. Thus, we call for increased transparency in the research process. 
%Preregistration, antagonistic collaboration, data-sharing, Simmons et al's 21 words, independent direct replication, large sample sizes, sensitivity analysis. 
Surely there can be no harm in a nice, pre-registered, large-sample replication by a disinterested third party.

% Problems with "consensus"
Scientific consensus is probably a good thing in certain contexts, but here I see it as stifling. I don't think Ferguson's research is great \citet[and we have even criticized his research as using too small of samples, see ][]{Hilgard:etal:2015}, but it seems that the reason he arrives at different results than other small studies is that he is less averse to the null. We urge researchers not to fall into the trap of "obedient replication, where investigators feel that the prevailing school of thought is so dominant that finding consistent results is perceived as a sign of being a good scientist and there is no room for dissenting results and objections." \citep{Ioannidis:2012} %page number needed, http://www.jpsychores.com/article/S0022-3999%2812%2900263-2/abstract

Research may have been too hasty in establishing a consensus, formulating strategies to exclude skeptics from public debate, serving as expert court witnesses, issuing public policy statements. The present results highlight the importance of transparent and unbiased research and meta-analysis, the archival and sharing of meta-analytic data for further scrutiny and analysis, and the continued tolerance of public debate and skepticism.

%Virtuous abstention from press releases
We wonder, to some degree, the extent to which frequent press releases from competing camps spoils the scientific discourse and sours the public on our research. 
To some extent, press releases may increase researcher rigidity and resistance to new findings. It is hard enough to entertain being wrong when one is on scientific record; it is harder still when one is in the public record.
While it is our civic duty to inform the public of our findings, it is also our duty to make very certain of our findings before sounding the alarm. We all know what became of the boy who cried wolf--his funding was slashed by a conservative senator from Nebraska. 
We will not be issuing a press release for the present research. The present matter speaks to our reasons for uncertainty as a research area and cannot be trumpeted as proof of one hypothesis over another. 

% What went wrong with Anderson 2010?
% Practical considerations for meta-analysis / reducing bias in meta-analysis
\subsection{Selection bias in meta-analysis} 
``Meta-analyses are known to suffer from the `junk in / junk out' phenomenon (which is unlikely to be fixed by `best-practices' efforts, when scholars may simply value their own junk higher than the junk of others.)'' \citep{Ferguson:Heene:2012}
We observe some instances of flexible application of the best-practices criteria offered by \citet{Anderson:etal:2010}. One criterion is that the violent and nonviolent game must be sufficiently different in violent content. Application of this criterion was inconsistent. Comparisons between the violent game {\em Mortal Kombat} and the nonviolent game {\em Sonic the Hedgehog} were discarded as not-best practices \citep[e.g.,][]{CITATION:NEEDED:2000} because ``the nonviolent game contained violence'' \citep[supplementary materials]{Anderson:etal:2010}. Another study comparing a racing game {\em Moto Racer} against the violent game \textbf{I DONT REMEMBER WHICH} \citep{Brooks:1996} was excluded for similar reasons, but we were not able to find any violent content in {\em Moto Racer}. Meanwhile, other studies involving comparisons between violent and not-entirely-nonviolent games were included. \citet{Konijn:etal:2007} was included, even though the game {\em Final Fantasy} is as violent or more violent than {\em Sonic the Hedgehog}. \citet{Brady:Mathews:2006} was included as best-practices despite comparing the violent {\em Grand Theft Auto 3} to the T-rated not-so-nonviolent game {\em Simpsons Hit and Run}. 
% The study by \citet{Brady:Matthews:2006} is a particularly interesting example, as the same study was included in its dissertation form \citep{Brady:2006} and listed as not-best-practices. 
% Craig: "As I recall, the dissertation study was the same as the B & M 2006 paper. So, there is no real contradiction here, other than that the dissertation should have been listed differently in the supplemental materials." I'll have to check the estimates and ks.
Flexibility in the application of this criterion may have contributed to selection biases, inflating the naive meta-analytic estimate while reducing the PET-PEESE estimate. A better approach might be to have manipulations rated by research assistants naive to hypotheses or to study results, or to seek a statistical quantification of the difference in violence between games, such as a Cohen's $d$ describing a manipulation check.

Selection bias may also have been facilitated by the application of best-practices criterion 5: The outcome measure could reasonably be expected to be influenced by the independent variable if the hypothesis were true. For an example of selection bias, see \citet[study 2]{Anderson:etal:2004}. In this study, participants were assigned to play a violent or nonviolent game, then complete a competitive reaction-time task measure of aggressive behavior with either an ambiguously or unambiguously provoking confederate. A significant effect was found amount the 90  subjects assigned to the ambiguous provocation condition ($r = .25$), but not among the 90 subjects assigned to the unambiguous provocation condition ($r = -.03$). These 90 subjects with a nonsignificant effect were dropped from both the best-practices and not-best-practices meta-analyses. When asked for comment, the authors said ``Only the ambiguous provocation condition was used because we now know that the unambiguous (increasing) provocation version of the task is not as sensitive to a variety of independent variables as is the ambiguous provocation pattern. In other words, the increasing provocation conditions don't meet Criterion 5.'' While it is possible that only one form of the task is sensitive to the manipulation, the meta-analysis does not seek to model such fine-grained moderators. We are also dismayed by the prospect that the validity or invalidity of measurements can be determined on whether they provide the researcher with the desired $p<.05$ in an experiment. Since a significant effect in either the ambiguous or unambiguous provocation group would be taken as evidence for an effect of violent video games, we feel that the selective exclusion of groups for not demonstrating such an effect risks introducing selection bias.

%  Selection bias in choice of effect sizes
Selection bias may also influence which effect size among those reported was entered into analysis. As a general rule, it seems that \citet{Anderson:etal:2010} attempted to avoid subjectivity in effect size entry by averaging all reported effect sizes together. However, on several instances, effect sizes were not averaged together, but rather the single largest available effect size was selected. For example, in the aforementioned \citet[study 2]{Anderson:etal:2004}, the effect of violent games on the first trial of the CRTT was entered, but not the reported effect size on the other 24 trials of the CRTT. In another example, \citet[study 2]{Anderson:etal:2007} report effects of violent video games on violent behavior ($r = .35$), physical aggression ($r = .46$), and verbal aggression ($r = .25$). Only the largest of these was used as the effect size of violent games on aggressive behavior. Similarly, the effect of violent games on aggressive affect was entered as the effect on trait anger ($r = .23$) but not the effect on trait hostility ($r = .21$). Selection of the largest effects risks capitalizing on chance and systematically overestimating the true effect.

We note further selection bias in the interpretation of violent games on physiological arousal. As presented by \citet{Anderson:etal:2010}, violent games cause significant increases in physiological arousal, e.g. heart rate or blood pressure. However, in researching this meta-analysis, we became aware of studies in which null effects of violent games were excluded from meta-analysis. For example, in the best-practices studies by \citet{Carnagey:Anderson:2005}, the violent and nonviolent versions of the game were not found to effect players' physiological arousal. Rather than present these findings as null results of violent games on physiological arousal, the authors presented this result as evidence that the violent and nonviolent games were matched stimuli. We observe a similar treatment in the meta-analysis: the null results on physiological arousal were omitted from the meta-analysis investigating effects of violent games on physiological arousal. We find this approach to be too flexible and forbids falsification of the theory, as concordant results are taken as evidence for the theory, but discordant results are excluded from consideration.

% Miscoded effect size in Carnagey & Anderson 2005?
Some null findings were censored or miscoded. For example, in the course of the experiment reported in \citet{Carnagey:Anderson:2005}, a nonexperimental assessment was also made of the effects of previous violent game exposure on aggressive outcomes. Nonexperimental effects were entered into the \citet{Anderson:etal:2010} meta-analysis but were much larger (and more statistically significant) than reported in the \citet{Carnagey:Anderson:2005} manuscript.

% Does looking for unpublished studies help?
The \citep{Anderson:etal:2010} meta-analysis did make an attempt to collect and analyze unpublished studies. That the resulting analysis remained biased despite these attempts gives us concern that searching for unpublished studies may not actually alleviate bias in meta-analysis. 
In reading this literature, it seems that there is confusion as to what constitutes unpublished research that ought to be included in meta-analyses. In our ignorance, we had assumed that unpublished literature relevant to meta-analysis would consist of all those studies that had been conducted and never submitted or accepted for publication on the basis of their dissatisfactory effect sizes or statistical significance. Instead, \citet{Bushman:etal:2010} describe unpublished studies as ``studies not published in a peer-reviewed journal, although it could have been published in another outlet (e.g., book).'' We are less concerned about the prestige of journals relative to textbooks and more concerned about studies hidden forever in file-drawers.
% re-read Anderson et al 2010 with special attention to how they define unpublished research.
\citet{Anderson:etal:2010} seem emphatic that there is little of this research and that the effect size is 

We note that few of these unpublished studies were accepted as best-practices research. Although we had hoped that the application of best-practices criteria would alleviate bias, recognizing well-performed research regardless of its results, it instead appears to have intensified bias. It is also possible that unpublished null findings were not available to be collected. Null findings are sometimes reanalyzed and massaged until they become positive research findings.

In all cases, the clear and accessible archival of meta-analytic data is a considerable aid to research transparency. We commend Anderson and colleagues for sharing the data and for responding to questions as to how best reproduce their analyses. The present analyses would not have been possible without their assistance. We suggest that future meta-analyses routinely include the data, funnel plots (in supplemental materials, if need be), and other supplementary materials. Meta-analyses that cannot be personally inspected or reproduced should be regarded with caution.

% Reducing bias in empirical research
We need to restructure incentives so that failure to detect a significant effect is an accepted research outcome. %Obedient replication.
% This part is too hot, needs to be toned down, isn't worth the whole paragraph.
Part of the mess appears to be that researchers quickly took the basic phenomenon for granted and began to cast about for more sophisticated models that would advance theory. In some cases, these more sophisticated models led to attempted conceptual, rather than direct, replications. In other cases, these more sophisticated models required some amount of post-hoc moderator munging. Suppose we conduct a basic two-cell violent and nonviolent game manipulation. Results reveal no significant effect of violent games, an unacceptable and unpublishable result. Thus, we begin to test post-hoc moderator after moderator: perhaps the effects hold for males, but not females? Those high in trait anger, but not those low in trait anger? Morning experiments, or night experiments? Such post-hoc exploratory analyses are, of course, important and valuable \citep[indeed, we present them ourselves in][]{Engelhardt:etal:2014}, but become dangerous when presented as confirmatory, or when patterns of statistical significance are taken to identify the validity or invalidity of the measures.

% Theoretical considerations
The estimated corrected effect is larger in less-controlled studies (cross-sectional vs experimental). Bias is also reduced in the cross-sectional research, as both sample sizes and the likely effect are both larger, more readily yielding statistical significance. 
We have wondered sometimes whether playing violent video games is itself a deviant behavior, as parents often prohibit it and polite society finds it unsavory. We also wonder at the confounding of game genre with game content, as many studies %CITATION NEEDED
measure violent game use not by measuring the violent content of games, but whether the games contain action or not.

We are also reluctant to apply PET-PEESE to longitudinal studies, as there are not enough data points to permit a good estimate. It is certainly possible, perhaps even likely, that there are small but noticeable longitudinal effects of hundreds of hours of gameplay over several years. This is more plausible than the prospect of a substantial and reliable effect obtained within fifteen to thirty minutes of gameplay. We echo the words of \citet[p. 51]{Bushman:Huesmann:2014}, ``In many ways it is quite impressive that playing a violent video game for just 15-30 min, on a single occasion can have significant and measurable effects on aggressive behavior.'' Our tone, however, is different.

We need to establish the existence of the phenomenon before attempting to elaborate on the effect, its moderators, and its broader implications. If the effect is indeed as small as we estimate here (r = .16) then identifying meaningful and reliable moderators of the effect will be challenging.  Moderators of the effect, if any, should be expected to have similarly small effects, and so may take impractically large samples to study. This may explain the counter-intuitive finding that effect sizes among children are not significantly larger than effects among adults: differences, if any, will be small, and are likely to be obscured by research bias. 

It may also be necessary to reevaluate our theories of aggressive behavior, specifically the General Aggression Model (GAM). According to GAM, aggressive feelings, thoughts, and behaviors are all closely related processes which feed into each other. Aggressive feelings are thought to inspire aggressive behavior, and aggressive thoughts increase the likelihood of aggressive behavior. The present analysis indicates that violent video games have minimal effect on aggressive feelings \citep[a finding paralleled by][]{Przybylski:etal:2014} but that, depending on inclusion criteria and the exclusion of influential data points, there may be some effect on aggressive thoughts and some smaller effect on aggressive behavior. If this is the case, changes in aggressive behavior caused by violent game exposure would seem to be more related to aggressive-thought accessibility than to aggressive feelings. In our own research, we have attempted to collect measures of aggressive thoughts, feelings, and behavior from participants within a single session and found them to correlate poorly \citep{Engelhardt:etal:2015}.

We echo the astonishment registered by \citet[p. 62]{Warburton:2014}: Given the theories and evidence in the rest of social psychology and media psychology, ``Violent media can and must have some psychological impact on those who experience it, and probably does so via well-understood psychological processes.'' It is remarkable that, in the face of such strong theory, only middling and biased evidence could be obtained. It may be necessary also to reconsider this theory. If A, the set of social psychological theories of media influence and aggressive behavior, implies B, that violent video games can and must have impact on players, then surely not-B, an absence of unbiased evidence that violent games influence players, might imply not-A, the set of social psychological theories. This is, of course, an enormous and crazy argument to make. But given the crisis of confidence psychology is presently experiencing, and given the middling effects we observe here and the apparent skill of psychologists to make mountains out of molehills, is it so bizarre to think that something is rotten in the state of Denmark?

Continuing the previous quote from \citet[p. 62]{Warburton:2014}, % having determined a priori that the effect must exist, and never mind the data,
``Thus, for me, research in media violence no longer needs to establish whether such media can have a psychological and behavioral impact, but should instead rigorously examine the boundary conditions for such impacts.'' If the effects are indeed so small as we estimate, researchers will be hard-pressed to detect the boundary conditions. To detect r = .16 with 80\% power, one-tailed, will require 240 subjects. To detect the small moderators that reduce the effect to insignificance may require a staggering amount of data.

Personally, I have never been a big fan of theory---it seems that there is far too much theory to explain far too little data, sometimes. Being in possession of an attractive theory is something like having a motte and bailey in a lush valley. The bailey would be pleasant to have, if only one did not have to spend so much time hiding in the keep. In a similar sense, having a fashionable and popular theory is quite pleasant, save all the work one must perform to defend it from the arrows and trebuchets of detractors. One unfortunate gap in the armor can lead to broad dissatisfaction with the theory, so who in their right mind would publish evidence against their own theory? One's every moment is preoccupied chasing off the raiders, and one even sips sparingly from the stream of data lest the next sample disprove one's own theory. % More like a wealthy playboy with a price on his head, IMO.

% Fuck Greitemeyer & Mugge 2014
We observed above that effect sizes vary dramatically between correlational and experimental research. Effect sizes are larger in correlational research, which has poorer controls than experimental research and cannot be used to demonstrate causality. For this reason, it is unwise to aggregate experimental and correlational research in meta-analysis. It conflates the large effect of non-causal relationships with the small effect of causal relationships. Similarly, we are reluctant to combine effect sizes from different outcomes, given that some have more evidence than others. % could allude to Mega-Silly 1

All I have ever wanted is to learn and make rational decisions from data. It is with this principle in mind that I designed and conducted my dissertation research, carefully preparing the stimuli and methodology, pre-registering the details, collecting a large sample, and treating the null hypothesis fairly, on equal footing with the alternative hypothesis. It is with some grief, then, that I find the last twenty years of research have provided me with a body of data that is tainted and might not be learned from without reservation. At this moment, my research estimates the effect as r = .07, n = 200something. (Fear not, dear reader. The sample size is preregistered, and I would not dream of altering it in pursuit of a p-value greater than or less than some arbitrary threshold.)

% Maybe the CRTT bias isn't showing up because everything, not just the CRTT, is biased. Would be nice to read Giancola & Parrott 2008. They show validity of 1st trial, mean, and count of intensity in 350+ subjects, but not of duration or product of duration stuff. Also, it's electricity, not noise.
% Who among us has not received an email from a collaborator suggesting that the means are in the right direction, and perhaps if we ran an additional forty subjects, the desired satistical significance might be attained?

% perhaps r=.20 is the ambient correlational effect size in social psych thanks to pub bias. Even if you're willing to believe that -only- half of published research findings are false \citep[c.f.,][]{Ioannidis:etal:2011}, that implies that the median effect size is roughly the effect size of nonsense and bias.

\end{document}

%Fun quotes
%Donnerstein, Strasburger, Bushman, comparing skepticism to holocaust denial
%Paul Broca proves that women's brains are smaller and that women are inferior
% Gustave Le Bon (1879), "In the most intelligent races, as among the Parisians, there are a large number of women whose brains are closer in size to those of gorillas than to the most developed male brains. This inferiority is so obvious that no one can contest it for a moment; only its degree is worth discussion. All psychologists who have studied the intelligence of women, as well as poets and novelists, recognize today that they represent the most inferior forms of human evolution and that they are closer to children and savages than to an adult, civilized man.


%Bushman & Huesmann 2014, p. 51, "Furthermore, the Anderson et al. (2010) meta-analysis included extensive testing for possible publication bias, and found none. [...] In many ways it is quite impressive that playing a violent video game for just 15-30 min, on a single occasion can have significant and measurable effects on aggressive behavior."
%Bushman & Huesmann 2014 cites Carlson, Marcus-Newhall, & Miller 1989 meta-analysis as demonstrating "impressive convergence across a wide range of laboratory aggression measures" and Anderson & Bushman 1997 meta-analysis as finding that "'real' and laboratory measures of aggression are influenced in similar ways by situational variables (e.g., alcohol, provocation, anonymity) and by individual difference variables (e.g., trait aggressiveness, participant sex, Type A personality)."
% Bushman & Huesmann emphasize that Anderson 2010 did not ask -anyone- for -unpublished- studies, and that some unpublished dissertations were found but had been published or had not met the inclusion criteria. [best-practices inclusion criteria??]
% Bushman & Huesmann suggest turning Pearson r into an odds ratio because it sounds more impressive. It is, maybe, if you are willing to assume even odds as priors.
% Parting shot, bagging on Ferguson for writing ``violent prose''.

% Warburton 2014 coming off the top rope talking about "Absolute truths in science are elusive'' but basically arguing that the research attains a reasonable degree of proof.
% "That is, unless there is a credible explanation as to why the effects of violent media (including video games) should be an exception to established findings from other media, or a valid reason why different psychological mechanisms would underlie those effects, it must be assumed that media violence effects are likely to follow patterns similar to those that have already been demonstrated." (maybe they're all bunk because psychologists are knuckleheads)
% Cites Greietemeyer a couple times as part of a ``growing research stream on positive media impacts'' which I aso think are rot based on their terrible power
% Warburton argues that other media does have impact, violence exposure factors have impact, and the processes must be the same as under other social psychological processess. He seems to decide a priori that "violent media can and must have some psychological impact on those who experience it, and probably does so via well-understood psychological processes. Thus, for me, research in media violence no longer needs to establish whether such media can have a psychological and behavioral impact, but should instead rigorously examine the boundary conditions for such impacts." (p 62)
% Warburton claims to have improved the hot sauce paradigm methodologically, Waburton Williams Carins, 2006; Warburton, 2014
% Adorable to see him mis-state p-value. "At law [on the balance of probabilities] is a lesser burden of proof. In science, many effects are tested in this way because statistically, the significance of a finding is usually measured in terms of the probability that it is erroneous (i.e., a Type I error). Most commonly there is a cut-off, a value at which the probability is too high that an effect found is simply due to an error in the study (e.g., a p-value of 5 %). Above the value it is thought that, on the balance of probabilities, an effect is not likely to be real. Below the value, it is thought that, on the balance of probabilities, an effect is likely to be real." Barf!
%Again argues publication bias claim ``strongly refuted'' but Anderson et al., and cite Rothstein & Bushman (2012) as published meta-analysis experts.
% I'm a little bothered by this idea that larger metas are always better. Anderson and Greitemeyer both seem to include a lot of studies and effects I'm not sure really belong in there. They argue selectivity based on their biased best-practices sample but argue massive sample size based on the no-criteria sample.
% See also Bushman, Rothstein, and Anderson about the pub bias argument.
% Warburton talks about the triangulation of experimental, cross-sectional, longitudinal, and brain-imaging research. I see a triangulation of research bias!
% Public policy statement 2000 from AAP, AACAP, APA, AMA, AAFP, APA.

% Barbara Krahe
% Cites Bargh, Chen, Burrows (1996) as definitive evidence of priming effects, lmao. People still believed this stuff in 2014?

% Bushman Rothstein Anderson 2010 Much Ado about Something
% very odd: "The term -unpublished study- means that the study was not published in a peer-reviewed journal, although it could have been published in another outlet (e.g., book)". Chiefly I am concerned about studies that never made it to any public attention (e.g., were swallowed due to $p > .05 %$)
% Cite a bunch of experts as emphasizing searching for books, book chapters, conference proceedings, dissertations, and other "gray" literature. So where is the gray literature?
% This whole article is gold, they harp endlessly upon how important it is that they sought out unpublished studies.
% Talking about "small study effects" but we really know what's going on here.

% For everybody else, everything else has been so consistently statistically significant that they can't fathom this NOT being statistically significant.
% I'm coming from the opposite direction: This is significant or at least exaggerated through dramatic publication and analytic bias. It is hard to believe that the foundational research of the 70s, 80s, 90s that we build upon is similarly biased. If we want to develop a research tool and argue for its efficacy, there are clear right and wrong answers. Research producing the right answer is more likely to be published and accepted than research producing the wrong answer.
% "Spun-glass theory of the mind" -- Meehl, 1973, Why I Do Not Attend Case Conferences